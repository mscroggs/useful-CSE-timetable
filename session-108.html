<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Information Fusion for Computational Models and Inverse Problems - Part II of II</h2><div class='index-talk' id='talk487' style='display:block'><a href='javascript:toggle_star(487)' class='star'><span class='star487'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (E105)</b> Alex Gorodetsky, Graph Construction in Multi-Fidelity Networked Surrogates <span id='bitlink-451'><small><a href='javascript:show_bit(451)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-451' style='display:none'><small><a href='javascript:hide_bit(451)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Graph Construction in Multi-Fidelity Networked Surrogates</b><br />Alex Gorodetsky<br />Wednesday, March 1 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-108.html'>Information Fusion for Computational Models and Inverse Problems - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E105<br /><br /><small>This talk discusses graph-inference methodologies for MFNets -- a paradigm for multifidelity information fusion via directed acyclic graphs. MFNets provide a flexible approach to modeling the relationships between unstructured ensembles of models and information sources by linking the outputs of each information source though a network of models. These structures can then be leveraged for forward and inverse problems. One of the challenges in this approach is determining good graphs to represent the information sources. In this talk we discuss and overview several graph-building strategies and apply them to MFNets. Specifically, we compare graph incremental construction, model selection, and model averaging. We take a probabilistic viewpoint for assessing and developing graph construction algorithms and investigate how the inner inference loops affect the resulting outcome. For example, we determine how posteriors based on variational inference affect the resulting graphs. Examples from both synthetic and physical models are provided.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75382' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk488' style='display:block'><a href='javascript:toggle_star(488)' class='star'><span class='star488'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (E105)</b> Cosmin Safta, Modeling Spatio-Temporal Processes in Climate Models via Functional Tensor Networks <span id='bitlink-452'><small><a href='javascript:show_bit(452)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-452' style='display:none'><small><a href='javascript:hide_bit(452)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Modeling Spatio-Temporal Processes in Climate Models via Functional Tensor Networks</b><br />Cosmin Safta<br />Wednesday, March 1 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-108.html'>Information Fusion for Computational Models and Inverse Problems - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E105<br /><br /><small>We present a flexible framework for capturing high-dimensional non-linear interactions in computationally expensive models. In this talk we will use the land model component of E3SM to drive algorithm developments. Specifically we will rely on functional tensor networks to construct surrogate models for the land model dynamics at several observation sites. We will compare the performance of several network topologies to capture the interactions between model components and will use the resulting spatio-temporal surrogates to extract parameter sensitivity indices and for subsequent model calibration studies.     
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75382' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk489' style='display:block'><a href='javascript:toggle_star(489)' class='star'><span class='star489'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (E105)</b> Aku Kammonen, Adaptive Random Fourier Features with Metropolis Sampling <span id='bitlink-453'><small><a href='javascript:show_bit(453)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-453' style='display:none'><small><a href='javascript:hide_bit(453)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Adaptive Random Fourier Features with Metropolis Sampling</b><br />Aku Kammonen<br />Wednesday, March 1 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-108.html'>Information Fusion for Computational Models and Inverse Problems - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E105<br /><br /><small>The supervised learning problem to approximate a function $f:\mathbb R^d\to\mathbb R$ by  a neural network approximation   $\mathbb{R}^d\ni x\mapsto\sum_{k=1}^K\hat\beta_k e^{{\mathrm{i}}\omega_k\cdot x}$  with one hidden layer is studied as  a random Fourier features algorithm.   Here the mean square loss problem can be solved easily, since it is convex in the amplitude parameters $\hat\beta_k$ given a density $p:\mathbb R^d\to [0,\infty)$ for independent frequencies $\omega_k$. It is also well known that the corresponding  generalization error is bounded by $K^{-1}\Vert \vert \hat f\vert ^2/((2\pi)^dp)\Vert _{L^1(\mathbb R^d)} $, where $\hat f$ is the Fourier transform of $f$. In my talk I will first show how the constant $\Vert \vert \hat f\vert ^2/((2\pi)^dp)\Vert _{L^1(\mathbb R^d)}$ can be minimized by optimally choosing the density $p$ and then  how to approximately sample from this density, only using the data and certain adaptive Metropolis steps. I will also show results with other activation functions.    
[1] Kammonen, Aku and Kiessling, Jonas and Plechac, Petr and Sandberg, Mattias and Szepessy, Anders. Adaptive random Fourier features with Metropolis sampling. Foundations of Data Science, 2020.    
[2] Kammonen, Aku and Kiessling, Jonas and Plechac, Petr and Sandberg, Mattias and Szepessy, Anders and Tempone, Raul. Smaller generalization error derived for a deep residual neural network compared with shallow networks. IMA Journal of Numerical Analysis, 2022.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75382' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk490' style='display:block'><a href='javascript:toggle_star(490)' class='star'><span class='star490'>&star;</span></a> <b>2:50 PM&ndash;3:05 PM (E105)</b> Massimo Buscema, Meta Net Ann: a New Fusion Algorithm <span id='bitlink-454'><small><a href='javascript:show_bit(454)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-454' style='display:none'><small><a href='javascript:hide_bit(454)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Meta Net Ann: a New Fusion Algorithm</b><br />Massimo Buscema<br />Wednesday, March 1 2:50 PM&ndash;3:05 PM<br />This is the 4th talk in <a href='session-108.html'>Information Fusion for Computational Models and Inverse Problems - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E105<br /><br /><small>The fundamental characteristic of the Meta-Net consists of considering not only the “positive credibility” of its composing classifiers (i.e., “this pattern is white”), but also their “negative credibility” (i.e., “this pattern is not white”). So, the characterizing connection of the Meta-Net is to connect each output node of each composing classifier with each output class.  “Complete grid” connections are planned between Meta-Net inputs and outputs, and each connection can be either excitatory (positive numbers), or inhibitory (negative numbers).  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75382' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk491' style='display:block'><a href='javascript:toggle_star(491)' class='star'><span class='star491'>&star;</span></a> <b>3:10 PM&ndash;3:25 PM (E105)</b> Zhen Chen, Deep Learning of Systems with Physical Structures <span id='bitlink-455'><small><a href='javascript:show_bit(455)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-455' style='display:none'><small><a href='javascript:hide_bit(455)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Deep Learning of Systems with Physical Structures</b><br />Zhen Chen<br />Wednesday, March 1 3:10 PM&ndash;3:25 PM<br />This is the 5th talk in <a href='session-108.html'>Information Fusion for Computational Models and Inverse Problems - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E105<br /><br /><small>We propose a data-driven method to learn the dynamics of systems with physical structures using deep neural networks. We incorporate the physical structures into the design of the networks and show that such models can predict dynamics obeying desired physical laws of the system. To demonstrate our method, we focus on conservation laws in partial differential equations and derive conservative form networks. Numerical results show that models with physical structures outperform models without ones in both accuracy and physical property.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75382' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
