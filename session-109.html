<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Parallel Linear Solvers for Ill-Conditioned Engineering Design Problems - Part I of II</h2><div class='index-talk' id='talk2058' style='display:block'><a href='javascript:toggle_star(2058)' class='star'><span class='star2058'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D301)</b> Lisa Gaedke-Merzhaeuser, GPU Acceleration of Spatial-Temporal Bayesian Inference <span id='bitlink-1883'><small><a href='javascript:show_bit(1883)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1883' style='display:none'><small><a href='javascript:hide_bit(1883)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>GPU Acceleration of Spatial-Temporal Bayesian Inference</b><br />Lisa Gaedke-Merzhaeuser<br />Tuesday, February 28 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-109.html'>Parallel Linear Solvers for Ill-Conditioned Engineering Design Problems - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D301<br /><br /><small>Sparse direct solvers are a central component of many computational science analysis codes and often represent most of the full application’s run-time. An instance of this is found in large-scale spatial-temporal approximate Bayesian computing, which has applications in a wide variety of fields like epidemology, drug testing, geophysics and finance. Popular Bayesian methodologies can require iteratively solving a non-convex optimization problem. The matrices involved are sparse, symmetric, positive-definite and often ill-conditioned. They often exhibit a recurring block tridiagonal arrowhead sparsity structure that relates to the spatial-temporal nature of the model. The arising kernel operations are the numerical factorization and selected inversion of such matrices.   This work puts forward a GPU-accelerated block solver tailored to address the computational bottlenecks. It makes use of the particular sparsity pattern of the matrices and leverages dense GPU-operations in an otherwise sparse setting. A scaling analysis for applications of different sizes is presented and comparative benchmarks with PARDISO are shown. We discuss potential other solvers suited to such problems and in particular the GPU version of PARDISO 8.0, for which, in addition to data-integrated simulations results from Bayesian inference, we will also show results from industrial circuit simulations.     
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75384' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk493' style='display:block'><a href='javascript:toggle_star(493)' class='star'><span class='star493'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D301)</b> Tobias Ribizel, GPU-Resident Sparse Factorizations <span id='bitlink-456'><small><a href='javascript:show_bit(456)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-456' style='display:none'><small><a href='javascript:hide_bit(456)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>GPU-Resident Sparse Factorizations</b><br />Tobias Ribizel<br />Tuesday, February 28 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-109.html'>Parallel Linear Solvers for Ill-Conditioned Engineering Design Problems - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D301<br /><br /><small>We present a set of preprocessing steps, symbolic and numerical factorization algorithms on GPU-resident matrices well-suited for the solution of sparse linear systems producing only small amounts of fill-in, where classical supernodal approaches fail to provide adequate performance. We present the performance characteristics of the heavily latency-bound factorization kernels on NVIDIA and AMD GPUs and suggest alternatives to established approaches where feasible.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75384' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk494' style='display:block'><a href='javascript:toggle_star(494)' class='star'><span class='star494'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D301)</b> Rabah Aider, An Efficient Iterative Solver for the GEM Model <span id='bitlink-457'><small><a href='javascript:show_bit(457)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-457' style='display:none'><small><a href='javascript:hide_bit(457)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>An Efficient Iterative Solver for the GEM Model</b><br />Rabah Aider<br />Tuesday, February 28 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-109.html'>Parallel Linear Solvers for Ill-Conditioned Engineering Design Problems - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D301<br /><br /><small>Efficient and highly scalable numerical algorithms are increasingly needed to solve atmospheric fluid dynamics equations in weather forecasting models.   In the Canadian GEM weather forecasting model, a system of nonlinear equations is linearized around a basic state and then reduced to an elliptic boundary value (EBV) problem. The large linear system arising from the discretization of the EBV problem is solved either by a parallel direct method or by a parallel iterative method based on a variant of Krylov subspace methods, namely the Flexible GMRES (FGMRES) algorithm. The operational GEM parallel solver account for a significant amount of the total model run-time, pointing to the need for a performance upgrade. Recently the efficiency and the parallel scalability of the GEM model iterative solver have been improved by the implementation of a suitable preconditioner, based on the Restricted Additive Schwarz, leading to the FGMRES convergence rate acceleration, therefore to a reduction in the number of iterations. Furthermore, the use of a newly reformulated Arnoldi-Gram-Schmidt algorithm (Swirydowicz et al., 2019) resulted in a small number of global reductions and synchronizations (communication) required by the inner products.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75384' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk495' style='display:block'><a href='javascript:toggle_star(495)' class='star'><span class='star495'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D301)</b> Shelby Lockhart, Anderson Acceleration on Emerging Architectures <span id='bitlink-458'><small><a href='javascript:show_bit(458)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-458' style='display:none'><small><a href='javascript:hide_bit(458)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Anderson Acceleration on Emerging Architectures</b><br />Shelby Lockhart<br />Tuesday, February 28 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-109.html'>Parallel Linear Solvers for Ill-Conditioned Engineering Design Problems - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D301<br /><br /><small>Anderson Acceleration (AA) is a method to accelerate the convergence of fixed point iterations for nonlinear, algebraic systems of equations. Due to the requirement of solving a least squares problem (LSP) at each iteration, every iteration of AA requires some number of synchronization steps for global reductions. Performance of AA at scale on distributed systems is typically dependent upon the cost of solving this LSP or application of the underlying fixed point function. In this talk, we highlight approaches to low synchronization orthogonalization routines to reduce the overheard of the LSP solve and note that when performance is dependent upon the fixed point function evaluations, the reduction in synchronizations has little impact on overall performance. To reduce the overhead of costly function evaluations, there has been recent work done to modify the AA algorithm itself, such as alternating AA or non-stationary AA, methods which result in more rapid convergence than standard AA, thus reducing the number of function evaluations required for convergence. Overall, we explore several of these advances within the field of AA and their potential in developing scalable AA solvers on emerging supercomputer architectures.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75384' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk496' style='display:block'><a href='javascript:toggle_star(496)' class='star'><span class='star496'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (D301)</b> Gregor Olenik, Integrating Ginkgos Distributed Linear GPU Solvers for CFD Applications <span id='bitlink-459'><small><a href='javascript:show_bit(459)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-459' style='display:none'><small><a href='javascript:hide_bit(459)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Integrating Ginkgos Distributed Linear GPU Solvers for CFD Applications</b><br />Gregor Olenik<br />Tuesday, February 28 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-109.html'>Parallel Linear Solvers for Ill-Conditioned Engineering Design Problems - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D301<br /><br /><small>To serve the ever-increasing demand for computing resources, many HPC systems are  equipped with multiple highly parallel general-purpose GPUs. In most cases, the GPUs  are integrated as discrete server-type GPUs that are attached to the nodes as coprocessors  and provide the lion’s share of the theoretical compute performance. This concept allows  to significantly increase the theoretical compute power, however, it poses a challenge to  the scientific software developers that need to redesign their software such that it can  benefit from the additional resources. An attractive strategy is to use the GPUs for the  computationally most expensive part of an application. For computational fluid dynamics  (CFD) simulations, a substantial part of the overall effort is spent on the solution of the  system of linear equations arising from the discretized transport equations. This paper  investigates how the currently ongoing effort within the open-source linear algebra library  Ginkgo towards distributed GPU computing can be used as a backend for the CFD framework OpenFOAM  to improve the simulation performance by offloading the linear algebra computations to  GPUs.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75384' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
