<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part I of II</h2><div class='index-talk' id='talk1250' style='display:block'><a href='javascript:toggle_star(1250)' class='star'><span class='star1250'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (Emerald Room)</b> Juan M. Restrepo, Defining a Trend from a MultiScale Time Series <span id='bitlink-1156'><small><a href='javascript:show_bit(1156)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1156' style='display:none'><small><a href='javascript:hide_bit(1156)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Defining a Trend from a MultiScale Time Series</b><br />Juan M. Restrepo<br />Monday, February 27 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-275.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part I of II</a> (1:50 PM&ndash;3:30 PM)<br />Emerald Room<br /><br /><small>We propose criteria that define a trend for time series with inherent multi-scale features. We call this trend the tendency of a time series. The tendency can represent an executive summary of a complex time series, and as such can be viewed of a dimension-reduced representation of original signal. The tendency is defined empirically by a set of criteria and captures the large-scale temporal variability of the original signal as well as the most frequent events in its histogram. Among other properties, the tendency has a variance no larger than that of the original signal; the histogram of the difference between the original signal and the tendency is as symmetric as possible; and with reduced complexity, the tendency captures essential features of the signal.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75623' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1251' style='display:block'><a href='javascript:toggle_star(1251)' class='star'><span class='star1251'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (Emerald Room)</b> Yanzhao Cao, Stochastic Gradient Descent Alternating Least Square Method for High-Order Tensor Decomposition <span id='bitlink-1157'><small><a href='javascript:show_bit(1157)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1157' style='display:none'><small><a href='javascript:hide_bit(1157)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Stochastic Gradient Descent Alternating Least Square Method for High-Order Tensor Decomposition</b><br />Yanzhao Cao<br />Monday, February 27 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-275.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part I of II</a> (1:50 PM&ndash;3:30 PM)<br />Emerald Room<br /><br /><small>High-order tensors have applications in many areas (biology, finance, engineering, etc.). One of the key issues in high-order tensor research is the optimal rank one decomposition of a tensor, which is comparable to the singular value decompositions (SVD) for matrices. Unlike SVD, the rank one decomposition problem for high-order tensors is NP-hard.   The Stochastic Gradient Descent Alternating Least Squares (SALS) method is a generalization of the well-known Alternating Least Squares (ALS) method that approximates the canonical decomposition of averages of sampled random tensors. Its simplicity and efficient memory usage make the SALS   algorithm an ideal tool for decomposing tensors in an online setting. This talk will discuss the convergence of the SALS as well as its application to scientific data analysis.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75623' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1252' style='display:block'><a href='javascript:toggle_star(1252)' class='star'><span class='star1252'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (Emerald Room)</b> Sifan Wang, Random Weight Factorization Improves the Training of Continuous Neural Representations <span id='bitlink-1158'><small><a href='javascript:show_bit(1158)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1158' style='display:none'><small><a href='javascript:hide_bit(1158)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Random Weight Factorization Improves the Training of Continuous Neural Representations</b><br />Sifan Wang<br />Monday, February 27 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-275.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part I of II</a> (1:50 PM&ndash;3:30 PM)<br />Emerald Room<br /><br /><small>Continuous neural representations have recently emerged as a powerful and flexible alternative to classical discretized representations of signals. However, training them to capture fine details in multi-scale signals is difficult and computationally expensive. Here we propose random weight factorization as a simple drop-in replacement of conventional dense layers for accelerating and improving the training of coordinate-based multi-layer perceptrons (MLPs). We show that this factorization essentially alters the loss landscape and effectively enables each neuron in the network to learn its own self-adaptive learning rate. As a result, it not only helps with mitigating spectral bias, but also allows networks to quickly recover from poor initializations and reach better local minima. We demonstrate how random weight factorization can be leveraged to accelerate and improve the training of neural representations on a variety of tasks, including image regression, shape representation, computed tomography, inverse rendering, solving partial differential equations, and learning operators between function spaces.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75623' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1253' style='display:block'><a href='javascript:toggle_star(1253)' class='star'><span class='star1253'>&star;</span></a> <b>2:50 PM&ndash;3:05 PM (Emerald Room)</b> Lu Zhang, MetaNO: A Meta-Learnt Nonlocal Neural Operator Approach for Efficient Material Modeling <span id='bitlink-1159'><small><a href='javascript:show_bit(1159)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1159' style='display:none'><small><a href='javascript:hide_bit(1159)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>MetaNO: A Meta-Learnt Nonlocal Neural Operator Approach for Efficient Material Modeling</b><br />Lu Zhang<br />Monday, February 27 2:50 PM&ndash;3:05 PM<br />This is the 4th talk in <a href='session-275.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part I of II</a> (1:50 PM&ndash;3:30 PM)<br />Emerald Room<br /><br /><small>In real-world material modeling problems, the data acquisition is often very challenging and expensive, which makes learning the material model with a limited number of measurements critical. Herein, we propose a meta-learnt approach for transfer-learning between neural operators, Meta-NO, based on the implicit Fourier neural operator (IFNO) approach. The overall goal is to efficiently provide accurate solution surrogates for new and unknown material-learning tasks (e.g., with different microstructure or mechanical parameters), from multiple training tasks where each task corresponds to different materials. The proposed sample-efficient meta-learning algorithm consists of two phases: (1) learning a common representation by sharing the same iterative integral layers from existing tasks; and (2) transferring the learned knowledge and rapidly learning surrogate operators for new and unseen tasks with a different material, where only a few test samples are required. The result demonstrates that the meta-learnt representation would handle complex and nonlinear material response learning tasks, while greatly improving the sampling efficiency in new and unseen microstructures. Hence, it would substantially reduce the cost on lab testing for new materials.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75623' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1254' style='display:block'><a href='javascript:toggle_star(1254)' class='star'><span class='star1254'>&star;</span></a> <b>3:10 PM&ndash;3:25 PM (Emerald Room)</b> Vijay K. Yadav, Lagrange $\alpha$-Exponential Synchronization of Non Identical Fractional Order Complex Valued Neural Networks <span id='bitlink-1160'><small><a href='javascript:show_bit(1160)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1160' style='display:none'><small><a href='javascript:hide_bit(1160)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Lagrange $\alpha$-Exponential Synchronization of Non Identical Fractional Order Complex Valued Neural Networks</b><br />Vijay K. Yadav<br />Monday, February 27 3:10 PM&ndash;3:25 PM<br />This is the 5th talk in <a href='session-275.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part I of II</a> (1:50 PM&ndash;3:30 PM)<br />Emerald Room<br /><br /><small>In this article, Lagrange $\alpha$-exponential synchronization of non-identical fractional-order complex-valued neural networks (FOCVNNs) is studied. Numerous favorable conditions for achieving Lagrange a-exponential synchronization and a-exponential convergence of the descriptive networks are constructed using additional inequalities and the Lyapunov method. Furthermore, the structure of the a-exponential convergence ball, in which the rate of convergence is linked with the systemâ€™s characteristics and order of differential, has also been demonstrated. These findings, which do not require consideration of the existence and uniqueness of equilibrium points, help to generalize and improve previous works and may be used to mono-stable and multi-stable of the FOCVNNs. The salient feature of the article is the graphical exhibition of the effectiveness of the proposed method by using numerical simulation for synchronization of a particular case of the considered fractional-order drive and response systems.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75623' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
