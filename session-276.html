<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part II of II</h2><div class='index-talk' id='talk1255' style='display:block'><a href='javascript:toggle_star(1255)' class='star'><span class='star1255'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (Emerald Room)</b> Lili Ju, Level Set Learning with Pseudo-Reversible Neural Networks for Nonlinear Dimension Reduction in Function Approximation <span id='bitlink-1161'><small><a href='javascript:show_bit(1161)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1161' style='display:none'><small><a href='javascript:hide_bit(1161)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Level Set Learning with Pseudo-Reversible Neural Networks for Nonlinear Dimension Reduction in Function Approximation</b><br />Lili Ju<br />Tuesday, February 28 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-276.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Emerald Room<br /><br /><small>Due to the curse of dimensionality and the limitation on training data, approximating high-dimensional functions is a very challenging task even for powerful deep neural networks. Inspired by the Nonlinear Level set Learning (NLL) method that uses the reversible residual network (RevNet), in this paper we propose a new method of Dimension Reduction via Learning Level Sets (DRiLLS) for function approximation. Our method contains two major components: one is the pseudo-reversible neural network (PRNN) module that effectively transforms high-dimensional input variables to low-dimensional active variables, and the other is the synthesized regression module for approximating function values based on the transformed data in the low-dimensional space. The PRNN not only relaxes the invertibility constraint of the nonlinear transformation present in the NLL method due to the use of RevNet, but also adaptively weights the influence of each sample and controls the sensitivity of the function to the learned active variables. The synthesized regression uses Euclidean distance in the input space to select neighboring samples, whose projections on the space of active variables are used to perform local least-squares polynomial fitting. This helps to resolve numerical oscillation issues present in traditional local and global regressions.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75624' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1256' style='display:block'><a href='javascript:toggle_star(1256)' class='star'><span class='star1256'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (Emerald Room)</b> Diego Del-Castillo-Negrete, Machine Learning Methods for Particle-Based Plasma Computations <span id='bitlink-1162'><small><a href='javascript:show_bit(1162)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1162' style='display:none'><small><a href='javascript:hide_bit(1162)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Machine Learning Methods for Particle-Based Plasma Computations</b><br />Diego Del-Castillo-Negrete<br />Tuesday, February 28 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-276.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Emerald Room<br /><br /><small>The development of accurate and efficient methods for transport computations is critical in magnetically confined plasmas for controlled nuclear fusion. In this presentation we discuss how machine learning methods can help alleviate two aspects of this problem that are a bottle neck in current particle-based computations. The first one is magnetic field interpolation needed for particle tracking. We present a neural network-based divergence-free (NN-DivFree) interpolation method for arbitrary magnetic field training data points. The method is mesh-free and based on a data-driven reconstruction of the magnetic field using a feedforward neural network. The NN-DivFree method exhibits a significant reduction of the computational complexity compared to local splines while maintaining a small error in the divergence. The second problem is the estimation of the particle density, given the 6-dimensional coordinates of an ensemble of computed particle orbits. To address this problem, we propose the use of Normalizing Flows (NF) which are a family of generative models where both sampling and density evaluation can be efficient and exact. Our method is based on a pseudo-reversible neural network architecture, which improves the estimation by relaxing the invertibility constraint of the nonlinear transformation. Although NF can be computationally inefficiency in very high-dimensional problems, numerical experiments show that this is not the case for the 6-D problem of interest.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75624' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1257' style='display:block'><a href='javascript:toggle_star(1257)' class='star'><span class='star1257'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (Emerald Room)</b> Nick Dexter, Effective Deep Neural Network Architectures for Learning High-Dimensional Banach-Valued Functions from Limited Data <span id='bitlink-1163'><small><a href='javascript:show_bit(1163)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1163' style='display:none'><small><a href='javascript:hide_bit(1163)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Effective Deep Neural Network Architectures for Learning High-Dimensional Banach-Valued Functions from Limited Data</b><br />Nick Dexter<br />Tuesday, February 28 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-276.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Emerald Room<br /><br /><small>In the past few decades the problem of reconstructing high-dimensional functions taking values in abstract spaces from limited samples has received increasing attention, largely due to its relevance to uncertainty quantification (UQ) for computational science and engineering. These UQ problems are often posed in terms of parameterized partial differential equations whose solutions take values in Hilbert or Banach spaces. Impressive results have been achieved on such problems with deep learning (DL), i.e. machine learning with deep neural networks (DNN). This work focuses on approximating high-dimensional smooth functions taking values in reflexive and typically infinite-dimensional Banach spaces. Our novel approach to this problem is fully algorithmic, combining DL, compressed sensing (CS), orthogonal polynomials, and finite element discretization. We present a full theoretical analysis for DNN approximation with explicit guarantees on the error and sample complexity, and a clear accounting of all sources of error. We also provide numerical experiments showing that DNNs can produce accurate approximations on challenging Banach-valued benchmark problems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75624' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1258' style='display:block'><a href='javascript:toggle_star(1258)' class='star'><span class='star1258'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (Emerald Room)</b> Victor Churchill, Learning the Evolution of Unknown Systems via Deep Neural Networks <span id='bitlink-1164'><small><a href='javascript:show_bit(1164)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1164' style='display:none'><small><a href='javascript:hide_bit(1164)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Learning the Evolution of Unknown Systems via Deep Neural Networks</b><br />Victor Churchill<br />Tuesday, February 28 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-276.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Emerald Room<br /><br /><small>Many phenomena in science and engineering are observable but not yet explainable. That is, we can observe solution data generated from many physical systems, but the actual physics, e.g. an ordinary or partial differential equation model, are unknown. In this case, developing a deep neural network based model that replicates the system’s behavior is desirable. Hence in this talk, we’ll explore how to learn the time evolution of unknown ODE and PDE systems from their solution data using deep neural networks. The specific neural network architectures used are grounded in numerical methods for solving ODEs and PDEs. We also consider the case of partially observing the solution vector, where a time history of the observed variables are required.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75624' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1259' style='display:block'><a href='javascript:toggle_star(1259)' class='star'><span class='star1259'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (Emerald Room)</b> Marius Kurz, Reinforcement Learning for Discretization-Aware LES Models <span id='bitlink-1165'><small><a href='javascript:show_bit(1165)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1165' style='display:none'><small><a href='javascript:hide_bit(1165)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Reinforcement Learning for Discretization-Aware LES Models</b><br />Marius Kurz<br />Tuesday, February 28 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-276.html'>Machine Learning for Large-Scale Scientific Data Analysis and Model Simulations - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Emerald Room<br /><br /><small>Over the past few years, increasing efforts have been devoted to leveraging the recent advances in machine learning for the field of turbulence modeling. Most approaches in this field were based on Supervised Learning, for which artificial neural networks (ANN) are trained by means of a precomputed training dataset. However, for large eddy simulations (LES), this approach can cause instabilities in practical simulations, since the dynamics of the discretized equations are not captured by the training process. An approach which avoids this pitfall is the Reinforcement Learning (RL) paradigm, which, in contrast to Supervised Learning, trains ANN by interacting directly with the discretized dynamical system.  We demonstrate how the RL paradigm can be applied for turbulence modeling by presenting the novel RL framework Relexi, which couples flow solvers with the machine learning library TensorFlow, while leveraging modern high-performance computing resources. Relexi is applied to canonical flows, for which ANN are trained to adapt the coefficients of analytical LES models dynamically in space and time. We show that these data-driven LES models provide stable simulations, while outperforming traditional LES models in terms of accuracy. Thus, the proposed framework can provide a novel class of discretization-aware LES models, which can incorporate complex LES filter formulations and discretization effects by design.    
  	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75624' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
