<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Sparse Computations in Science and Engineering - Part II of II</h2><div class='index-talk' id='talk515' style='display:block'><a href='javascript:toggle_star(515)' class='star'><span class='star515'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (G107)</b> Helen Xu, Optimizing Locality in Dynamic Graph Data Structures <span id='bitlink-474'><small><a href='javascript:show_bit(474)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-474' style='display:none'><small><a href='javascript:hide_bit(474)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Optimizing Locality in Dynamic Graph Data Structures</b><br />Helen Xu<br />Wednesday, March 1 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-113.html'>Sparse Computations in Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>This talk discusses dynamic graph data structures and how to choose data structures that optimize for locality, which is key to performance in graph computations. The focus is on how different use cases, which can cause different data patterns and access patterns, can influence the design of these structures and how to exploit these differences to maximize performance.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75389' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk516' style='display:block'><a href='javascript:toggle_star(516)' class='star'><span class='star516'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (G107)</b> Mariya Ishteva, Decoupling Multivariate Functions <span id='bitlink-475'><small><a href='javascript:show_bit(475)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-475' style='display:none'><small><a href='javascript:hide_bit(475)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Decoupling Multivariate Functions</b><br />Mariya Ishteva<br />Wednesday, March 1 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-113.html'>Sparse Computations in Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>While linear functions are well-understood, for nonlinear (multivariate vector) functions it is unclear how to i) define their complexity, ii) reduce the complexity and iii) increase their interpretability.    
To answer these questions, we propose a decomposition of nonlinear functions [P. Dreesen, M. Ishteva, and J. Schoukens. Decoupling multivariate polynomials using first-order information and tensor decompositions. SIMAX, 36:864--879, 2015], which can be viewed as a generalization of the singular value decomposition. In this decomposition, univariate nonlinear mappings replace the simpler scaling performed by the singular values. We discuss the computation of the decomposition, which is based on tensor techniques. We also mention an application in nonlinear system identification.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75389' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk517' style='display:block'><a href='javascript:toggle_star(517)' class='star'><span class='star517'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (G107)</b> Vivek Bharadwaj, Sampling Algorithms for Distributed-Memory Sparse CP Decomposition <span id='bitlink-476'><small><a href='javascript:show_bit(476)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-476' style='display:none'><small><a href='javascript:hide_bit(476)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Sampling Algorithms for Distributed-Memory Sparse CP Decomposition</b><br />Vivek Bharadwaj<br />Wednesday, March 1 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-113.html'>Sparse Computations in Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>Low rank Candecomp / Parafac (CP) Tesnsor Decomposition is a powerful computational tool to extract patterns from sparse data, but its computational cost may become intractable for massive sparse tensors. Recently, a number of randomized sketching algorithms have been proposed to drive down the cost of Alternating Least Squares, a popular heuristic to compute the CP decomposition. We extend two algorithms based on statistical leverage-score sampling to the distributed-memory setting, where processor-to-processor communication overhead is a major obstacle to achieving the speedup enjoyed in a shared-memory system. We investigate multiple strategies to combat this problem by adapting techniques from distributed sparse matrix algorithms and produce high-performance implementations of these algorithms. Using NERSC Perlmutter, our communication-avoiding algorithms produce low-rank CP decompositions of billion-scale sparse tensors in seconds.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75389' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk518' style='display:block'><a href='javascript:toggle_star(518)' class='star'><span class='star518'>&star;</span></a> <b>2:50 PM&ndash;3:05 PM (G107)</b> Md Taufique Hussain, Incremental Graph Clustering in Parallel <span id='bitlink-477'><small><a href='javascript:show_bit(477)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-477' style='display:none'><small><a href='javascript:hide_bit(477)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Incremental Graph Clustering in Parallel</b><br />Md Taufique Hussain<br />Wednesday, March 1 2:50 PM&ndash;3:05 PM<br />This is the 4th talk in <a href='session-113.html'>Sparse Computations in Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>We develop a distributed memory graph clustering algorithm to find clusters in a graph where new nodes and edges are being added incrementally. At each stage of the algorithm, we maintain a summary of the clustered graph computed from all incremental batches received thus far. As we receive a new batch of nodes and edges, we cluster the new graph and merge new clusters with the previous summary clusters. We use sparse linear algebra to perform these operations. Our algorithm would make it possible to find clusters in very large graphs for which regular graph clustering algorithms could not run due to computation/communication bottlenecks. We use this algorithm to cluster billions of metagenomic proteins that are being collected incrementally in various databases.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75389' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk519' style='display:block'><a href='javascript:toggle_star(519)' class='star'><span class='star519'>&star;</span></a> <b>3:10 PM&ndash;3:25 PM (G107)</b> Brian Bantsoukissa, Towards a Modified Nested Dissection Ordering to Enhance Low-Rank Compressibility in Sparse Direct Solvers <span id='bitlink-478'><small><a href='javascript:show_bit(478)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-478' style='display:none'><small><a href='javascript:hide_bit(478)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Towards a Modified Nested Dissection Ordering to Enhance Low-Rank Compressibility in Sparse Direct Solvers</b><br />Brian Bantsoukissa<br />Wednesday, March 1 3:10 PM&ndash;3:25 PM<br />This is the 5th talk in <a href='session-113.html'>Sparse Computations in Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>The advent of rank-structured compression techniques for the solution of large-scale sparse linear systems has been demonstrated to significantly reduce both the computational cost and the memory footprint of sparse direct solvers. The block low-rank (BLR) compression format exploits the blockwise low-rank property of sparse matrices that arise in many scientific applications. In this talk, we investigate the potential of new combinatorial algorithms that enhance BLR solvers through the computation of a smarter ordering of the unknowns prior to the factorization phase. Our purely algebraic approach aims to bridge the gap between traditional dense and sparse linear algebra methods, and improve compression rates at the cost of additional fill-in. Instead of building low-rank clusters on top of nested dissection, we adopt an alternative approach by clustering the unknowns before exhibiting a nested dissection on top of it. We demonstrate the efficiency of this new approach by means of experiments that have been conducted for the PaStiX solver.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75389' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
