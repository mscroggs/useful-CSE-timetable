<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Deep Learning for Sequence Modeling - Part I of II</h2><div class='index-talk' id='talk881' style='display:block'><a href='javascript:toggle_star(881)' class='star'><span class='star881'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (E107)</b> Qianxiao Li, Approximation Theory of Deep Learning for Sequence Modelling <span id='bitlink-811'><small><a href='javascript:show_bit(811)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-811' style='display:none'><small><a href='javascript:hide_bit(811)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Approximation Theory of Deep Learning for Sequence Modelling</b><br />Qianxiao Li<br />Wednesday, March 1 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-193.html'>Deep Learning for Sequence Modeling - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />E107<br /><br /><small>In this talk, we present some recent results on the approximation theory of deep learning architectures for sequence modelling. In particular, we formulate a basic mathematical framework, under which different popular architectures such as recurrent neural networks, dilated convolutional networks (e.g. WaveNet), encoder-decoder structures can be rigorously compared. These analyses reveal some interesting connections between approximation, memory, sparsity and low rank phenomena that may guide the practical selection and design of these network architectures.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75499' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk882' style='display:block'><a href='javascript:toggle_star(882)' class='star'><span class='star882'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (E107)</b> Romit Maulik, Stabilized Neural Ordinary Differential Equations for Long-Time Forecasting of Dynamical Systems <span id='bitlink-812'><small><a href='javascript:show_bit(812)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-812' style='display:none'><small><a href='javascript:hide_bit(812)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Stabilized Neural Ordinary Differential Equations for Long-Time Forecasting of Dynamical Systems</b><br />Romit Maulik<br />Wednesday, March 1 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-193.html'>Deep Learning for Sequence Modeling - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />E107<br /><br /><small>In data-driven modeling of spatiotemporal phenomena careful consideration often needs to be made in capturing the dynamics of the high wavenumbers. This problem becomes especially challenging when the system of interest exhibits shocks or chaotic dynamics. We present a data-driven modeling method that accurately captures shocks and chaotic dynamics by proposing a novel architecture, the stabilized neural ordinary differential equation (ODE). Here we learn the right hand side of an ODE by adding the outputs of two neural networks (NN) together where one learns a linear term and the other a nonlinear term. Specifically, we implement this by training a sparse linear convolutional NN to learn the linear term and a dense fully-connected nonlinear NN to learn the nonlinear term. This is in contrast with the standard neural ODE which involves training only a single NN for learning the RHS. Our method is applied for learning the viscous Burgers and Kuramoto-Sivashinsky equations where we outperform the standard variant not only in terms of metrics but also in terms of the capture of key physical characteristics such as discontinuities in the former and chaotic invariant manifolds in the latter.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75499' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk884' style='display:block'><a href='javascript:toggle_star(884)' class='star'><span class='star884'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (E107)</b> Konstantin Rusch, Physics-Inspired Machine Learning for Sequence Modeling <span id='bitlink-813'><small><a href='javascript:show_bit(813)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-813' style='display:none'><small><a href='javascript:hide_bit(813)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Physics-Inspired Machine Learning for Sequence Modeling</b><br />Konstantin Rusch<br />Wednesday, March 1 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-193.html'>Deep Learning for Sequence Modeling - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />E107<br /><br /><small>Combining physics with machine learning is a rapidly growing field of research. Thereby, most work focuses on leveraging machine learning methods to solve problems in physics. Here, however, we focus on the reverse direction of leveraging structure of physical systems (e.g. dynamical systems modeled by ODEs or PDEs) to construct novel machine learning algorithms, where the existence of highly desirable properties of the underlying method can be rigorously proved. In particular, we propose several physics-inspired deep learning architectures for sequence modelling based on coupled oscillators, Hamiltonian systems and multi-scale dynamical systems. The proposed architectures address central problems in the field of recurrent sequence modeling, namely the vanishing and exploding gradients problem as well as the issue of limited expressive power. Finally, we show that this leads to state-of-the-art performance on several widely used benchmark problems ranging from image recognition over speech recognition to NLP applications.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75499' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk885' style='display:block'><a href='javascript:toggle_star(885)' class='star'><span class='star885'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (E107)</b> Philipp Schmocker, Universal Approximation of Random Neural Networks <span id='bitlink-814'><small><a href='javascript:show_bit(814)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-814' style='display:none'><small><a href='javascript:hide_bit(814)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Universal Approximation of Random Neural Networks</b><br />Philipp Schmocker<br />Wednesday, March 1 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-193.html'>Deep Learning for Sequence Modeling - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />E107<br /><br /><small>In this paper, we study single-layer feedforward neural networks with randomly initialized weight matrices and bias vectors, which is inspired by the works on extreme learning machines, random feature models, and reservoir computing. In this case, only the linear readout needs to be trained, which can be performed, e.g., by a linear regression. Despite the popularity of this approach in empirical tasks, only little is known about the approximation capabilities of such networks. By considering these so-called &#8220;random neural networks' as Banach space-valued random variables, we provide several universal approximation theorems within the underlying Bochner space. Moreover, we extend the results to more general function spaces such as $L^p$-spaces and also consider the simultaneous approximation including the derivatives, which can be used for the approximation in Sobolev spaces.    
	    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75499' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
