<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Efficient Numerical Linear Algebra Methods for Gaussian Processes</h2><div class='index-talk' id='talk422' style='display:block'><a href='javascript:toggle_star(422)' class='star'><span class='star422'>&star;</span></a> <b>4:45 PM&ndash;5:00 PM (D406)</b> Jonathan Wenger, Preconditioning for Scalable Gaussian Process Hyperparameter Optimization <span id='bitlink-390'><small><a href='javascript:show_bit(390)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-390' style='display:none'><small><a href='javascript:hide_bit(390)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Preconditioning for Scalable Gaussian Process Hyperparameter Optimization</b><br />Jonathan Wenger<br />Thursday, March 2 4:45 PM&ndash;5:00 PM<br />This is the 1st talk in <a href='session-94.html'>Efficient Numerical Linear Algebra Methods for Gaussian Processes</a> (4:45 PM&ndash;6:25 PM)<br />D406<br /><br /><small>Gaussian process hyperparameter optimization requires linear solves with, and log-determinants of, large kernel matrices. Iterative numerical techniques are becoming popular to scale to larger datasets, relying on the conjugate gradient method (CG) for the linear solves and stochastic trace estimation for the log-determinant. This work introduces new algorithmic and theoretical insights for preconditioning these computations. While preconditioning is well understood in the context of CG, we demonstrate that it can also accelerate convergence and reduce variance of the estimates for the log-determinant and its derivative. We prove general probabilistic error bounds for the preconditioned computation of the log-determinant, log-marginal likelihood and its derivatives. Additionally, we derive specific rates for a range of kernel-preconditioner combinations, showing that up to exponential convergence can be achieved. Our theoretical results enable provably efficient optimization of kernel hyperparameters, which we validate empirically on large-scale benchmark problems. There our approach accelerates training by up to an order of magnitude.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75365' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk423' style='display:block'><a href='javascript:toggle_star(423)' class='star'><span class='star423'>&star;</span></a> <b>5:05 PM&ndash;5:20 PM (D406)</b> Yuanzhe Xi, Adaptive Factorized Nystrom Preconditioner for Gaussian Kernel Matrices <span id='bitlink-391'><small><a href='javascript:show_bit(391)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-391' style='display:none'><small><a href='javascript:hide_bit(391)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Adaptive Factorized Nystrom Preconditioner for Gaussian Kernel Matrices</b><br />Yuanzhe Xi<br />Thursday, March 2 5:05 PM&ndash;5:20 PM<br />This is the 2nd talk in <a href='session-94.html'>Efficient Numerical Linear Algebra Methods for Gaussian Processes</a> (4:45 PM&ndash;6:25 PM)<br />D406<br /><br /><small>The spectrum of a kernel matrix significantly depends on the parameter  values of the kernel function used to define the kernel matrix.  This makes it challenging to design a preconditioner for a regularized  kernel matrix that is robust across different parameter values.  In this talk, we present the Adaptive Factorized Nystr\"om  (AFN) preconditioner.  The preconditioner is designed for  the case where the rank $k$ of the Nystr\"om approximation  is large, i.e., for kernel function parameters that lead to kernel  matrices with eigenvalues that decay slowly.  Other Nystr\"om preconditioners for a regularized kernel matrix use the  Sherman--Morrison--Woodbury (SMW) formula and must solve with or  compute an eigendecomposition of a $k$-by-$k$ matrix.  In contrast, AFN is a factorized form that avoids  the SMW formula.  It deliberately chooses a well-conditioned $k$-by-$k$  matrix to solve with that has a sparse approximate inverse. This makes  AFN efficient for large $k$.  AFN also adaptively  chooses $k$ to balance accuracy and cost.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75365' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk424' style='display:block'><a href='javascript:toggle_star(424)' class='star'><span class='star424'>&star;</span></a> <b>5:25 PM&ndash;5:40 PM (D406)</b> Xinran Zhu, Efficient and Scalable Stochastic Variational Gaussian Processes <span id='bitlink-392'><small><a href='javascript:show_bit(392)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-392' style='display:none'><small><a href='javascript:hide_bit(392)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Efficient and Scalable Stochastic Variational Gaussian Processes</b><br />Xinran Zhu<br />Thursday, March 2 5:25 PM&ndash;5:40 PM<br />This is the 3rd talk in <a href='session-94.html'>Efficient Numerical Linear Algebra Methods for Gaussian Processes</a> (4:45 PM&ndash;6:25 PM)<br />D406<br /><br /><small>Gaussian processes (GP) are a popular probabilistic learning framework, especially when inference with uncertainty estimation is necessary. Oftentimes, GPs with derivative information can be useful in many settings where derivative information is available, including numerous Bayesian optimization and regression tasks that arise in the natural sciences. Given the $\mathcal{O}(n^3)$ cost in computation for $n$ training points for GPs, stochastic variational Gaussian processes (SVGP) scale GP inference to large datasets through inducing points and stochastic training. However, the SVGP training process involves hard multimodal optimization, and often suffers from slow and suboptimal convergence when initializing inducing points directly from training data. Moreover, SVGP scales poorly with the number of dimensions when derivative information is incorporated. We split this presentation into two parts to focus on the two issues. In the first part, we discuss an efficient initialization of SVGP to make the training more efficient and in the second part we discuss a fully scalable SVGP method with derivatives, the training cost of which is independent of both the number of training data and the number of input dimensions.     
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75365' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk425' style='display:block'><a href='javascript:toggle_star(425)' class='star'><span class='star425'>&star;</span></a> <b>5:45 PM&ndash;6:00 PM (D406)</b> Max Pfeffer, Low-Rank Tensor Methods for High-Dimensional Gaussian Processes <span id='bitlink-393'><small><a href='javascript:show_bit(393)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-393' style='display:none'><small><a href='javascript:hide_bit(393)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Low-Rank Tensor Methods for High-Dimensional Gaussian Processes</b><br />Max Pfeffer<br />Thursday, March 2 5:45 PM&ndash;6:00 PM<br />This is the 4th talk in <a href='session-94.html'>Efficient Numerical Linear Algebra Methods for Gaussian Processes</a> (4:45 PM&ndash;6:25 PM)<br />D406<br /><br /><small>We consider the special case of Gaussian process kernel learning where the covariance function is given by a sum of products of RBF kernels. For a given dataset, the parameters of the kernel are learned by minimizing the log marginal likelihood. Computing the log-determinant of the covariance matrix is prohibitive for large datasets or in high dimensions unless one exploits its low-rank tensor structure. We employ a stochastic trace estimation together with a Lanczos algorithm for TT-tensors (Tensor Trains). This allows us to break the curse of dimensionality and to perform a gradient-based optimization even in high dimensions.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75365' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
