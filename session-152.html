<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Randomized Numerical Algorithms for Matrix and Tensor Analysis - Part II of II</h2><div class='index-talk' id='talk690' style='display:block'><a href='javascript:toggle_star(690)' class='star'><span class='star690'>&star;</span></a> <b>11:30 AM&ndash;11:45 AM (G105)</b> Elizabeth Yang, Capacity Analysis of Vector Symbolic Architectures <span id='bitlink-642'><small><a href='javascript:show_bit(642)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-642' style='display:none'><small><a href='javascript:hide_bit(642)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Capacity Analysis of Vector Symbolic Architectures</b><br />Elizabeth Yang<br />Friday, March 3 11:30 AM&ndash;11:45 AM<br />This is the 1st talk in <a href='session-152.html'>Randomized Numerical Algorithms for Matrix and Tensor Analysis - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G105<br /><br /><small>Hyperdimensional computing (HDC) is a biologically-inspired framework that uses high-dimensional vectors and various vector operations to represent and manipulate symbols. The ensemble of a particular vector space and two choice vector operations (one addition-like for "bundling" and one outer-product-like for "binding") form what we call a "vector symbolic architecture" (VSA). While VSAs have been employed in numerous applications and studied empirically, many theoretical questions about VSAs remain open.    
We provide theoretical analyses for the representation capacities of two popular VSAs: MAP-I and MAP-B. Representation capacity refers to lower bounds on the dimensions of the VSA vectors required to perform certain symbolic tasks (such as testing for set membership and estimating set intersection sizes) to a sufficient degree of accuracy. Our analysis of MAP-I also demonstrates a connection between VSAs and sketching (dimensionality reduction) algorithms such as the Johnson-Lindenstrauss transform. Time permitting, we will also describe a relationship between the MAP-I VSA to Hopfield networks, which are simple models for associative memory.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75447' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk691' style='display:block'><a href='javascript:toggle_star(691)' class='star'><span class='star691'>&star;</span></a> <b>11:50 AM&ndash;12:05 AM (G105)</b> Alex Gittens, Acceleration of Randomized Tensor Decompositions <span id='bitlink-643'><small><a href='javascript:show_bit(643)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-643' style='display:none'><small><a href='javascript:hide_bit(643)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Acceleration of Randomized Tensor Decompositions</b><br />Alex Gittens<br />Friday, March 3 11:50 AM&ndash;12:05 AM<br />This is the 2nd talk in <a href='session-152.html'>Randomized Numerical Algorithms for Matrix and Tensor Analysis - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G105<br /><br /><small>In this talk, we discuss the application of acceleration methods to randomized tensor low rank approximations (LRAs). Randomized tensor LRA algorithms have been proposed to address the need for efficient decompositions algorithms for large dense tensors; such algorithms rely on stochastic first-order or second-order methods. Acceleration methods have been designed and applied for general first-order optimization problems. We theoretically and empirically investigate the advantages of combining these two lines of work.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75447' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk692' style='display:block'><a href='javascript:toggle_star(692)' class='star'><span class='star692'>&star;</span></a> <b>12:10 AM&ndash;12:25 AM (G105)</b> Edgar Solomonik, Faster Accurate Sketching for Tensor Networks <span id='bitlink-644'><small><a href='javascript:show_bit(644)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-644' style='display:none'><small><a href='javascript:hide_bit(644)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Faster Accurate Sketching for Tensor Networks</b><br />Edgar Solomonik<br />Friday, March 3 12:10 AM&ndash;12:25 AM<br />This is the 3rd talk in <a href='session-152.html'>Randomized Numerical Algorithms for Matrix and Tensor Analysis - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G105<br /><br /><small>Linear sketches with tensor product structure provide an effective tool for accelerating decomposition of tensors. We describe a new approach for inexact optimization of Tucker decomposition via linear sketching (arXiv:2104.01101). Then, we propose efficient a general scheme for constructing computationally-efficient sketches of tensor network inputs with arbitrary structure (arXiv:2205.13163). We provide bounds on the complexity and accuracy of both schemes and show that the general scheme is optimal or near-optimal among possible tensor networks consisting of Gaussian tensors.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75447' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk693' style='display:block'><a href='javascript:toggle_star(693)' class='star'><span class='star693'>&star;</span></a> <b>12:30 AM&ndash;12:45 AM (G105)</b> Aleksandros Sobczyk, Approximate Euclidean Lengths and Distances Beyond Johnson-Lindenstrauss <span id='bitlink-645'><small><a href='javascript:show_bit(645)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-645' style='display:none'><small><a href='javascript:hide_bit(645)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Approximate Euclidean Lengths and Distances Beyond Johnson-Lindenstrauss</b><br />Aleksandros Sobczyk<br />Friday, March 3 12:30 AM&ndash;12:45 AM<br />This is the 4th talk in <a href='session-152.html'>Randomized Numerical Algorithms for Matrix and Tensor Analysis - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G105<br /><br /><small>In this presentation we will dive in the core of Randomized Numerical Linear Algebra and present out recent results  (Sobczyk and Luisier, NeurIPS 2022) related to the Johnson-Lindenstrauss (JL) lemma and its applications. We revisit a key problem: how to approximate the Euclidean lengths (or distances) of a set of high-dimensional vectors. Can we do better than JL? Based on techniques which are inspired by the recently published Hutch++ algorithm for trace estimation, we proposed algorithms which can (provably) improve the standard JL bounds, both in theory and in practice. We will describe all these algorithms and sketch the proof techniques. These improvements can ultimately lead to more practical and efficient randomized algorithms for scientific computing, especially when high accuracy is required, which is a known shortcoming of JL-based random projections. We will also discuss how these results can be applied to other problems, namely to compute Euclidean distances between high-dimensional data points, to estimate the statistical leverage scores of a matrix, and to approximate the charge densities in a quantum mechanical system.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75447' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
