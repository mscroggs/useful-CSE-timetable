<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Optimization, Design, and Machine Learning in the FASTMath SciDAC-5 Institute - Part II of II</h2><div class='index-talk' id='talk368' style='display:block'><a href='javascript:toggle_star(368)' class='star'><span class='star368'>&star;</span></a> <b>9:20 AM&ndash;9:35 AM (E103)</b> Feng Bao, Stochastic Methods for Machine Learning <span id='bitlink-338'><small><a href='javascript:show_bit(338)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-338' style='display:none'><small><a href='javascript:hide_bit(338)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Stochastic Methods for Machine Learning</b><br />Feng Bao<br />Friday, March 3 9:20 AM&ndash;9:35 AM<br />This is the 1st talk in <a href='session-82.html'>Optimization, Design, and Machine Learning in the FASTMath SciDAC-5 Institute - Part II of II</a> (9:20 AM&ndash;11:00 AM)<br />E103<br /><br /><small>We develop a backward stochastic differential equation based probabilistic machine learning method, which formulates a class of stochastic neural networks as a stochastic optimal control problem. An efficient stochastic gradient descent algorithm is introduced with the gradient computed through a backward stochastic differential equation. Convergence analysis for stochastic gradient descent optimization and numerical experiments for applications of stochastic neural networks are carried out to validate our methodology in both theory and performance.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75340' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk369' style='display:block'><a href='javascript:toggle_star(369)' class='star'><span class='star369'>&star;</span></a> <b>9:40 AM&ndash;9:55 AM (E103)</b> Hong Zhang, Sann: Stiffness-Aware Neural Network for Learning Hamiltonian Systems <span id='bitlink-339'><small><a href='javascript:show_bit(339)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-339' style='display:none'><small><a href='javascript:hide_bit(339)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Sann: Stiffness-Aware Neural Network for Learning Hamiltonian Systems</b><br />Hong Zhang<br />Friday, March 3 9:40 AM&ndash;9:55 AM<br />This is the 2nd talk in <a href='session-82.html'>Optimization, Design, and Machine Learning in the FASTMath SciDAC-5 Institute - Part II of II</a> (9:20 AM&ndash;11:00 AM)<br />E103<br /><br /><small>In this talk, I will present stiffness-aware neural network (SANN), a new method for learning Hamiltonian dynamical systems from data. SANN identifies and splits the training data into stiff and nonstiff portions based on a stiffness-aware index, a simple, yet effective metric that can be used to quantify the stiffness of the dynamical system. This classification along with a resampling technique allows one to apply different time integration strategies such as step size adaptation to better capture the dynamical characteristics of the Hamiltonian vector fields. SANN has been evaluated using complex physical systems including a three-body problem and billiard model. The results show that SANN is more stable and can better preserve energy when compared with the state-of-the-art methods, leading to significant improvement in accuracy.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75340' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk370' style='display:block'><a href='javascript:toggle_star(370)' class='star'><span class='star370'>&star;</span></a> <b>10:00 AM&ndash;10:15 AM (E103)</b> Emil M. Constantinescu, Representing Subgrid-scale Models with Neural Ordinary Differential Equations <span id='bitlink-340'><small><a href='javascript:show_bit(340)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-340' style='display:none'><small><a href='javascript:hide_bit(340)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Representing Subgrid-scale Models with Neural Ordinary Differential Equations</b><br />Emil M. Constantinescu<br />Friday, March 3 10:00 AM&ndash;10:15 AM<br />This is the 3rd talk in <a href='session-82.html'>Optimization, Design, and Machine Learning in the FASTMath SciDAC-5 Institute - Part II of II</a> (9:20 AM&ndash;11:00 AM)<br />E103<br /><br /><small>We explore a new approach to learning the subgrid-scale model effects when simulating partial differential equations (PDEs) solved by the method of lines and their representation in chaotic ordinary differential equations based on neural ordinary differential equations (NODEs).   Solving systems with fine temporal and spatial grid scales is computationally challenging, and closure models are generally difficult to tune. We propose a machine-learning strategy to represent the coarse- to fine-grid map, which can be viewed as subgrid-scale parameterization. Our approach is based on NODEs and partial knowledge of the coarse system; therefore, we learn the source dynamics operator. Our method inherits the advantages of NODEs and can be used to parameterize subgrid scales, approximate coupling operators, and improve the efficiency of low-order solvers. Numerical results are used to illustrate this approach using the two-scale Lorenz 96 equation, the convection-diffusion equation, and the Navier--Stokes equations.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75340' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk371' style='display:block'><a href='javascript:toggle_star(371)' class='star'><span class='star371'>&star;</span></a> <b>10:20 AM&ndash;10:35 AM (E103)</b> Tiernan Casey, Reduced-Order Neural Operator Surrogates for Dynamical System Models <span id='bitlink-341'><small><a href='javascript:show_bit(341)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-341' style='display:none'><small><a href='javascript:hide_bit(341)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Reduced-Order Neural Operator Surrogates for Dynamical System Models</b><br />Tiernan Casey<br />Friday, March 3 10:20 AM&ndash;10:35 AM<br />This is the 4th talk in <a href='session-82.html'>Optimization, Design, and Machine Learning in the FASTMath SciDAC-5 Institute - Part II of II</a> (9:20 AM&ndash;11:00 AM)<br />E103<br /><br /><small>A primary challenge in achieving high accuracy in simulations of physical systems is the expense of resolving the high dimensional and multiscale nature of dynamical interactions. In order to make many-query predictive simulation of such systems tractable, as required for e.g. design optimization and uncertainty quantification efforts, accurate surrogate models are typically required. In this work we employ neural operator surrogates using the deep operator network (DeepONet) architecture to construct surrogates for the full temporal solution of chemical dynamical systems models arising in plasma and reacting flow applications.  In particular we explore the opportunity for constructing such surrogates on reduced dimensional representations of the chemical species state, propose network augmentations to overcome expense associated with temporal symmetries arising in the training data, and investigate user-defined temporal modes for accelerating network training. Results are presented for surrogates applied to spatially homogeneous chemistry models relevant to CFD solvers for reacting flows and low-temperature fluid plasmas.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75340' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
