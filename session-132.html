<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Recent Advances in Modeling, Simulation, and Sampling via Artificial Intelligence for Physical Sciences and Engineering - Part I of II</h2><div class='index-talk' id='talk601' style='display:block'><a href='javascript:toggle_star(601)' class='star'><span class='star601'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (G109)</b> Ling Guo, Mutual Information Based Uncertainty Quantification in Scientific Machine Learning <span id='bitlink-559'><small><a href='javascript:show_bit(559)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-559' style='display:none'><small><a href='javascript:hide_bit(559)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Mutual Information Based Uncertainty Quantification in Scientific Machine Learning</b><br />Ling Guo<br />Monday, February 27 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-132.html'>Recent Advances in Modeling, Simulation, and Sampling via Artificial Intelligence for Physical Sciences and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G109<br /><br /><small>Neural networks (NNs) are currently changing the computational paradigm on how to combine data with mathematical laws in physics and engineering in a profound way, tackling challenging inverse and ill-posed problems not solvable with traditional methods. However, quantifying errors and uncertainties in NN-based inference is more complicated than in traditional methods. We have presented a comprehensive framework that includes uncertainty modeling, new and existing solution methods, as well as evaluation metrics and post-hoc improvement approaches in the review work Uncertainty Quantification in Scientific Machine Learning: Methods, Metrics, and Comparisons. In this talk, we will present a new approach for UQ in scientific machine learning based on the mutual information theory. Some numerical examples are tested to demonstrate the applicability and reliability of the new method.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75420' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk602' style='display:block'><a href='javascript:toggle_star(602)' class='star'><span class='star602'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (G109)</b> Rene Lohmann, Posterior Sampling Methods with Stochastic Gradients <span id='bitlink-560'><small><a href='javascript:show_bit(560)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-560' style='display:none'><small><a href='javascript:hide_bit(560)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Posterior Sampling Methods with Stochastic Gradients</b><br />Rene Lohmann<br />Monday, February 27 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-132.html'>Recent Advances in Modeling, Simulation, and Sampling via Artificial Intelligence for Physical Sciences and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G109<br /><br /><small>I will discuss the usage of stochastic gradients in sampling methods for Bayesian statistical models.  When sampling from Bayesian posterior distributions, widely used schemes such as Langevin Dynamics or Hamiltonian Monte Carlo quickly become unfeasible as the data sets (and models) grow. This is because their dynamics is governed by the gradient of the log-posterior, whose computational complexity is linear in the size of the data set. I will present some work on using stochastic gradients in the samplers, obtained by data set subsampling, in order to decrease computational cost (an approach that is well-known from the neighbouring field of loss function optimization in neural network training). In particular, I will focus on the use of Metropolis correction of the bias introduced by gradient noise and highlight efficiency related issues.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75420' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk603' style='display:block'><a href='javascript:toggle_star(603)' class='star'><span class='star603'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (G109)</b> Andrew Gillette, Adaptive Marking for Adaptive Mesh Refinement via Reinforcement Learning <span id='bitlink-561'><small><a href='javascript:show_bit(561)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-561' style='display:none'><small><a href='javascript:hide_bit(561)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Adaptive Marking for Adaptive Mesh Refinement via Reinforcement Learning</b><br />Andrew Gillette<br />Monday, February 27 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-132.html'>Recent Advances in Modeling, Simulation, and Sampling via Artificial Intelligence for Physical Sciences and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G109<br /><br /><small>Adaptive mesh refinement in the context of adaptive finite element methods (AFEM) requires an often overlooked and time-consuming offline effort: tuning parameters that control which elements are marked for refinement. To automate this effort and improve efficiency of adaptive methods, we recast adaptive mesh refinement as a partially-observed Markov decision process that can be optimized using methods from reinforcement learning. This recasting delivers a tractable optimization framework that can be tuned once by automated training and then deployed successfully in contexts outside the training regime. We use the Poisson equation to demonstrate various applications of our framework including representative h- and hp-refinement AFEM simulations on non-convex polyhedra. Our experiments indicate that superior marking policies remain undiscovered for many canonical AFEM applications.  An unexpected observation is that marking policies trained on one family of PDEs can be robust enough to perform well on problems far outside the training family. For instance, we show that a simple hp-refinement policy optimized on 2D problems can be used for 3D problems without significant performance loss.  Extensions of these ideas to transient PDEs and multi-objective optimization frameworks will also be discussed.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75420' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk604' style='display:block'><a href='javascript:toggle_star(604)' class='star'><span class='star604'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (G109)</b> Mengwu Guo, Bayesian Learning of Reduced-Order Dynamics <span id='bitlink-562'><small><a href='javascript:show_bit(562)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-562' style='display:none'><small><a href='javascript:hide_bit(562)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Bayesian Learning of Reduced-Order Dynamics</b><br />Mengwu Guo<br />Monday, February 27 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-132.html'>Recent Advances in Modeling, Simulation, and Sampling via Artificial Intelligence for Physical Sciences and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G109<br /><br /><small>Two probabilistic methods for the learning of reduced-order dynamics will be discussed. The first method is the Bayesian reduced-order operator inference, a non-intrusive, ‘glass-box’ approach that inherits the formulation structure of projection-based, reduced-state governing equations yet without requiring access to the full-order solvers. The reduced-order operators are learned using Bayesian inference with Gaussian priors and recovered as posterior Gaussian distributions conditioning on projected state data, which provides a quantification of modeling uncertainties and a naturally embedded Tikhonov regularization. The second method employs deep kernel learning, a manifold Gaussian process with a deep neural network embedded inside, for the data-driven reduced-order modeling from high-dimensional measurements given by noise-corrupted images. Such a probabilistic deep learning model is utilized for both dimensionality reduction and the representation of reduced-order dynamics. Numerical results show the effectiveness of deep kernel learning in the denoising and uncertainty quantification of reduced models. The first method is a joint work with S. A. McQuarrie and K. E. Willcox (UT Austin), while the second with N. Botteghi and C. Brune (UTwente).    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75420' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk605' style='display:block'><a href='javascript:toggle_star(605)' class='star'><span class='star605'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (G109)</b> Simone Ciarella, Solving the Integro-Differential Equation of Supercooled Liquid Dynamics Using Machine Learning <span id='bitlink-563'><small><a href='javascript:show_bit(563)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-563' style='display:none'><small><a href='javascript:hide_bit(563)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Solving the Integro-Differential Equation of Supercooled Liquid Dynamics Using Machine Learning</b><br />Simone Ciarella<br />Monday, February 27 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-132.html'>Recent Advances in Modeling, Simulation, and Sampling via Artificial Intelligence for Physical Sciences and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G109<br /><br /><small>We introduce a machine-learning approach to predict the complex non-Markovian dynamics of supercooled liquids from their static averaged quantities. Our method is based on a theoretical framework that uses as input and output system-averaged quantities and descibe the dynamics as an integro-differential equation. Compared to a particle resolved approach, our methd is easier to apply in an experimental context where particle specific information is not available. First, using a deep neural network we predict the self intermediate scattering function of the binary mixtures we investigate. While its performances are excellent when training data across all the temperature range are available, we also show that the model retains some transferability being able to make decent predictions at temperatures lower than the one it was trained for, or when we use it for similar systems. Then, we develop an evolutionary strategy that is able to predict the elusive memory function underlying the integro-differential equation describing the observed dynamics. This method, which is much easier than any Laplace inversion of the memory equation, lets us conclude that the memory function of supercooled liquids can be effectively parameterized as the sum of two stretched exponentials, which physically corresponds to two dominant relaxation modes.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75420' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
