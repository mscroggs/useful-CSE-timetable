<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Recent Advances in Combinatorial Scientific Computing - Part II of II</h2><div class='index-talk' id='talk1403' style='display:block'><a href='javascript:toggle_star(1403)' class='star'><span class='star1403'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (D508)</b> Simon M&#228;rtens, The Chain Rule of Differentiation Is Associative - So What? <span id='bitlink-1290'><small><a href='javascript:show_bit(1290)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1290' style='display:none'><small><a href='javascript:hide_bit(1290)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>The Chain Rule of Differentiation Is Associative - So What?</b><br />Simon M&#228;rtens<br />Monday, February 27 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-309.html'>Recent Advances in Combinatorial Scientific Computing - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />D508<br /><br /><small>The Chain Rule of Differentiation is Associative - So What?     
... well, feasibility of backpropagation for training artificial neural  networks by some variant of stochastic gradient descent is one  rather obvious consequence. Adjoint algorithmic differentiation (AD)  of numerical simulation programs enabling large-scale error control,  uncertainty quantification or nonlinear optimization turns out to be  the more general concept. Resulting challenges are discussed briefly.    
... in fact, several combinatorial optimization problems arise when  aiming to design a near-optimal AD algorithm for a given differentiable  program. We discuss the main challenges and ideas behind AD mission  planning.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75697' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1405' style='display:block'><a href='javascript:toggle_star(1405)' class='star'><span class='star1405'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (D508)</b> Johannes Langguth, ML Accelerator Hardware: A New Model For Parallel Sparse Computations? <span id='bitlink-1291'><small><a href='javascript:show_bit(1291)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1291' style='display:none'><small><a href='javascript:hide_bit(1291)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>ML Accelerator Hardware: A New Model For Parallel Sparse Computations?</b><br />Johannes Langguth<br />Monday, February 27 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-309.html'>Recent Advances in Combinatorial Scientific Computing - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />D508<br /><br /><small>A major recent development in computer hardware was the rise of dedicated accelerator hardware for machine learning applications such as the Graphcore IPUs and Cerebras WSE. These processors have evolved from the experimental state into market-ready products, and they have the potential to constitute the next major architectural shift after GPUs saw widespread adoption a decade ago.     
In this talk we will present the new hardware and discuss the programming techniques that are required to unlock their potential. We present implementations of basic graph and matrix algorithms and show early results on the attainable performance, as well as comparisons to other architectures. We follow up by discussing the wider implications of the architecture for algorithm design and programming, along with the wider implications of adopting such hardware.     
    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75697' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1406' style='display:block'><a href='javascript:toggle_star(1406)' class='star'><span class='star1406'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (D508)</b> Somesh Singh, High Performance Sparse Tensor Contractions <span id='bitlink-1292'><small><a href='javascript:show_bit(1292)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1292' style='display:none'><small><a href='javascript:hide_bit(1292)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>High Performance Sparse Tensor Contractions</b><br />Somesh Singh<br />Monday, February 27 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-309.html'>Recent Advances in Combinatorial Scientific Computing - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />D508<br /><br /><small>Tensors are widely encountered in several application domains such as scientific computing, machine learning and data analytics.  Tensor contraction is a key algebraic operation in many applications involving multi-dimensional data. It is a higher dimensional analog of matrix-matrix multiplication.   Sparse tensor contraction suffers from poor data-locality and irregular accesses, which poses a significant performance challenge.   We explore the use of hashing-based methods to make sparse tensor contraction operation more efficient and performant on shared-memory systems.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75697' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
