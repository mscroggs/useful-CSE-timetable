<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Randomized Linear Algebra in Scientific Computing - Part I of II</h2><div class='index-talk' id='talk0' style='display:block'><a href='javascript:toggle_star(0)' class='star'><span class='star0'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D403)</b> Daniel Kressner, An Overview of Randomized Linear Algebra in Scientific Computing <span id='bitlink-0'><small><a href='javascript:show_bit(0)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-0' style='display:none'><small><a href='javascript:hide_bit(0)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>An Overview of Randomized Linear Algebra in Scientific Computing</b><br />Daniel Kressner<br />Wednesday, March 1 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-0.html'>Randomized Linear Algebra in Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>This talk gives an overview of the potential offered by randomization to speed up, improve the robustness, and increase the flexibility of numerical algorithms for solving large-scale linear algebra tasks in scientific computing. For more than a decade, randomization has proven its effectiveness in performing low-rank approximation and related tasks. Recently, the scope of randomization has been extended to essentially every part of numerical linear algebra, including linear systems, eigenvalue problems, model reduction, and matrix functions.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75152' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1' style='display:block'><a href='javascript:toggle_star(1)' class='star'><span class='star1'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D403)</b> Gunnar Martinsson, Randomized Compression Algorithms for Rank Structured Matrices <span id='bitlink-1'><small><a href='javascript:show_bit(1)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1' style='display:none'><small><a href='javascript:hide_bit(1)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Randomized Compression Algorithms for Rank Structured Matrices</b><br />Gunnar Martinsson<br />Wednesday, March 1 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-0.html'>Randomized Linear Algebra in Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>The talk describes a set of recently developed randomized algorithms for computing a data sparse representation of a rank structured matrices (such as an H-matrix, or an HSS matrix). The algorithms are black box in the sense that they interact with the matrix to be compressed only through its action on vectors, making them ideal for tasks such as forming Schur complements or matrix matrix multiplication. In situations where the operator to be compressed (and its transpose) can be applied in O(N) operations, the compression as a whole does in many environments have linear complexity as well.    
    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75152' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk2' style='display:block'><a href='javascript:toggle_star(2)' class='star'><span class='star2'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D403)</b> Nicolas Boulle, Learning Green's Functions with Randomized Numerical Linear Algebra <span id='bitlink-2'><small><a href='javascript:show_bit(2)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-2' style='display:none'><small><a href='javascript:hide_bit(2)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Learning Green's Functions with Randomized Numerical Linear Algebra</b><br />Nicolas Boulle<br />Wednesday, March 1 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-0.html'>Randomized Linear Algebra in Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>Can one learn a differential operator from pairs of solutions and righthand sides? If so, how many pairs are required? These two questions have received significant research attention in differential equation learning. Given input-output pairs from an unknown partial differential equation, we will derive a theoretically rigorous scheme for learning the associated Green's function G. By exploiting the hierarchical low-rank structure of Green’s functions and extending the randomized SVD algorithm to Hilbert-Schmidt operators, we will identify a learning rate associated with elliptic and parabolic partial differential operators and bound the number of input-output training pairs required to recover a Green’s function approximately.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75152' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk3' style='display:block'><a href='javascript:toggle_star(3)' class='star'><span class='star3'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D403)</b> Jonathan Weare, Convergence of a Jacobi Iteration with Repeated Random Sparsification <span id='bitlink-3'><small><a href='javascript:show_bit(3)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-3' style='display:none'><small><a href='javascript:hide_bit(3)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Convergence of a Jacobi Iteration with Repeated Random Sparsification</b><br />Jonathan Weare<br />Wednesday, March 1 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-0.html'>Randomized Linear Algebra in Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>The traditional methods of numerical linear algebra are prohibitively expensive for high-dimensional problems for which even a single matrix multiplication by a dense vector may be too costly.  In this talk I will discuss a general framework for reducing the cost of classical iterative schemes like Jacobi iteration by randomly sparsifying the approximate solution at each iteration.  I will provide a characterization of the error properties of Jacobi iteration with repeated random sparsification and show results of numerical tests applying the scheme to coupled cluster quantum chemistry calculations.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75152' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
