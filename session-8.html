<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Scaling Data Science, AI, and, ML on Massively Multi-Threaded Systems - Part II of II</h2><div class='index-talk' id='talk33' style='display:block'><a href='javascript:toggle_star(33)' class='star'><span class='star33'>&star;</span></a> <b>11:30 AM&ndash;11:45 AM (D504)</b> Kamesh Madduri, Exploiting Parallelism in Extreme Multi-Label Graph Classification Problems <span id='bitlink-33'><small><a href='javascript:show_bit(33)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-33' style='display:none'><small><a href='javascript:hide_bit(33)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Exploiting Parallelism in Extreme Multi-Label Graph Classification Problems</b><br />Kamesh Madduri<br />Friday, March 3 11:30 AM&ndash;11:45 AM<br />This is the 1st talk in <a href='session-8.html'>Scaling Data Science, AI, and, ML on Massively Multi-Threaded Systems - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />D504<br /><br /><small>We are developing graph-based techniques for extreme multilabel text classification problems that arise in e-commerce applications. Our techniques are relatively easy to interpret, use intuitive parameters, and are competitive with fast neural network-based classification methods. This talk will introduce the motivating applications and present data structures that make the training and inference algorithms amenable to exploiting shared-memory multicore parallelism. We will also comment on extending the implementations to GPUs and distributed-memory systems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75194' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk34' style='display:block'><a href='javascript:toggle_star(34)' class='star'><span class='star34'>&star;</span></a> <b>11:50 AM&ndash;12:05 AM (D504)</b> Alok Tripathy, Reducing Communication in Graph Neural Network Training <span id='bitlink-34'><small><a href='javascript:show_bit(34)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-34' style='display:none'><small><a href='javascript:hide_bit(34)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Reducing Communication in Graph Neural Network Training</b><br />Alok Tripathy<br />Friday, March 3 11:50 AM&ndash;12:05 AM<br />This is the 2nd talk in <a href='session-8.html'>Scaling Data Science, AI, and, ML on Massively Multi-Threaded Systems - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />D504<br /><br /><small>Graph Neural Networks (GNNs) are powerful and flexible neural networks that use the naturally sparse connectivity information of the data. GNNs represent this connectivity as sparse matrices, which have lower arithmetic intensity and thus higher communication costs compared to dense matrices, making GNNs harder to scale to high concurrencies than convolutional or fully-connected neural networks. We show that communication-avoiding matrix multiplication algorithms can accelerate GNN training compared to existing training methods with two families of algorithms. The first asymptotically reduces communication in full-batch training by leveraging 1D, 1.5D, 2D, and 3D SpMM algorithms. The second applies communication-avoiding algorithms for SpGEMM to parallelize GNN minibatch sampling algorithms (e.g. GraphSAGE, LADIES, GraphSAINT). Altogether, these algorithms optimize communication across the full GNN training pipeline. We experiment on graph datasets with billions of edges on over a hundred GPUs to show the performance benefits of these algorithms.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75194' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk35' style='display:block'><a href='javascript:toggle_star(35)' class='star'><span class='star35'>&star;</span></a> <b>12:10 AM&ndash;12:25 AM (D504)</b> Michael Mandulak, Vertex Ordering Refinement and Coarsening Methods for Accelerated Graph Analysis <span id='bitlink-35'><small><a href='javascript:show_bit(35)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-35' style='display:none'><small><a href='javascript:hide_bit(35)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Vertex Ordering Refinement and Coarsening Methods for Accelerated Graph Analysis</b><br />Michael Mandulak<br />Friday, March 3 12:10 AM&ndash;12:25 AM<br />This is the 3rd talk in <a href='session-8.html'>Scaling Data Science, AI, and, ML on Massively Multi-Threaded Systems - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />D504<br /><br /><small>Vertex ordering and graph coarsening are critical topics within large-scale graph analysis towards efficient memory access patterns through vertex locality improvements and graph size reduction, respectively, in a distributed setting. Subsequently, modern ordering schemes show inefficiencies through heuristic-based solutions and, when coupled with coarsening methods, can be runtime inefficient, especially as real-world networks continue to grow. Thus, we conduct an experimental study focused on the combination of ordering and coarsening methods with respect to the algebraic distance metric. We additionally propose a framework implementing our coarsening method on the GPU alongside the CPU shared-memory parallel explicit refinement of vertex labels relative to analysis measures. We show improvements in cache misses and analysis runtimes for the PageRank, Louvain and Multistep connectivity algorithms on a variety of different graph topologies and sizes. Through these results, we demonstrate the potential of joint parallel ordering and coarsening methods on large-scale networks for improved analysis measures.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75194' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk36' style='display:block'><a href='javascript:toggle_star(36)' class='star'><span class='star36'>&star;</span></a> <b>12:30 AM&ndash;12:45 AM (D504)</b> Marco Minutoli, Graman: Graph Network Based Simulator for Forecasting Molecular Polarizabilities <span id='bitlink-36'><small><a href='javascript:show_bit(36)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-36' style='display:none'><small><a href='javascript:hide_bit(36)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Graman: Graph Network Based Simulator for Forecasting Molecular Polarizabilities</b><br />Marco Minutoli<br />Friday, March 3 12:30 AM&ndash;12:45 AM<br />This is the 4th talk in <a href='session-8.html'>Scaling Data Science, AI, and, ML on Massively Multi-Threaded Systems - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />D504<br /><br /><small>Realistic, high-quality simulations of complex physical systems often require substantial computational resources making them challenging to scale up to larger systems. A promising approach to overcome these limitations is to use machine learning models trained on experimental and simulation data. One of the many applications that can benefit from such an approach is simulating Raman spectra from ab initio molecular dynamics (AIMD) simulations. Recent works have shown that Graph Network-based simulators can be trained to simulate complex dynamical systems. In this talk, we will present our Graph Network-based simulator to accelerate the simulation of Raman spectra. The proposed simulator constitutes of three components: (1) encoder which constructs the graph given the atomic positions and embeds the atoms/edges into a latent space; (2) Graph-Network processor which learns the spatial interactions among atoms and updates the latent embeddings; and (3) decoder which learns the dynamics and predicts the next state of the simulation. Currently, our simulator is trained to simulate molecule trajectories. We trained our simulator on synthetic simulation data obtained from NWChem. We observed satisfactory performance on single-step prediction. We plan to develop novel methodologies to improve multi-step prediction performance. Finally, we dive into future research opportunities on generalizing this approach to other physical systems.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75194' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk37' style='display:block'><a href='javascript:toggle_star(37)' class='star'><span class='star37'>&star;</span></a> <b>12:50 AM&ndash;1:05 AM (D504)</b> Mario Lino, Multi-Scale and Rotation-Equivariant Graph Neural Networks for the Simulation of Fluid Dynamics <span id='bitlink-37'><small><a href='javascript:show_bit(37)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-37' style='display:none'><small><a href='javascript:hide_bit(37)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Multi-Scale and Rotation-Equivariant Graph Neural Networks for the Simulation of Fluid Dynamics</b><br />Mario Lino<br />Friday, March 3 12:50 AM&ndash;1:05 AM<br />This is the 5th talk in <a href='session-8.html'>Scaling Data Science, AI, and, ML on Massively Multi-Threaded Systems - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />D504<br /><br /><small>The numerical simulation of fluid dynamics is an essential tool in many areas of science and engineering. However, its high computational cost can limit application in practice. Recent deep-learning approaches have demonstrated the potential to yield surrogate models for fluid dynamics simulation. While such models exhibit lower accuracy in comparison, their low runtime makes them appealing for design-space exploration and real-time simulation. We introduce two novel graph neural networks (GNNs) for extrapolating the time evolution of a fluid in an unstructured discretization of the domain. In both models, previous states are processed through multiple coarsenings of the graph, which enables faster information propagation through the fluid domain and improves the capture and forecast of the system state. One of the models is architecturally equivariant to rotations, which allows the network to learn the underlying physics more efficiently. We analyse these models using two canonical fluid models: advection and incompressible fluid dynamics. Our results show that the proposed models can generalise from uniform advection fields to high-gradient fields on complex domains. The multi-scale graph architecture allows for inference of incompressible Navier-Stokes solutions, within a range of Reynolds numbers and design parameters, more effectively than a single-scale GNN. Simulations are between $10^2$ to $10^4$ times faster than the numerical solutions on which they were trained.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75194' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
