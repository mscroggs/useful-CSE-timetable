<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Advances in Deep Neural Operators - Part II of II</h2><div class='index-talk' id='talk359' style='display:block'><a href='javascript:toggle_star(359)' class='star'><span class='star359'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (Forum Centre)</b> Jacob Seidman, Linear and Nonlinear Output Representations in Operator Networks <span id='bitlink-329'><small><a href='javascript:show_bit(329)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-329' style='display:none'><small><a href='javascript:hide_bit(329)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Linear and Nonlinear Output Representations in Operator Networks</b><br />Jacob Seidman<br />Thursday, March 2 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-80.html'>Advances in Deep Neural Operators - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Forum Centre<br /><br /><small>Supervised learning in function spaces is an emerging area of machine learning research with applications to the prediction of complex physical systems such as fluid flows, solid mechanics, and climate modeling.  By directly learning maps (operators) between infinite dimensional function spaces, these models are able to learn discretization invariant representations of target functions.  A common approach is to represent such target functions as linear combinations of basis elements learned from data. However, there are simple scenarios where, even though the target functions form a low dimensional submanifold, a very large number of basis elements is needed for an accurate linear representation. Here we present NOMAD, a novel operator learning framework with a nonlinear decoder map capable of learning finite dimensional representations of nonlinear submanifolds in function spaces.  We show this method is able to accurately learn low dimensional representations of solution manifolds to partial differential equations while outperforming linear models of larger size.  Additionally, we compare to state-of-the-art operator learning methods on a complex fluid dynamics benchmark and achieve competitive performance with a significantly smaller model size and training cost.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75335' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk360' style='display:block'><a href='javascript:toggle_star(360)' class='star'><span class='star360'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (Forum Centre)</b> Adar Kahana, Using Spiking Neural Networks for Scientific Computations <span id='bitlink-330'><small><a href='javascript:show_bit(330)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-330' style='display:none'><small><a href='javascript:hide_bit(330)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Using Spiking Neural Networks for Scientific Computations</b><br />Adar Kahana<br />Thursday, March 2 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-80.html'>Advances in Deep Neural Operators - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Forum Centre<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75335' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk361' style='display:block'><a href='javascript:toggle_star(361)' class='star'><span class='star361'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (Forum Centre)</b> Max Welling, Will Neural PDE Surrogates Disrupt the Field of PDE Solving? <span id='bitlink-331'><small><a href='javascript:show_bit(331)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-331' style='display:none'><small><a href='javascript:hide_bit(331)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Will Neural PDE Surrogates Disrupt the Field of PDE Solving?</b><br />Max Welling<br />Thursday, March 2 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-80.html'>Advances in Deep Neural Operators - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Forum Centre<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75335' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk362' style='display:block'><a href='javascript:toggle_star(362)' class='star'><span class='star362'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (Forum Centre)</b> Levi E. Lingsch, A Vandermonde Neural Operator: Extending the Fourier Neural Operator to Nonequispaced Distributions <span id='bitlink-332'><small><a href='javascript:show_bit(332)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-332' style='display:none'><small><a href='javascript:hide_bit(332)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A Vandermonde Neural Operator: Extending the Fourier Neural Operator to Nonequispaced Distributions</b><br />Levi E. Lingsch<br />Thursday, March 2 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-80.html'>Advances in Deep Neural Operators - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Forum Centre<br /><br /><small>The Fourier neural operator has shown impressive capabilities to learn parametric partial differential equations. It relies on the fast Fourier transform, which imposes a strict requirement for equispaced data. In this work, we propose a neural operator learning method utilizing a generalized Vandermonde structured matrix to act as a surrogate for the forward and inverse pass of the Fourier transform on nonequispaced data, while still maintaining quasi-linear operational complexity. Our results show that the proposed Vandermonde neural operator surpasses the Fourier neural operator in speed, while maintaining its accuracy. The simple structure makes this method ideal for applying neural operator learning to the nonequispaced lattice, as well as real-world data collected on a global scale.     </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75335' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk363' style='display:block'><a href='javascript:toggle_star(363)' class='star'><span class='star363'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (Forum Centre)</b> Soraya Terrab, Learning Moment-Preserving Filters for Discontinuous Data <span id='bitlink-333'><small><a href='javascript:show_bit(333)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-333' style='display:none'><small><a href='javascript:hide_bit(333)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Learning Moment-Preserving Filters for Discontinuous Data</b><br />Soraya Terrab<br />Thursday, March 2 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-80.html'>Advances in Deep Neural Operators - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />Forum Centre<br /><br /><small>Filters are used to enhance accuracy, remove noise, and reduce spurious oscillations in a numerical approximation. This is especially useful when unphysical artifacts such as aliasing error or Gibbs phenomena arise, or when extracting higher accuracy in a numerical solution. As part of this, filters should increase the decay rate of the coefficients away from a discontinuity. Near a shock, filters reduce spurious oscillations.      The motivation of this work is to minimize the size of the region with order one numerical error typically found near a discontinuity, which in turn contributes to enhancing the overall accuracy of a numerical approximation. We propose a novel way to learn filters for discontinuous data while preserving moments. The learned filter is a convolutional neural network, which learns from time-evolved, linearly advected data of different initial condition profiles that include $C^0$ and $C^1$ discontinuities.  We enforce a consistency condition as a hard constraint and maintain polynomial reproduction through a penalty term in the training. In this talk, we will present construction and training aspects of our learned filter along with benchmark tests of the effectiveness of the filter.    
  	    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75335' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
