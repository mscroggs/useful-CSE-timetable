<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Exascale Applications of High-Order PDE Solvers - Part I of II</h2><div class='index-talk' id='talk1781' style='display:block'><a href='javascript:toggle_star(1781)' class='star'><span class='star1781'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D503)</b> MiSun Min, Towards Exascale for Reactor and Wind Energy Simulations <span id='bitlink-1628'><small><a href='javascript:show_bit(1628)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1628' style='display:none'><small><a href='javascript:hide_bit(1628)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Towards Exascale for Reactor and Wind Energy Simulations</b><br />MiSun Min<br />Monday, February 27 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-392.html'>Exascale Applications of High-Order PDE Solvers - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D503<br /><br /><small>As part of DOE's ECP Center for Efficient Exascale Discretizations, we have  developed Nek5000/RS, a highly-performant open-source thermal-fluids code based on high-order spectral element discretizations that is targeting GPU-based platforms.    
Recent developments and optimizations have led to an unprecedented milestone in the ability to perform large-eddy simulation of a full reactor core. Careful tuning of the high-performance kernels, the multilevel solvers, and timestepping algorithms led to a 4X speed-up in performance when running on all GPUs of Summit (4608 nodes=27648 GPUs) at Oak Ridge Leadership Computing Facility. As a result, it is now possible to simulate flow through a 352000-pebble bed reactor (98 million spectral elements of order N=8, n=51 billion gridpoints) in just six hours of wall clock time.    
We also explore large-eddy-simulation modeling approaches and computational  performance for the simulation of atmospheric boundary layer (ABL) flows that are of direct relevance to wind energy production. We present the performance of NekRS on Summit and Crusher, the testbed for the Frontier exascale system, using 18 to 384 Graphics Compute Dies on AMD MI250X GPUs. We compare strong- and weak-scaling capabilities, linear solver performance, and time to solution of NekRS with AMR-Wind. We also identify leading inhibitors to parallel scaling.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75868' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1782' style='display:block'><a href='javascript:toggle_star(1782)' class='star'><span class='star1782'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D503)</b> Oriol Lehmkuhl, Large Eddy Simulation of Aircraft at Affordable Cost: a Milestone in Computational Fluid Dynamics <span id='bitlink-1629'><small><a href='javascript:show_bit(1629)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1629' style='display:none'><small><a href='javascript:hide_bit(1629)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Large Eddy Simulation of Aircraft at Affordable Cost: a Milestone in Computational Fluid Dynamics</b><br />Oriol Lehmkuhl<br />Monday, February 27 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-392.html'>Exascale Applications of High-Order PDE Solvers - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D503<br /><br /><small>While there have been numerous applications of large eddy simulations (LES) to complex flows, their application to practical engineering configurations, such as full aircraft models, have been limited to date. Recently, however, advances in rapid, high quality mesh generation, low-dissipation numerical schemes and physics-based subgrid- scale and wall models have led to, for the first time, accurate simulations of a realistic aircraft in landing configuration (the Japanese Aerospace Exploration Agency Standard Model) in less than a day of turnaround time with modest resource requirements. In this paper, a systematic study of the predictive capability of LES across a range of angles of attack (including maximum lift and post-stall regimes), the robustness of the predictions to grid resolution and the incorporation of wind tunnel effects is carried out. Integrated engineering quantities of interest, such as lift, drag and pitching moment will be compared with experimental data, while sectional pressure forces will be used to corroborate the accuracy of the integrated quantities.     </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75868' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1783' style='display:block'><a href='javascript:toggle_star(1783)' class='star'><span class='star1783'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D503)</b> Rezgar Shakeri, Performance-Portable p-Multigrid for Nonlinear Solid Mechanics in Nearly Incompressible Regime <span id='bitlink-1630'><small><a href='javascript:show_bit(1630)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1630' style='display:none'><small><a href='javascript:hide_bit(1630)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Performance-Portable p-Multigrid for Nonlinear Solid Mechanics in Nearly Incompressible Regime</b><br />Rezgar Shakeri<br />Monday, February 27 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-392.html'>Exascale Applications of High-Order PDE Solvers - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D503<br /><br /><small>We introduce a matrix-free $p$-multigrid finite element method with Newton-Krylov and quasi-Newton iterative solvers for hyperelasticity problems. Contrary to standard formulations of finite strain constitutive models, which lead to loss of several digits of relative accuracy for small deformations, we introduce a stable formulation that is accurate for all-deformation elasticity. We investigate reliability, efficiency, and accuracy on a collection of multiscale compressible and incompressible materials up to billions of degrees of freedom running on CPU and GPU architectures. We find that high order methods deliver significant costs savings even for loose error tolerances and physical singularities in complex geometric models.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75868' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1784' style='display:block'><a href='javascript:toggle_star(1784)' class='star'><span class='star1784'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D503)</b> Marcel Koch, Adapting Exascale Techniques Through Ginkgo <span id='bitlink-1631'><small><a href='javascript:show_bit(1631)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1631' style='display:none'><small><a href='javascript:hide_bit(1631)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Adapting Exascale Techniques Through Ginkgo</b><br />Marcel Koch<br />Monday, February 27 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-392.html'>Exascale Applications of High-Order PDE Solvers - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D503<br /><br /><small>This talk presents modern, exascale-ready techniques within the Ginkgo high-performance linear algebra library, focused on manycore systems. The application area is the implicit or semi-implicit time stepping of hyperbolic-parabolic partial differential equations discretized with discontinuous Galerkin methods. Within this area, strategies such as batched solver, or mixed-precison are discussed. The talk will highlight the necessary abstractions and implementations on GPU hardware, and present the performance on sample problems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75868' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1785' style='display:block'><a href='javascript:toggle_star(1785)' class='star'><span class='star1785'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (D503)</b> Adeeb Arif Kor, Fast Finite Element Solver for Focused Ultrasound Applications <span id='bitlink-1632'><small><a href='javascript:show_bit(1632)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1632' style='display:none'><small><a href='javascript:hide_bit(1632)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Fast Finite Element Solver for Focused Ultrasound Applications</b><br />Adeeb Arif Kor<br />Monday, February 27 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-392.html'>Exascale Applications of High-Order PDE Solvers - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D503<br /><br /><small>Focused ultrasound (FUS) application is a therapeutic medical technology that uses ultrasound for tissue heating and ablation. In recent years, interest in FUS application has been growing, especially for the treatment of some cancers. In contrast to conventional cancer therapy, FUS application is non-invasive and as such makes it an attractive treatment method. However, effective patient-specific FUS treatment would be supported by accurate simulations of treatment. This presents a computational challenge as the ratio of the domain size to the wavelength is typically large, and therefore computationally demanding. We have developed a fast, high-order matrix-free finite element solvers to simulate a 3D FUS application. We show that by employing a high-order methods and fast assembly techniques, we simulate with high accuracy and acceptable computation time. We show that we can use methods with the lowest known computational cost complexity and achieve a very significant fraction of the peak performance of modern CPU architectures, and with excellent parallel scalability. Overall, we show that our solver is fast, accurate, scalable, and is suitable for large-scale time-domain nonlinear acoustics simulations.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75868' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
