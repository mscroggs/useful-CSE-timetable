<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Computational Methods for Statistical Inverse Problems</h2><div class='index-talk' id='talk266' style='display:block'><a href='javascript:toggle_star(266)' class='star'><span class='star266'>&star;</span></a> <b>11:30 AM&ndash;11:45 AM (D506)</b> Neil Chada, On a Dynamic Variant of the Iteratively Regularized Gauss-Newton Method with Sequential Data <span id='bitlink-248'><small><a href='javascript:show_bit(248)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-248' style='display:none'><small><a href='javascript:hide_bit(248)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>On a Dynamic Variant of the Iteratively Regularized Gauss-Newton Method with Sequential Data</b><br />Neil Chada<br />Friday, March 3 11:30 AM&ndash;11:45 AM<br />This is the 1st talk in <a href='session-59.html'>Computational Methods for Statistical Inverse Problems</a> (11:30 AM&ndash;1:10 PM)<br />D506<br /><br /><small>In this talk we will present a variant of the iterative regularized Gauss-Newton method, which is a well-known methodology for solving inverse problems. This talk, and work, is motivated from the question of whether one can improve the computation based on sequential data rather than using one single instance of the data. As a result, we present a dynamic version which considers sequential data at every iteration. We introduce two new algorithm, which we analyze both numerically and computationally. We provide various results such as convergence, well-defindness and error bounds, while testing our new algorithms on a range of PDEs.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75294' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk267' style='display:block'><a href='javascript:toggle_star(267)' class='star'><span class='star267'>&star;</span></a> <b>11:50 AM&ndash;12:05 AM (D506)</b> Remo Kretschmann, Optimal Regularized Hypothesis Testing in Statistical Inverse Problems <span id='bitlink-249'><small><a href='javascript:show_bit(249)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-249' style='display:none'><small><a href='javascript:hide_bit(249)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Optimal Regularized Hypothesis Testing in Statistical Inverse Problems</b><br />Remo Kretschmann<br />Friday, March 3 11:50 AM&ndash;12:05 AM<br />This is the 2nd talk in <a href='session-59.html'>Computational Methods for Statistical Inverse Problems</a> (11:30 AM&ndash;1:10 PM)<br />D506<br /><br /><small>In many inverse problems, one is not primarily interested in the whole solution $u^\dagger \in \mathcal{X}$, but in specific features of it that can be described by a family of linear functionals of $u^\dagger$. We perform statistical inference for such features by means of hypothesis testing.    
This problem has recently been treated by multiscale methods based upon unbiased estimates of those functionals. Constructing hypothesis tests using unbiased estimators, however, has two severe drawbacks: Firstly, unbiased estimators only exist for sufficiently smooth linear functionals, and secondly, they suffer from a huge variance due to the ill-posedness of the problem, so that the corresponding tests have bad detection properties.    
We overcome both of these issues by considering hypothesis tests with maximal power among all tests based upon linear estimators that have a given level of significance. While the construction of such optimal tests requires knowledge of the true solution $u^\dagger$, we present a way to compute approximately optimal hypothesis tests adaptively. We study this approach both analytically and numerically for linear inverse problems and compare it with unregularized hypothesis testing.    
    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75294' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk269' style='display:block'><a href='javascript:toggle_star(269)' class='star'><span class='star269'>&star;</span></a> <b>12:10 AM&ndash;12:25 AM (D506)</b> Yoonsang Lee, Inhomogeneous Regularization for Ensemble Kalman Inversion <span id='bitlink-250'><small><a href='javascript:show_bit(250)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-250' style='display:none'><small><a href='javascript:hide_bit(250)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Inhomogeneous Regularization for Ensemble Kalman Inversion</b><br />Yoonsang Lee<br />Friday, March 3 12:10 AM&ndash;12:25 AM<br />This is the 3rd talk in <a href='session-59.html'>Computational Methods for Statistical Inverse Problems</a> (11:30 AM&ndash;1:10 PM)<br />D506<br /><br /><small>Regularization stabilizes ill-conditioned inverse problems by imposing prior information/characteristics of the unknown signal to recover. $l_1$ regularization, for example, imposes sparsity of the unknown signal, while $l_2$ regularization imposes smoothness of the signal. For a signal with both sparsity and smoothness, it is natural to use regularization that changes over different signal locations to account for corresponding characteristics, which we call inhomogeneous regularization. In this work, we use Ensemble Kalman Inversion for inhomogeneous regularization. The ensemble provides prior information for classifying characteristics to determine the power $p$ for $l_p$ regularization. Once the characteristics are determined, Ensemble Kalman Inversion uses the transformation-based approach for $l_p$ regularization as an efficient solver for inhomogeneous regularization problems. The work validates the effectiveness and robustness of the inhomogeneous regularization problem using Ensemble Kalman inversion through a suite of stringent 1D and 2D test problems, including sea ice image recovery that shows both smooth heterogeneous variations and local cracks.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75294' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk270' style='display:block'><a href='javascript:toggle_star(270)' class='star'><span class='star270'>&star;</span></a> <b>12:30 AM&ndash;12:45 AM (D506)</b> Erkki Somersalo, MCMC Sampling For Sparsity Promoting Bayesian Hypermodels <span id='bitlink-251'><small><a href='javascript:show_bit(251)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-251' style='display:none'><small><a href='javascript:hide_bit(251)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>MCMC Sampling For Sparsity Promoting Bayesian Hypermodels</b><br />Erkki Somersalo<br />Friday, March 3 12:30 AM&ndash;12:45 AM<br />This is the 4th talk in <a href='session-59.html'>Computational Methods for Statistical Inverse Problems</a> (11:30 AM&ndash;1:10 PM)<br />D506<br /><br /><small>In this talk, we review some conditionally Gaussian hypermodels that are shown to favor sparse solutions to the Maximum A Posteriori (MAP) estimation problem. A natural question is whether the whole posterior distribution is concentrated on sparse solutions. In this presentation, the question is addressed by MCMC sampling.   	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75294' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
