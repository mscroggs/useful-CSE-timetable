<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Bridging Numerical Analysis and Machine Learning - Part I of II</h2><div class='index-talk' id='talk1200' style='display:block'><a href='javascript:toggle_star(1200)' class='star'><span class='star1200'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D402)</b> Matthias Moller, IgaNets: Physics-Informed Machine Learning Embedded Into Isogeometric Analysis <span id='bitlink-1111'><small><a href='javascript:show_bit(1111)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1111' style='display:none'><small><a href='javascript:hide_bit(1111)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>IgaNets: Physics-Informed Machine Learning Embedded Into Isogeometric Analysis</b><br />Matthias Moller<br />Wednesday, March 1 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-264.html'>Bridging Numerical Analysis and Machine Learning - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D402<br /><br /><small>In this talk we present a novel approach to embed physics-informed neural networks (PINN) into the framework of Isogeometric Analysis. IGA is an extension of the finite element method that integrates simulation-based analysis into the computer-aided design pipeline. In short, the same mathematical formalism, namely B-splines or NURBS, that is used to model the geometry is adopted to represent the approximate solution, which is computed following the same strategy as in classical finite elements.  In contrast to classical PINNs, which predict point-wise solution values to (initial-)boundary-value problems directly, our IgaNets learn solutions in terms of their expansion coefficients relative to a given B-Spline or NURBS basis. This approach is also used to encode the geometry and other problem parameters such as boundary conditions and feed them into the network as inputs. Once trained, our IgaNets make it possible to explorer various designs from a family of similar problem configurations efficiently without the need to perform a computationally expensive simulation for each new problem configuration.    
Next to discussing the method conceptually and presenting numerical results, we will shed some light on the technical details of our C++ reference implementation in Torch. In particular, we will discuss matrix-based implementation of B-splines that is particularly suited for efficient backpropagation.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75604' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1201' style='display:block'><a href='javascript:toggle_star(1201)' class='star'><span class='star1201'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D402)</b> Tjeerd Jan Heeringa, Higher-Order Activation Functions in Neural PDE Theory <span id='bitlink-1112'><small><a href='javascript:show_bit(1112)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1112' style='display:none'><small><a href='javascript:hide_bit(1112)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Higher-Order Activation Functions in Neural PDE Theory</b><br />Tjeerd Jan Heeringa<br />Wednesday, March 1 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-264.html'>Bridging Numerical Analysis and Machine Learning - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D402<br /><br /><small>Neural networks have been used successfully in solving very high dimensional PDEs, overcoming the curse of dimensionality plaguing classical methods. A neural theory of PDE can explain why and when these neural networks can overcome the curse of dimensionality. The Barron spaces and tree-like spaces have been introduced as function spaces on which to build this neural theory of PDE. Their study has mostly been focused on the ReLU activation function. Although the ReLU is ubiquitous in deep learning, it is of limited use for solving PDEs due to its limited smoothness. In this work we unify previous work on different activation functions, and extend the list of relations between activation functions to create a taxonomy of the Barron spaces based on the activation function used. We show how the taxonomy can be used for operators.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75604' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1202' style='display:block'><a href='javascript:toggle_star(1202)' class='star'><span class='star1202'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D402)</b> Yankun Hong, Physics-Informed Two-Tier Neural Network for Non-Linear Model Order Reduction <span id='bitlink-1113'><small><a href='javascript:show_bit(1113)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1113' style='display:none'><small><a href='javascript:hide_bit(1113)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Physics-Informed Two-Tier Neural Network for Non-Linear Model Order Reduction</b><br />Yankun Hong<br />Wednesday, March 1 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-264.html'>Bridging Numerical Analysis and Machine Learning - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D402<br /><br /><small>In recent years, machine learning (ML) has had a great impact in the area of non-intrusive, non-linear model order reduction (MOR). However, the offline training phase still suffers from high computational costs since it requires numerous expensive full-order solutions as the training data. Furthermore, in state-of-the-art methods, neural networks trained by a small amount of the training data cannot be expected to generalize well enough, and the training phase generally ignores the underlying physical information. Moreover, state-of-the-art affine decomposition and hyper reduction techniques are intrusive or entail huge offline computational costs. To resolve these challenges, inspired by recent developments in physics-informed neural networks and the physics-reinforced neural networks, we propose a non-intrusive, physics-informed, two-tier deep network (TTDN) method. The proposed network, wherein the first tier achieves the regression of the unknown quantity of interest (QoI) and the second tier rebuilds the physical constitutive law between the unknown QoI's and derived quantities, is trained using pretraining and semi-supervised learning strategies. To showcase the efficiency of the proposed approach over the state-of-the-art methods, we perform numerical experiments on challenging non-linear and non-affine problems and study the behavior associated with the high or relatively low computational cost of the full-order data.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75604' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1203' style='display:block'><a href='javascript:toggle_star(1203)' class='star'><span class='star1203'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D402)</b> Jan-Frederik Pietschmann, Data Driven Gradient Flows <span id='bitlink-1114'><small><a href='javascript:show_bit(1114)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1114' style='display:none'><small><a href='javascript:hide_bit(1114)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Data Driven Gradient Flows</b><br />Jan-Frederik Pietschmann<br />Wednesday, March 1 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-264.html'>Bridging Numerical Analysis and Machine Learning - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D402<br /><br /><small>We present a framework enabling variational data assimilation for gradient flows in general metric spaces, based on the minimizing movement (or Jordan-Kinderlehrer-Otto) approximation scheme. After discussing stability properties in the most general case, we specialise to the space of probability measures endowed with the Wasserstein distance. This setting covers many non-linear partial differential equations (PDEs), such as the porous medium equation or general drift-diffusion-aggregation equations, which can be treated by our methods independent of their respective properties (such as finite speed of propagation or blow-up). We then focus on the numerical implementation of our approach using an primal-dual algorithm. The strength of our approach lies in the fact that by simply changing the driving functional, a wide range of PDEs can be treated without the need to adopt the numerical scheme. We conclude by presenting detailed numerical examples.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75604' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1204' style='display:block'><a href='javascript:toggle_star(1204)' class='star'><span class='star1204'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (D402)</b> Rouhollah Tavakoli, Quantitative Phase-Field Modeling by Physics-Constrained Deep Learning <span id='bitlink-1115'><small><a href='javascript:show_bit(1115)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1115' style='display:none'><small><a href='javascript:hide_bit(1115)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Quantitative Phase-Field Modeling by Physics-Constrained Deep Learning</b><br />Rouhollah Tavakoli<br />Wednesday, March 1 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-264.html'>Bridging Numerical Analysis and Machine Learning - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D402<br /><br /><small>Phase-field models are recently known as standard approaches for the prediction of solidification microstructure. However, to perform a predictive phase-field simulation is remained still computationally challenge in practice, because numerical solutions exhibit mesh dependency issue. As a result, the quantitative phase field modeling is limited to very small spatial scales and low Peclet number regimes. Recent advances in deep neural networks revealed that the direct differentiation of neural networks makes it possible to solve nonlinear partial differential equations (PDEs) in a computational complexity and accuracy competitive to classic grid-based approaches like finite different and finite element methods. In physics-informed neural networks methods, instead of using existing data, the physics of problem is directly included into the training phase and the neural-network is trained without any required data. This work communicates results on the solution of PDEs corresponding to the quantitative phase-field modeling of pure elements solidification. Different network architectures and ways of training on the accuracy of results are studied. The results reveal that this method competes with existing mesh-based approaches in terms of accuracy and efficiency. In particular, we can train the network in a small space-time domain, and then can solve the corresponding PDEs with the already trained network in larger spatial domains and longer times.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75604' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
