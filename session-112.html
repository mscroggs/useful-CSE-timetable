<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Sparse Computations in Science and Engineering - Part I of II</h2><div class='index-talk' id='talk510' style='display:block'><a href='javascript:toggle_star(510)' class='star'><span class='star510'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (G107)</b> Aydin Buluc, The Ubiquitous Sparse Matrix-Matrix Products <span id='bitlink-470'><small><a href='javascript:show_bit(470)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-470' style='display:none'><small><a href='javascript:hide_bit(470)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>The Ubiquitous Sparse Matrix-Matrix Products</b><br />Aydin Buluc<br />Wednesday, March 1 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-112.html'>Sparse Computations in Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G107<br /><br /><small>Multiplication of a sparse matrix with another matrix is a fundamental operation that captures the computational patterns of many data science applications, including but not limited to graph algorithms, sparsely connected neural networks, graph neural networks, clustering, and many-to- many comparisons of biological sequencing data.     
In the majority of these application scenarios, the matrix multiplication takes places on an arbitrary algebraic semiring where the scalar operations are overloaded with user-defined functions with certain properties or a more general heterogenous algebra where even the domains of the input matrices can be different. Here we provide unifying treatment of the sparse matrix-matrix operation and its rich application space.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75388' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk2059' style='display:block'><a href='javascript:toggle_star(2059)' class='star'><span class='star2059'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (G107)</b> Raghavendra Kanakagiri, A General Distributed Framework for Contraction of a Sparse Tensor with a Tensor Network <span id='bitlink-1884'><small><a href='javascript:show_bit(1884)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1884' style='display:none'><small><a href='javascript:hide_bit(1884)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A General Distributed Framework for Contraction of a Sparse Tensor with a Tensor Network</b><br />Raghavendra Kanakagiri<br />Wednesday, March 1 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-112.html'>Sparse Computations in Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G107<br /><br /><small>Sparse tensor decomposition and completion are common in numerous applications, ranging from machine learning to computational quantum chemistry. Typically, the main bottleneck in optimization of these models are contractions of a single large sparse tensor with a network of several dense matrices or tensors (SpTTN). Prior works on high-performance tensor decomposition and completion have focused on performance and scalability optimizations for specific SpTTN kernels.      We present algorithms and a runtime system for identifying and executing the most efficient loop nest for any SpTTN kernel. We consider both enumeration of such loop nests for autotuning and efficient algorithms for finding the lowest cost loop-nest for simpler metrics, such as buffer size or cache miss models. Our runtime system identifies the best choice of loop nest without user guidance, and also provides a distributed-memory parallelization of SpTTN kernels. We evaluate our framework using both real-world and synthetic tensors. Our results demonstrate that our approach outperforms available generalized state-of-the-art libraries and matches the performance of specialized codes.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75388' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk512' style='display:block'><a href='javascript:toggle_star(512)' class='star'><span class='star512'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (G107)</b> Piyush Sao, Communication-Avoiding Algorithms for Sparse Triangular Matrices <span id='bitlink-471'><small><a href='javascript:show_bit(471)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-471' style='display:none'><small><a href='javascript:hide_bit(471)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Communication-Avoiding Algorithms for Sparse Triangular Matrices</b><br />Piyush Sao<br />Wednesday, March 1 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-112.html'>Sparse Computations in Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G107<br /><br /><small>Sparse triangular matrices occur extensively in scientific computing and data-intensive applications, most notably for solving systems of linear equations using Gaussian elimination. Such matrices have emerged as essential tools for artificial intelligence applications as they effectively model certain kinds of uni-directional information flow, such as the kind that occurs within feed-forward and graph neural networks or neuromorphic hardware. In this talk, I will discuss how we can exploit structured sparsity to reduce data transfer in parallel computation. Specifically, I will discuss new communication-avoiding algorithms for solving sparse triangular equations systems, which illustrate the more general idea of path decomposition and duplication â€” two techniques that facilitate performing sparse triangular computation with reduced communication. The path-duplication technique, while generalizing traditional edge duplication-based methods like 3D matrix multiplication and ghost cells in stencil computation for reducing communication, also shows some differences that may be useful for broader sparse linear algebraic and graph computations and beyond.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75388' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk513' style='display:block'><a href='javascript:toggle_star(513)' class='star'><span class='star513'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (G107)</b> Aditya Devarakonda, 2D S-Step Methods for Machine Learning <span id='bitlink-472'><small><a href='javascript:show_bit(472)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-472' style='display:none'><small><a href='javascript:hide_bit(472)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>2D S-Step Methods for Machine Learning</b><br />Aditya Devarakonda<br />Wednesday, March 1 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-112.html'>Sparse Computations in Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G107<br /><br /><small>Stochastic gradient descent (SGD) is one of the most widely used optimization methods for solving various machine learning problems. SGD solves an optimization problem by iteratively sampling a few data points from the input data, computing gradients for the selected data points, and updating the solution. However, in a parallel setting, SGD requires interprocess communication at every iteration.    
We introduce an s-step SGD algorithm which adapts the s-step technique from Krylov methods to re-organize the SGD computations into a form that communicates every s iterations instead of every iteration, where s is a tuning parameter. Furthermore, we develop 2D distributed-memory variants of the s-step SGD algorithm by leveraging existing work on 2D sparse matrix kernels and combining 1D s-step SGD with other existing SGD variants such as divide and conquer SGD and asynchronous SGD.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75388' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk514' style='display:block'><a href='javascript:toggle_star(514)' class='star'><span class='star514'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (G107)</b> Charlotte Debus, Predicting ILU(0) Effectiveness for Sparse Matrix Systems via Explainable Machine Learning <span id='bitlink-473'><small><a href='javascript:show_bit(473)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-473' style='display:none'><small><a href='javascript:hide_bit(473)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Predicting ILU(0) Effectiveness for Sparse Matrix Systems via Explainable Machine Learning</b><br />Charlotte Debus<br />Wednesday, March 1 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-112.html'>Sparse Computations in Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G107<br /><br /><small>Incomplete LU factorization of sparse matrices, which preserves the sparsity pattern of the original system, plays an important role in preconditioning iterative solvers. ILU(0) is a simple yet powerful preconditioner technique for many problems. However, generation of the ILU(0) preconditioner is costly, and it is thus useful to know whether it would be useful for a given problem beforehand. In this work we investigate the feasibility of machine learning techniques to aid in determining whether ILU(0) comprises a suitable preconditioner for a given matrix system. Our contributions are threefold: 1) We train a deep artificial neural network on examples from the SuiteSparse matrix collection to predict the effectiveness of ILU(0) based on meta-features describing the sparsity pattern. 2) Since deep learning approaches require huge amounts of training data, which cannot be provided by real-world matrix systems of appropriate sizes, we further evaluate the possibility of using partial matrices as a synthetic dataset for pretraining the model. 3) Additional insight into which characteristics are crucial for the successful ILU(0) is revealed through explainable machine learning via feature importance evaluation in tree-ensemble approaches. Our results show that ILU(0) effectiveness can be predicted with sufficient accuracy based on a few representative matrix characteristics, and that accuracy for very large matrices improves through learning on partial matrices.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75388' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
