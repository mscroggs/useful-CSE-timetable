<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Alleviating the Memory Footprint of Backpropagation</h2><div class='index-talk' id='talk1623' style='display:block'><a href='javascript:toggle_star(1623)' class='star'><span class='star1623'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D506)</b> Navjot Kukreja, Revolve and Its Extensions <span id='bitlink-1490'><small><a href='javascript:show_bit(1490)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1490' style='display:none'><small><a href='javascript:hide_bit(1490)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Revolve and Its Extensions</b><br />Navjot Kukreja<br />Monday, February 27 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-359.html'>Alleviating the Memory Footprint of Backpropagation</a> (9:45 AM&ndash;11:25 AM)<br />D506<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75787' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1624' style='display:block'><a href='javascript:toggle_star(1624)' class='star'><span class='star1624'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D506)</b> Michel Schanen, Checkpoint Code Generation in Julia for Numerical Simulations <span id='bitlink-1491'><small><a href='javascript:show_bit(1491)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1491' style='display:none'><small><a href='javascript:hide_bit(1491)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Checkpoint Code Generation in Julia for Numerical Simulations</b><br />Michel Schanen<br />Monday, February 27 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-359.html'>Alleviating the Memory Footprint of Backpropagation</a> (9:45 AM&ndash;11:25 AM)<br />D506<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75787' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1625' style='display:block'><a href='javascript:toggle_star(1625)' class='star'><span class='star1625'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D506)</b> Alena Shilova, Gradient Checkpointing for Training Neural Networks <span id='bitlink-1492'><small><a href='javascript:show_bit(1492)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1492' style='display:none'><small><a href='javascript:hide_bit(1492)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Gradient Checkpointing for Training Neural Networks</b><br />Alena Shilova<br />Monday, February 27 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-359.html'>Alleviating the Memory Footprint of Backpropagation</a> (9:45 AM&ndash;11:25 AM)<br />D506<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75787' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1626' style='display:block'><a href='javascript:toggle_star(1626)' class='star'><span class='star1626'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D506)</b> Jo&#227;o Speglich, Offloading to Nvme: A Hardware Alternative to Checkpointing <span id='bitlink-1493'><small><a href='javascript:show_bit(1493)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1493' style='display:none'><small><a href='javascript:hide_bit(1493)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Offloading to Nvme: A Hardware Alternative to Checkpointing</b><br />Jo&#227;o Speglich<br />Monday, February 27 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-359.html'>Alleviating the Memory Footprint of Backpropagation</a> (9:45 AM&ndash;11:25 AM)<br />D506<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75787' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1627' style='display:block'><a href='javascript:toggle_star(1627)' class='star'><span class='star1627'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (D506)</b> Salar Fattahi, Blessing of Nonconvexity in Factorized Models <span id='bitlink-1494'><small><a href='javascript:show_bit(1494)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1494' style='display:none'><small><a href='javascript:hide_bit(1494)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Blessing of Nonconvexity in Factorized Models</b><br />Salar Fattahi<br />Monday, February 27 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-359.html'>Alleviating the Memory Footprint of Backpropagation</a> (9:45 AM&ndash;11:25 AM)<br />D506<br /><br /><small>Factorized models, from low-rank matrix recovery to deep neural networks, play a central role in many modern machine learning problems. Despite their widespread applications, problems based on factorized models are deemed difficult to solve in their worst case due to their inherent nonconvexity. Our talk is inspired by the recent observations in the optimization and machine learning communities that many realistic and practical instances of factorized models are far from their worst case scenarios.    
We study a natural nonconvex and nonsmooth formulation of two prototypical factorized models, namely low-rank matrix factorization and deep linear regression, where the goal is to recover a low-dimensional model from a limited number of measurements, a subset of which may be grossly corrupted with noise. On the negative side, we show that this problem does not have a benign landscape: with high probability, there always exists a true solution that is not a global minimum of the loss function. However, on the positive side, we show that a simple subgradient method with small initialization is oblivious to such “problematic” solutions; instead, it converges to a balanced solution that is not only close to the ground truth but also enjoys a flat local landscape. Lastly, we empirically verify that the desirable optimization landscape of factorized models extends to other robust learning tasks, including deep matrix recovery and deep ReLU networks with L1-loss.  	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75787' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
