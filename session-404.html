<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part I of II</h2><div class='index-talk' id='talk1833' style='display:block'><a href='javascript:toggle_star(1833)' class='star'><span class='star1833'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (E104)</b> Carlo Marcati, Operator Network Approximations for Elliptic PDEs <span id='bitlink-1678'><small><a href='javascript:show_bit(1678)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1678' style='display:none'><small><a href='javascript:hide_bit(1678)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Operator Network Approximations for Elliptic PDEs</b><br />Carlo Marcati<br />Wednesday, March 1 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-404.html'>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />E104<br /><br /><small>The application of neural networks (NNs) to the numerical solution of  PDEs has seen growing popularity in the last five years: NNs have been  used as an ansatz space for the solutions, with different training  approaches (PINNs, deep Ritz methods, etc.); they have also been used  to infer discretization parameters and strategies.    
In this talk, I will focus on the convergence of operator networks  that approximate the solution operator of linear elliptic PDEs. I  will, in particular, consider operator networks that, given a fixed  right-hand side, map sets of diffusion-reaction coefficients into the  space of solutions (coefficient-to-solution map).  When the  coefficients are smooth and with periodic boundary conditions, the  size of the networks can be bounded with respect to the $H^1$ norm of  the error, uniformly over the parameter set. Specifically, the number  of non zero weights grows at most poly-logarithmically with respect to  the error. The proofs of our approximation rates combine elliptic  regularity, classical and recent results in numerical analysis, and  tools from NN approximation theory. Using the same techniques, we  extend the analysis to linear elasticity and parametric problems.    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75885' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1834' style='display:block'><a href='javascript:toggle_star(1834)' class='star'><span class='star1834'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (E104)</b> Hui Ji, Self-Supervised Deep Image Restoration via Adaptive Stochastic Gradient Langevin Dynamics <span id='bitlink-1679'><small><a href='javascript:show_bit(1679)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1679' style='display:none'><small><a href='javascript:hide_bit(1679)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Self-Supervised Deep Image Restoration via Adaptive Stochastic Gradient Langevin Dynamics</b><br />Hui Ji<br />Wednesday, March 1 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-404.html'>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />E104<br /><br /><small>While supervised deep learning has been a prominent tool for solving many image restoration problems, there is an increasing interest on studying self-supervised or un- supervised methods to address the challenges and costs of collecting truth images. Based on the neuralization of a Bayesian estimator of the problem, this paper presents a self-supervised deep learning approach to general image restoration problems. The key ingredient of the neuralized estimator is an adaptive stochastic gradient Langevin dynamics algorithm for efficiently sampling the posterior distribution of network weights. The proposed method is applied on two image restoration problems: compressed sensing and phase retrieval. The experiments on these applications showed that the proposed method not only outperformed existing non-learning and unsupervised solutions in terms of image restoration quality, but also is more computationally efficient.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75885' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1835' style='display:block'><a href='javascript:toggle_star(1835)' class='star'><span class='star1835'>&star;</span></a> <b>10:25 AM&ndash;10:50 AM (E104)</b> Juan M. Cardenas, CAS4DL: Christoffel Adaptive Sampling for Deep Learning in Scientific Computing Applications <span id='bitlink-1680'><small><a href='javascript:show_bit(1680)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1680' style='display:none'><small><a href='javascript:hide_bit(1680)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>CAS4DL: Christoffel Adaptive Sampling for Deep Learning in Scientific Computing Applications</b><br />Juan M. Cardenas<br />Wednesday, March 1 10:25 AM&ndash;10:50 AM<br />This is the 3rd talk in <a href='session-404.html'>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />E104<br /><br /><small>Many problems in computational science and engineering require the approximation of a high-dimensional function from data. In many such applications, data is costly to generate: for example, it each sample may require a costly PDE solve. Therefore, it is imperative to develop highly sample efficient algorithms. Recently, deep neural networks and deep learning have shown great promise to provide breakthrough performance in challenging function approximation tasks. In this work, we propose an adaptive sampling strategy, CAS4DL (Christoffel Adaptive Sampling for Deep Learning) to increase the sample efficiency of DL. Our novel approach is based on interpreting the second to last layer of a DNN as a dictionary of functions defined by the nodes on that layer. With this viewpoint, we then define an adaptive sampling strategy motivated by adaptive sampling schemes recently proposed for linear approximation schemes, wherein samples are drawn randomly with respect to the Christoffel function of the subspace spanned by this dictionary. We present numerical experiments comparing CAS4DL with standard Monte Carlo (MC) sampling. Our results demonstrate that CAS4DL often yields substantial savings in the number of samples required to achieve a given accuracy, particularly in the case of smooth activation functions. These results, therefore, are a promising step toward fully adapting DL to scientific computing applications.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75885' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
