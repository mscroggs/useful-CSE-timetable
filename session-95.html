<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Mixed Precision Algorithms in Numerical Linear Algebra - Part I of II</h2><div class='index-talk' id='talk426' style='display:block'><a href='javascript:toggle_star(426)' class='star'><span class='star426'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D406)</b> Erin C. Carson, Mixed Precision Randomized Preconditioners <span id='bitlink-394'><small><a href='javascript:show_bit(394)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-394' style='display:none'><small><a href='javascript:hide_bit(394)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Mixed Precision Randomized Preconditioners</b><br />Erin C. Carson<br />Thursday, March 2 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-95.html'>Mixed Precision Algorithms in Numerical Linear Algebra - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>Support for floating point arithmetic in multiple precisions is becoming increasingly common in emerging architectures. Mixed precision capabilities are already included in a quarter of the machines on the TOP500 list and are expected to be a crucial hardware feature in coming exascale machines. In this talk, we discuss recent work on exploiting mixed precision in randomized algorithms for low-rank approximation, and in particular, the use of such approximations within preconditioners for Krylov subspace methods.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75366' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk427' style='display:block'><a href='javascript:toggle_star(427)' class='star'><span class='star427'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D406)</b> Mantas Mikaitis, Monotonicity of Multi-Term Floating-Point Adders <span id='bitlink-395'><small><a href='javascript:show_bit(395)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-395' style='display:none'><small><a href='javascript:hide_bit(395)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Monotonicity of Multi-Term Floating-Point Adders</b><br />Mantas Mikaitis<br />Thursday, March 2 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-95.html'>Mixed Precision Algorithms in Numerical Linear Algebra - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>We demonstrate that standard techniques for performing multi-term addition with four or more arguments, without normalization of intermediate quantities, can be non-monotonic. Since summation is part of dot product and matrix multiplication operations, routines that have increasingly started appearing in the hardware of supercomputers, exploring monotonicity or nonmonotonicity is of wide interest. We provide details on what features multi-term addition algorithms need in order to avoid non-monotonic behaviour. We also look at algorithms that utilize multi-term summation and vector-vector, matrix-vector, or matrix-matrix operations that can have non-monotonic behaviour. We suggest that nonmonotonicity of multi-term summation, in some available hardware devices from large-scale chip companies, is a feature that may have appeared unintentionally as a consequence of design choices, and neither the fact of appearance in these devices nor that nonmonotonicity can happen in multi-term floating-point adders in general has been analysed in literature before, to the best of our knowledge.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75366' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk428' style='display:block'><a href='javascript:toggle_star(428)' class='star'><span class='star428'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D406)</b> Ichi Yamazaki, A New Mixed-Precision Benchmark for HP Computers <span id='bitlink-396'><small><a href='javascript:show_bit(396)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-396' style='display:none'><small><a href='javascript:hide_bit(396)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A New Mixed-Precision Benchmark for HP Computers</b><br />Ichi Yamazaki<br />Thursday, March 2 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-95.html'>Mixed Precision Algorithms in Numerical Linear Algebra - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>We present a new benchmark for high-performance (HP) computers.  Similar to HPCG, the new benchmark is designed to rank computers based on how fast they can solve a sparse linear system of equations. The main novelty of the new benchmark is that it is based on GMRES (combined with Geometric Multi-Grid preconditioner with Gauss Seidel smoother) and provides the flexibility to utilize lower precision arithmetic. We present our initial design of the new benchmark, its reference implementation, and its performance on current top-ranked architectures. We also discuss challenges of designing such a benchmark, along with our preliminary numerical results of using 16-bit numerical values (half and bfloat precisions) for solving a sparse linear system of equations.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75366' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk429' style='display:block'><a href='javascript:toggle_star(429)' class='star'><span class='star429'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D406)</b> Eda Oktay, Solving Total Least Squares Problems Using Mixed Precision <span id='bitlink-397'><small><a href='javascript:show_bit(397)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-397' style='display:none'><small><a href='javascript:hide_bit(397)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Solving Total Least Squares Problems Using Mixed Precision</b><br />Eda Oktay<br />Thursday, March 2 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-95.html'>Mixed Precision Algorithms in Numerical Linear Algebra - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>With the recent emergence of mixed precision hardware, there has been a  renewed interest in its use for solving numerical linear algebra  problems fast and accurately. The solution of total least squares  problems, i.e., solving $\min_{E,f} \Vert  [E, f]\Vert _F$ subject to  $(A+E)x=b+f$, arises in numerous application areas. This requires  finding the smallest singular value and corresponding right singular  vector of $[A,b]$, which is challenging when $A$ is large and sparse. An  efficient algorithm for this case due to Bj√∂rck et al. is based on  Rayleigh quotient iteration coupled with conjugate gradient  preconditioned via Cholesky factors. In this talk, we present a mixed  precision variant of this algorithm and several numerical  experiments.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75366' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk430' style='display:block'><a href='javascript:toggle_star(430)' class='star'><span class='star430'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (D406)</b> Ian McInerney, Chopblas: Simulating Mixed-Precision and Stochastically Rounded Linear Algebra <span id='bitlink-398'><small><a href='javascript:show_bit(398)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-398' style='display:none'><small><a href='javascript:hide_bit(398)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Chopblas: Simulating Mixed-Precision and Stochastically Rounded Linear Algebra</b><br />Ian McInerney<br />Thursday, March 2 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-95.html'>Mixed Precision Algorithms in Numerical Linear Algebra - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>In this talk, we present the ChopBLAS MATLAB library for simulating basic linear algebra operations using mixed precision and non-standard floating-point formats such as BFloat16, stochastic rounding, and custom formats. Existing simulation frameworks use MATLAB classes and operator overloading to implement custom precision data types, which makes implementing mixed-precision computations difficult and leads to decreased computational performance due to the overhead of MATLAB classes. Instead, ChopBLAS uses operation-level rounding, where instead of a custom data type, we store all data as double precision floating-point values, use double precision floating-point arithmetic, and then round the data to the desired precision using either the chop or cpfloat rounding function. By operating on double precision values we are able to exploit the built-in vectorization capabilities of MATLAB operations, leading to a speed-up of nearly 90x and 1050x when simulating stochastic rounding using chop and cpfloat, respectively, compared to a reference BLAS implementation in MATLAB with operation level rounding.  	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75366' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
