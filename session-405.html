<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part II of II</h2><div class='index-talk' id='talk1837' style='display:block'><a href='javascript:toggle_star(1837)' class='star'><span class='star1837'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (E104)</b> Ming-Jun Lai, The Kolmogorov Superposition Theorem can Break the Curse of Dimensionality When Approximating High Dimensional Functions <span id='bitlink-1681'><small><a href='javascript:show_bit(1681)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1681' style='display:none'><small><a href='javascript:hide_bit(1681)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>The Kolmogorov Superposition Theorem can Break the Curse of Dimensionality When Approximating High Dimensional Functions</b><br />Ming-Jun Lai<br />Wednesday, March 1 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-405.html'>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E104<br /><br /><small>We explain how to use Kolmogorov's Superposition Theorem (KST) to overcome the curse of dimensionality in approximating multi-dimensional functions and learning multi-dimensional data sets by using neural networks of two layers. That is, there is a class of functions called K-Lipschitz continuous in the sense that the K-outer function  of  is Lipschitz continuous can be approximated by a ReLU network of two layers with  widths to have an approximation order $\mathcal{O}(d^2/n)$. In addition, we show that polynomials of high degree can be expressed by using neural networks with activation function $\sigma_l(t)=(t_{+})^{l}$ with $l\geq 2$ with multiple layers and appropriate widths. More layers of neural networks, the higher degree polynomials can be reproduced. Hence, the deep learning algorithm can well approximate multi-dimensional data when the number of layers increases with high degree activation function $\sigma_l$. Finally, we present a mathematical justification for image classification by using a deep learning algorithm.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75886' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1838' style='display:block'><a href='javascript:toggle_star(1838)' class='star'><span class='star1838'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (E104)</b> Guannan Zhang, A Non-intrusive Domain-Decomposition Model Reduction Method for Linear Steady-State Partial Differential Equations with Random Coefficients <span id='bitlink-1682'><small><a href='javascript:show_bit(1682)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1682' style='display:none'><small><a href='javascript:hide_bit(1682)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A Non-intrusive Domain-Decomposition Model Reduction Method for Linear Steady-State Partial Differential Equations with Random Coefficients</b><br />Guannan Zhang<br />Wednesday, March 1 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-405.html'>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E104<br /><br /><small>Domain decomposition methods have been proved to be an effective strategy to reduce the dimension of parametric partial differential equations (PDEs). However, existing domain decomposition methods for parametric PDEs are usually intrusive, which means domain decomposition based solvers need to be implemented from scratch for each target parametric PDE. To address this issue, we develop a new non-intrusive domain-decomposition model reduction method for linear steady-state PDEs with random-field coefficients. As a variant of our previous work by Mu and Zhang, the new method only needs access to the final linear system, that is, the global stiffness matrix and the right hand side, of a deterministic PDE solver, in order to build a domain-decomposition-based reduced model without intrusive implementation from scratch. The key idea is to remove the interface condition between sub-domains and rely on the correlation between columns of the linear system to couple the sub-domains. The non-intrusive feature enables the applicability of the proposed method to a broader class of uncertainty quantification problems, where many legacy codes/solvers can be fully reused by our method. Two numerical examples including diffusion equations with random diffusivity and convection-dominated transport with random velocity, are provided to demonstrate the effectiveness and efficiency of our method.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75886' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1840' style='display:block'><a href='javascript:toggle_star(1840)' class='star'><span class='star1840'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (E104)</b> Robert R. Stephany, \texttt{pde-Learn}: Using Deep Learning to Discovery Partial Differential Equations from Noisy, Limited Data <span id='bitlink-1683'><small><a href='javascript:show_bit(1683)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1683' style='display:none'><small><a href='javascript:hide_bit(1683)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>\texttt{pde-Learn}: Using Deep Learning to Discovery Partial Differential Equations from Noisy, Limited Data</b><br />Robert R. Stephany<br />Wednesday, March 1 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-405.html'>Learning Deep Neural Networks and Sparse Approximations from Limited Data for High-Dimensional Problems in Computational Science and Engineering - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E104<br /><br /><small>Scientific progress is contingent upon finding predictive models for the physical world. In this paper, we introduce PDE-LEARN, a novel PDE-discovery algorithm that can identify PDEs directly from noisy, limited measurements of a physical system governed by a hidden PDE. PDE-LEARN uses a Rational Neural Network, $U$, to approximate the system response function and a sparse, trainable vector $\xi$ to characterize the hidden PDE. Our approach couples the training of $U$ and $\xi$ using a specially designed loss function that (1) makes $U$ approximate the system response function, (2) encapsulates the fact that $U$ satisfies a hidden PDE that $\xi$ characterizes, and (3) promotes sparsity in $\xi$ using ideas from iteratively reweighted least-squares. This approach yields a robust algorithm that applies to many physical systems. We demonstrate the efficacy of PDE-LEARN by identifying several PDEs from noisy and limited measurements.  	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75886' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
