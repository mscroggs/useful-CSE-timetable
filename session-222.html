<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Sparsity-Promoting Bayesian Inverse Problems</h2><div class='index-talk' id='talk1013' style='display:block'><a href='javascript:toggle_star(1013)' class='star'><span class='star1013'>&star;</span></a> <b>9:20 AM&ndash;9:35 AM (D506)</b> Daniela Calvetti, Bayesian Sparse Dictionary Learning <span id='bitlink-932'><small><a href='javascript:show_bit(932)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-932' style='display:none'><small><a href='javascript:hide_bit(932)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Bayesian Sparse Dictionary Learning</b><br />Daniela Calvetti<br />Friday, March 3 9:20 AM&ndash;9:35 AM<br />This is the 1st talk in <a href='session-222.html'>Sparsity-Promoting Bayesian Inverse Problems</a> (9:20 AM&ndash;11:00 AM)<br />D506<br /><br /><small>Dictionary learning can be seen as a possible data-driven alternative to solve inverse problems by identifying the data with possible outputs that are either generated numerically using a forward model or the results of earlier observations of controlled experiments.  Sparse dictionary learning is particularly interesting when the underlying  signal is known to be representable in terms of a few vectors in a given basis. In this talk we propose to use hierarchical Bayesian models for sparse dictionary learning that can capture features of the underlying signals, e.g., sparse representation and nonnegativity. The same framework can be employed to reduce the dimensionality of an annotated  dictionary through feature extraction, thus reducing the computational complexity of the learning task. Computed examples where our algorithms are applied to hyperspectral imaging and classification of electrocardiogram (ECG) will be presented.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75540' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1014' style='display:block'><a href='javascript:toggle_star(1014)' class='star'><span class='star1014'>&star;</span></a> <b>9:40 AM&ndash;9:55 AM (D506)</b> Alexander Strang, Varying Map Estimators with Varying Assumptions in Sparsity Promoting Gaussian Hierarchical Models <span id='bitlink-933'><small><a href='javascript:show_bit(933)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-933' style='display:none'><small><a href='javascript:hide_bit(933)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Varying Map Estimators with Varying Assumptions in Sparsity Promoting Gaussian Hierarchical Models</b><br />Alexander Strang<br />Friday, March 3 9:40 AM&ndash;9:55 AM<br />This is the 2nd talk in <a href='session-222.html'>Sparsity-Promoting Bayesian Inverse Problems</a> (9:20 AM&ndash;11:00 AM)<br />D506<br /><br /><small>Maximum a posteriori (MAP) estimation, like all Bayesian methods, depends on prior assumptions. These assumptions are often chosen to promote specific features in the recovered estimate like sparsity. The form of the chosen prior determines the shape of the posterior distribution, thus the behavior of the estimator, and the complexity of the associated optimization problem. Here, we consider a family of Gaussian hierarchical models with generalized gamma hyperpriors designed to promote sparsity in linear inverse problems. By varying the hyperparameters we can move continuously between priors that act as smoothed $\ell_p$ penalties with flexible $p$, smoothing, and scale. We then introduce methods for tracking MAP solution paths along paths through hyper parameter space. Path following allows a user to explore the space of possible MAP solutions under varying assumptions and to test the robustness and sensitivity of solutions to changes in the prior assumptions. By tracing paths from a convex region to a non-convex region, the user can find local minimizers in strongly sparsity promoting regimes that are consistent with a convex relaxation of the same problem derived using a consistent family of prior assumptions. We show experimentally that these solutions are less error prone than direct optimization of the non-convex problem.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75540' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1015' style='display:block'><a href='javascript:toggle_star(1015)' class='star'><span class='star1015'>&star;</span></a> <b>10:00 AM&ndash;10:15 AM (D506)</b> Jonathan Lindbloom, Computational Strategies for Bayesian Inversion with Conditionally Gaussian Sparsity Priors <span id='bitlink-934'><small><a href='javascript:show_bit(934)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-934' style='display:none'><small><a href='javascript:hide_bit(934)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Computational Strategies for Bayesian Inversion with Conditionally Gaussian Sparsity Priors</b><br />Jonathan Lindbloom<br />Friday, March 3 10:00 AM&ndash;10:15 AM<br />This is the 3rd talk in <a href='session-222.html'>Sparsity-Promoting Bayesian Inverse Problems</a> (9:20 AM&ndash;11:00 AM)<br />D506<br /><br /><small>In many practical Bayesian linear inverse problems, important parameters such as the noise covariance and the ideal strength of regularization are unknown a priori. Furthermore, adopting a sparse Bayesian learning (SBL) approach we may wish to employ a sparsity-promoting prior that strongly promotes sparsity in a linear transform of the unknown. It has been shown that both concerns may be addressed using hierarchical Bayesian models that employ conditionally Gaussian priors with various choices of hyper-priors. However, the resulting posterior densities are typically no longer log-concave, which creates convergence concerns when using either optimization or sampling to characterize the posterior distribution. With this in mind, here we study iterative algorithms for posterior point estimation, particularly with regards to more general measurement and regularization settings. In particular, we consider the scenario that the noise covariance of one or several data sources are unknown and made part of the inference procedure, and when sparsity is promoted in multiple transforms of the unknown. We then apply recent data-augmentation techniques to accelerate the methods for large-scale problems such as image-deblurring and synthetic aperture radar (SAR) despeckling. We demonstrate the performance of our methods on a suite of numerical examples.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75540' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1016' style='display:block'><a href='javascript:toggle_star(1016)' class='star'><span class='star1016'>&star;</span></a> <b>10:20 AM&ndash;10:35 AM (D506)</b> Rafael Flock, Certified Coordinate Selection for Linear Bayesian Inversion with Laplace Prior <span id='bitlink-935'><small><a href='javascript:show_bit(935)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-935' style='display:none'><small><a href='javascript:hide_bit(935)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Certified Coordinate Selection for Linear Bayesian Inversion with Laplace Prior</b><br />Rafael Flock<br />Friday, March 3 10:20 AM&ndash;10:35 AM<br />This is the 4th talk in <a href='session-222.html'>Sparsity-Promoting Bayesian Inverse Problems</a> (9:20 AM&ndash;11:00 AM)<br />D506<br /><br /><small>In this talk we are presenting a new and comprehensive method to tackle large-dimensional linear Bayesian inverse problems with Gaussian likelihood and Laplace prior. The unknown parameter is represented on an ad hoc basis the dimension of which can be very large. The inverse problem is then formulated w.r.t. the basis coefficients (coordinates), and we show how to select a possibly small subset of them which is most informative w.r.t. the data relative to the heavy-tailed prior. To this end, we propose a novel technique for the diagnosis of the contribution of each basis coefficient that is based on the MAP of the exact posterior density. In the new framework, we perform likelihood-informed dimension reduction by constructing efficiently a low-dimensional approximation to the likelihood function which accounts for the informed coefficients only. In addition, we provide a tractable upper bound on the resulting approximation error measured with the Hellinger distance. We show how MCMC sampling from the approximated but also exact posterior can be accelerated by using pseudo marginal MCMC sampling that is performed only on the selected set of coordinates. In the end of this talk, we present numerical results from examples in computed tomography and image deblurring that affirm the versatility of our method.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75540' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1017' style='display:block'><a href='javascript:toggle_star(1017)' class='star'><span class='star1017'>&star;</span></a> <b>10:40 AM&ndash;10:55 AM (D506)</b> Jasper M. Everink, Sparse Bayesian Inference with Regularized Gaussians <span id='bitlink-936'><small><a href='javascript:show_bit(936)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-936' style='display:none'><small><a href='javascript:hide_bit(936)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Sparse Bayesian Inference with Regularized Gaussians</b><br />Jasper M. Everink<br />Friday, March 3 10:40 AM&ndash;10:55 AM<br />This is the 5th talk in <a href='session-222.html'>Sparsity-Promoting Bayesian Inverse Problems</a> (9:20 AM&ndash;11:00 AM)<br />D506<br /><br /><small>In this talk, we will present a method for Bayesian inference that, unlike many existing Bayesian methods, results in posterior distributions that assign positive probability to sparse vectors. We combine Gaussian distributions with the deterministic effects of sparsity-inducing regularization like $l_1$ norms, total variation and/or constraints. The resulting posterior distributions assign positive probability to various low-dimensional subspaces and therefore promote sparsity. Samples from this distribution can be generated by solving regularized linear least-squares problems with properly chosen data perturbations. We will discuss some properties of the underlying prior and use this methodology to derive an efficient algorithm for sampling from a Bayesian hierarchical model with sparsity structure.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75540' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
