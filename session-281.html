<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Codesigning a Probabilistic Computing Future: Applications and Algorithms - Part I of II</h2><div class='index-talk' id='talk1277' style='display:block'><a href='javascript:toggle_star(1277)' class='star'><span class='star1277'>&star;</span></a> <b>9:20 AM&ndash;9:35 AM (G104)</b> Darby Smith, Sampling Distributions from Biased Coins <span id='bitlink-1181'><small><a href='javascript:show_bit(1181)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1181' style='display:none'><small><a href='javascript:hide_bit(1181)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Sampling Distributions from Biased Coins</b><br />Darby Smith<br />Friday, March 3 9:20 AM&ndash;9:35 AM<br />This is the 1st talk in <a href='session-281.html'>Codesigning a Probabilistic Computing Future: Applications and Algorithms - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G104<br /><br /><small>Flips of a fair coin can be exploited to approximate outcomes of distributions in expected ways.  Fair coins can even be used to simulate the flips of biased coins, and biased coins can also be exploited to sample distributions. Deterministic computing and PRNGs are able to exploit such simple relations. Stochastic devices that exhibit two state behavior, however, may not be able to be reliably classified as a Bernoulli coin flip with a fixed probability.  Devices may behave statistically as such a coin over many, many draws, but they may actually fluctuate between values or behave in some other pathological way. This talk examines the consequences of utilizing a noisy device for probabilistic computation under the assumption it is behaving as a fair or biased Bernoulli coin flip when the underlying dynamics are actually much more complex. We aim to categorize such a TRNG under appropriate metrics analogous to those used for PRNG today. We conclude with a discussion on how rich dynamics of noisy two state devices may be exploited for probabilistic computation.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75637' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1278' style='display:block'><a href='javascript:toggle_star(1278)' class='star'><span class='star1278'>&star;</span></a> <b>9:40 AM&ndash;9:55 AM (G104)</b> Emre Neftci, Neural Sampling Machine with Stochastic Synapse Allows Brain-Like Learning and Inference <span id='bitlink-1182'><small><a href='javascript:show_bit(1182)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1182' style='display:none'><small><a href='javascript:hide_bit(1182)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Neural Sampling Machine with Stochastic Synapse Allows Brain-Like Learning and Inference</b><br />Emre Neftci<br />Friday, March 3 9:40 AM&ndash;9:55 AM<br />This is the 2nd talk in <a href='session-281.html'>Codesigning a Probabilistic Computing Future: Applications and Algorithms - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G104<br /><br /><small>Estimating the confidence of neural networks predictions from real-world data is a critical requirement for mission-critical AI technologies. The brain is capable of such uncertainty estimation from data, but the principles underlying this feat and its implementation in a compact, low-power hardware remains a challenge. In this presentation, I will first introduce Neural Sampling Machines (NSM), a stochastic binary neural network inspired by the stochastic behavior of biological neurons that uses to perform probabilistic inference. We demonstrate a novel hardware fabric that can implement NSMs by exploiting the stochasticity in the synaptic connections. We experimentally demonstrate an hybrid stochastic synapse by pairing a ferroelectric field-effect transistor (FeFET)-based analog weight cell with a two-terminal stochastic selector element.   We show that the stochastic switching characteristic of the selector between the insulator and the metallic states resembles the multiplicative synaptic noise of the NSM. The stochastic NSM can not only perform highly accurate image classification with 98.25% accuracy on standard MNIST dataset, but also estimate the uncertainty in prediction (measured in terms of the entropy of prediction) when the digits of the MNIST dataset are rotated. Building such a probabilistic hardware platform that can support neuroscience inspired models can enhance the learning and inference capability of the current artificial intelligence (AI).    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75637' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1279' style='display:block'><a href='javascript:toggle_star(1279)' class='star'><span class='star1279'>&star;</span></a> <b>10:00 AM&ndash;10:15 AM (G104)</b> Christian Frasser, Stochastic Computing: From Classical to Morphological Neural Networks <span id='bitlink-1183'><small><a href='javascript:show_bit(1183)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1183' style='display:none'><small><a href='javascript:hide_bit(1183)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Stochastic Computing: From Classical to Morphological Neural Networks</b><br />Christian Frasser<br />Friday, March 3 10:00 AM&ndash;10:15 AM<br />This is the 3rd talk in <a href='session-281.html'>Codesigning a Probabilistic Computing Future: Applications and Algorithms - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G104<br /><br /><small>During the last few years, Stochastic Computing (SC) has emerged as a promising design technique for the implementation of energy-efficient Artificial Neural Networks (ANN) hardware. This efficiency is mainly due to the possibility of parallelizing the network through the use of SC and, therefore, minimizing its dependence on a central memory node. In the classical ANN implementation, the SC circuit is adapted to the standard ANN design essentially exploiting the ability to reduce a multiplication within a single logic gate. However, SC has many other advantages such as its ability to implement maximum and minimum functions also with individual gates. Therefore, the possibility of searching for the NN paradigm that best suits the capabilities of SC (and therefore using both products and max-min functions) is opened. In this work we show that NNs known as Morphological Neural Networks (MNN) are ideal to be implemented in hardware using SC. MNNs can be implemented without using activation functions and combine products, sums, maximum and minimum functions. They have been studied in the literature by some researchers in recent years and present the property of universal approximation. An FPGA implementation of a large SC-based MNN is presented and compared with other hardware methodologies in terms of energy efficiency and performance. The results show that the proposed SC-based MNN is as competitive as binary neural networks.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75637' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1280' style='display:block'><a href='javascript:toggle_star(1280)' class='star'><span class='star1280'>&star;</span></a> <b>10:20 AM&ndash;10:35 AM (G104)</b> Hashibah Hamid, A New Strategy in Developing Location Model <span id='bitlink-1184'><small><a href='javascript:show_bit(1184)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1184' style='display:none'><small><a href='javascript:hide_bit(1184)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A New Strategy in Developing Location Model</b><br />Hashibah Hamid<br />Friday, March 3 10:20 AM&ndash;10:35 AM<br />This is the 4th talk in <a href='session-281.html'>Codesigning a Probabilistic Computing Future: Applications and Algorithms - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G104<br /><br /><small>The location model (LM) is designed to enable classification when a dataset contains both continuous and categorical variables. Due to the issue of empty cells, a smoothed location model (smoothed LM) is introduced. However, the smoothing process caused changes in the original information of the non-empty cells. It is well known that original information is valuable and important that should be maintained. Thus, a new strategy is proposed by amalgamating of maximum likelihood and smoothing estimations to construct a new LM. Consequently, maximum likelihood estimation will be used if the cell was found to be non-empty, otherwise smoothing estimation will be used instead. The analysis shows that the newly constructed LM can provide optimal classification results and demonstrates better performance compared to the old models, i.e. classical LM and smoothed LM, where the estimation used is based on the cell’s conditions. The new proposed strategy of parameter estimation could handle all situations; whether the cells are empty or not, limited sample size with many variables measured mainly the binary.      </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75637' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
