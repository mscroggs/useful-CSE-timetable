<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Novel Numerical Methods for Partial Differential Equations - Part II of II</h2><div class='index-talk' id='talk250' style='display:block'><a href='javascript:toggle_star(250)' class='star'><span class='star250'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (G107)</b> Kris van Der Zee, Neural Control of Discrete Weak Formulations: Galerkin, Least-Squares and Minimal-Residual Methods with Quasi-Optimal Weights <span id='bitlink-232'><small><a href='javascript:show_bit(232)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-232' style='display:none'><small><a href='javascript:hide_bit(232)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Neural Control of Discrete Weak Formulations: Galerkin, Least-Squares and Minimal-Residual Methods with Quasi-Optimal Weights</b><br />Kris van Der Zee<br />Monday, February 27 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-55.html'>Novel Numerical Methods for Partial Differential Equations - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>There is tremendous potential in using neural networks to optimize numerical methods. In this talk, I will introduce and analyse a framework [1] for the neural optimization of discrete weak formulations, suitable for finite element methods. The main idea of the framework is to include a neural-network function acting as a control variable in the weak form. Finding the neural control that (quasi-) minimizes a suitable cost (or loss) functional, then yields a numerical approximation with desirable attributes. In particular, the framework allows in a natural way the incorporation of known data of the exact solution, or the incorporation of stabilization mechanisms (e.g., to remove spurious oscillations).     
The main result of the analysis pertains to the well-posedness and convergence of the associated constrained-optimization problem. In particular, under certain conditions, discrete weak forms are stable, and quasi-minimizing neural controls exist, which converge quasi-optimally. The analysis results are specialized to Galerkin, least- squares and minimal-residual formulations, where the neural-network dependence appears in the form of suitable weights. Elementary numerical experiments support our findings and demonstrate the potential of the framework.     
[1] Brevis, Muga, Van der Zee, Neural Control of Discrete Weak Formulations: Galerkin, Least-Squares and Minimal-Residual Methods with Quasi-Optimal Weights, arXiv:2206.07475, (2022)    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75281' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk251' style='display:block'><a href='javascript:toggle_star(251)' class='star'><span class='star251'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (G107)</b> Francesc Verdugo, A Data-Oriented Programming Model for Massively Parallel Finite Element Computations <span id='bitlink-233'><small><a href='javascript:show_bit(233)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-233' style='display:none'><small><a href='javascript:hide_bit(233)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A Data-Oriented Programming Model for Massively Parallel Finite Element Computations</b><br />Francesc Verdugo<br />Monday, February 27 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-55.html'>Novel Numerical Methods for Partial Differential Equations - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>The development of MPI-based finite element (FE) applications is significantly more involved than for their sequential counterparts. Some bugs can be tracked with conventional tools in a single MPI rank, but genuine parallel errors often need to be debugged in parallel. Fixing an application that requires at least 100 ranks to crash can be very challenging, even with state-of-the-art parallel debuggers. We developed a novel data-oriented parallel programming model that allows one to express parallel algorithms in a generic way, without explicitly relaying on MPI communication directives. This makes possible to use different back-ends to run the generic parallel algorithms. With a sequential back-end, the data structures are logically parallel from the user perspective, but they are processed using a single rank. This makes possible to use serial debuggers for code development, which dramatically improves the user experience. Once the code has been debugged with the sequential back-end, it can be deployed in a supercomputer using a MPI back-end. Using the new model, we have implemented distributed vectors, sparse matrices, and other functionality needed in distributed-memory FE computations. They are implemented in Julia and made freely available in github [github.com/fverdugo/PartitionedArrays.jl]. With these tools, we were able to solve large FE computations with nearly optimal weak and strong scaling up to tends of thousands of CPU cores [doi.org/10.21105/joss.04157].    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75281' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk252' style='display:block'><a href='javascript:toggle_star(252)' class='star'><span class='star252'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (G107)</b> Santiago Badia, Synergies Between Neural Networks and Finite Elements <span id='bitlink-234'><small><a href='javascript:show_bit(234)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-234' style='display:none'><small><a href='javascript:hide_bit(234)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Synergies Between Neural Networks and Finite Elements</b><br />Santiago Badia<br />Monday, February 27 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-55.html'>Novel Numerical Methods for Partial Differential Equations - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>The direct numerical approximation of PDEs using neural networks is gaining lots of attraction, but many problems still need to be solved. One is the numerical integration of the resulting terms (which involves integrating the neural network and its derivatives). Monte Carlo is the most common approach but suffers a low convergence rate. Another problem is the required regularity of the neural network to end up with a differentiable functional. E.g., ReLU activation functions are not regular enough, even for weak formulations. In this presentation, we will discuss how to design effective adaptive quadratures for smooth enough activation functions and compare the proposed approach with Monte Carlo methods.    
Alternatively, we explore using interpolated neural networks on finite element spaces to compute the PDE loss function instead of dealing with the neural network itself. Doing this, the integration is straightforward. However, this approach relies on a mesh. In this situation, we also discuss the ill-posedness of standard PDE losses and how to design well-posed preconditioned losses. We show how by combining standard finite element preconditioners and neural networks, we can substantially accelerate the training process, one of the most severe issues of neural network PDE approximations.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75281' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk253' style='display:block'><a href='javascript:toggle_star(253)' class='star'><span class='star253'>&star;</span></a> <b>2:50 PM&ndash;3:05 PM (G107)</b> Martin Berggren, CutFEM for Acoustic Shape Optimization: Features and Surprises <span id='bitlink-235'><small><a href='javascript:show_bit(235)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-235' style='display:none'><small><a href='javascript:hide_bit(235)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>CutFEM for Acoustic Shape Optimization: Features and Surprises</b><br />Martin Berggren<br />Monday, February 27 2:50 PM&ndash;3:05 PM<br />This is the 4th talk in <a href='session-55.html'>Novel Numerical Methods for Partial Differential Equations - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />G107<br /><br /><small>Fictitious-domain methods like CutFEM are nonstandard but attractive alternatives for shape optimization, as they bypass the need for mesh deformation or regeneration. The so-called boundary expressions for shape directional derivatives turn out to be exact for fictitious-domain methods. However, when mesh deformations are used, only the more cumbersome volume expressions turn out to be exact. Recent numerical experiences using CutFEM together with level-set geometry descriptions indicate less parameterization-induced artificial stiffness along boundaries compared to previously used methods. Rerunning previous optimization studies with the new method led to the discovery of unexpected but acoustically very beneficial subwavelength patterns when optimizing acoustic horns. In a 3D application, an analogous CutFEM approach was used to optimize the shape of the phase plug of a compression driver, the standard sound source of acoustic horns. The optimization used a base design that currently is assumed to be unpractical, due to the lack of simple design guidelines. However, after shape optimization, it turned out that this base design very well may be superior to the industry standard design.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75281' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
