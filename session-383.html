<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Cross-Cutting Aspects of Utilizing Exascale Platforms for High-Fidelity CFD - Part II of II</h2><div class='index-talk' id='talk1738' style='display:block'><a href='javascript:toggle_star(1738)' class='star'><span class='star1738'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (G106)</b> Martin Karp, Numerical Precision in High-Fidelity Computational Fluid Dynamics <span id='bitlink-1591'><small><a href='javascript:show_bit(1591)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1591' style='display:none'><small><a href='javascript:hide_bit(1591)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Numerical Precision in High-Fidelity Computational Fluid Dynamics</b><br />Martin Karp<br />Thursday, March 2 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-383.html'>Cross-Cutting Aspects of Utilizing Exascale Platforms for High-Fidelity CFD - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />G106<br /><br /><small>The impact of numerical precision, both with regard to accuracy and potential performance improvements, has garnered increased interest in several different domains of computational science. However, it is an open question what numerical precision is required for a specific simulation in computational fluid dynamics (CFD). While we have conventionally relied on double-precision arithmetics, several factors affect what numerical precision is required to obtain a resolved flow field. In this presentation, we talk about our work to assess how the numerical precision requirements depend on the fluid scales and simulation size, both in theory and practice. We investigate how numerical precision affects a series of different flow cases and try to isolate the impact of numerical precision and connect it to the dynamics and properties of the considered flow cases. To do this we perform experiments with different discretization schemes and perform a series of computational experiments. We focus on direct numerical simulations and shed some light on what machine epsilon is required in the best case to perform a resolved CFD simulation. Finally, we address whether even higher numerical precision will be required in order to perform direct numerical simulations on exascale and post-exascale parallel systems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75849' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1739' style='display:block'><a href='javascript:toggle_star(1739)' class='star'><span class='star1739'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (G106)</b> Samuel Kemmler, Walberla: a Multi-Physics Open-Source Software Framework for Scalable and Efficient CFD <span id='bitlink-1592'><small><a href='javascript:show_bit(1592)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1592' style='display:none'><small><a href='javascript:hide_bit(1592)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Walberla: a Multi-Physics Open-Source Software Framework for Scalable and Efficient CFD</b><br />Samuel Kemmler<br />Thursday, March 2 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-383.html'>Cross-Cutting Aspects of Utilizing Exascale Platforms for High-Fidelity CFD - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />G106<br /><br /><small>waLBerla is a modern open-source massively parallel multiphysics simulation framework with a focus on CFD applications.  It uses the lattice Boltzmann method (LBM), which is an alternative to classical Navier-Stokes solvers for computational fluid dynamics simulations.  waLBerla scales on some of the top clusters in the world due to carefully designed distributed data structures.  As a showcase, we use particle-resolved sediment transport simulations using LBM and the discrete element method (DEM).  In this scenario, the LBM simulation dominates the computational cost.  In our implementation, the LBM simulation runs on the GPU and the DEM simulation on the CPU.  We analyze the performance on a heterogeneous compute cluster.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75849' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1740' style='display:block'><a href='javascript:toggle_star(1740)' class='star'><span class='star1740'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (G106)</b> Niclas Jansson, Refactoring Legacy Fortran Applications to Leverage Modern Heterogeneous Architectures in Extreme-Scale CFD <span id='bitlink-1593'><small><a href='javascript:show_bit(1593)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1593' style='display:none'><small><a href='javascript:hide_bit(1593)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Refactoring Legacy Fortran Applications to Leverage Modern Heterogeneous Architectures in Extreme-Scale CFD</b><br />Niclas Jansson<br />Thursday, March 2 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-383.html'>Cross-Cutting Aspects of Utilizing Exascale Platforms for High-Fidelity CFD - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />G106<br /><br /><small>Recent trends and advancement in including more diverse and heterogeneous hardware in High-Performance Computing is challenging software developers in their pursuit for good performance and numerical stability. The well-known maxim "software outlives hardware" may no longer necessarily hold true, and developers are today forced to re-factor their codebases to leverage these powerful new heterogeneous systems.   In this talk, we present Neko - a portable framework for high-order spectral element flow simulations. Unlike prior works, Neko adopts a modern object-oriented Fortran 2008 approach, allowing multi-tier abstractions of the solver stack and facilitating various hardware backends ranging from general-purpose processors, accelerators down to exotic vector processors and Field-Programmable Gate Arrays (FPGAs) via Neko's device abstraction layer.   Focusing on Neko's performance, scalability, and accuracy, we present performance measurements on a wide range of accelerated computing platforms, including the EuroHPC pre-exascale system LUMI, for performing large-scale direct numerical simulation (DNS) of turbulent fluid flow.    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75849' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1741' style='display:block'><a href='javascript:toggle_star(1741)' class='star'><span class='star1741'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (G106)</b> Malachi Phillips, Improving Parallel Scalability of High-Order Preconditioners on GPUs <span id='bitlink-1594'><small><a href='javascript:show_bit(1594)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1594' style='display:none'><small><a href='javascript:hide_bit(1594)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Improving Parallel Scalability of High-Order Preconditioners on GPUs</b><br />Malachi Phillips<br />Thursday, March 2 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-383.html'>Cross-Cutting Aspects of Utilizing Exascale Platforms for High-Fidelity CFD - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />G106<br /><br /><small>Matrix-free methods for the solution of linear systems  arising from high-order finite element discretizations of partial differential equations,  such as the Poisson equation, require robust preconditioners.  Two classes of preconditioners prove most effective:  multigrid and low-order finite element methods.  For multigrid preconditioners built on geometric $p$-multigrid  on GPU architectures,  the communication dominated coarse grid solve hinders the parallel scalability  of the preconditioner.  We propose a strategy to mitigate the increase coarse grid solve cost at scale  by treating the coarse grid solve as an additive, rather than multiplicative, correction.  This allows overlapping the coarse grid solve, which utilizes the CPU,  and the remainder of the multigrid cycle, which utilizes the GPU.  A hybrid $p$-multigrid and low-order finite element preconditioner  is also proposed.  We demonstrate the effectiveness of these two novel approaches on a variety of  problems arising from the spectral element discretization of the Navier-Stokes equations,  spanning up to $P=6144$ NVIDIA V100 GPUs on Summit.    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75849' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1742' style='display:block'><a href='javascript:toggle_star(1742)' class='star'><span class='star1742'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (G106)</b> Kartik Jain, Studies of Transition to Turbulence in Oscillatory Cerebrospinal Fluid Flow <span id='bitlink-1595'><small><a href='javascript:show_bit(1595)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1595' style='display:none'><small><a href='javascript:hide_bit(1595)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Studies of Transition to Turbulence in Oscillatory Cerebrospinal Fluid Flow</b><br />Kartik Jain<br />Thursday, March 2 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-383.html'>Cross-Cutting Aspects of Utilizing Exascale Platforms for High-Fidelity CFD - Part II of II</a> (9:45 AM&ndash;11:25 AM)<br />G106<br /><br /><small>Presence of turbulence like flow fluctuations in physiologic flows has been  known since a long time. In this work we study oscillatory flow in stenosed  geometries to quantify and characterize the onset of turbulence in zero mean  oscillatory flows.    
Simulations on a stenosed pipe in axisymmetric and eccentric configurations  were conducted using the LBM solver Musubi with various Reynolds numbers and  pulsation frequencies. Stenosis with area reduction of 75%, 60%, 50% and 25%  were studied in both axisymmetric and eccentric configurations. Meshes of up to  2.8 billion cells were created and simulations were conducted on 300’000 CPU  cores of the SuperMUC-NG petascale system in Munich, GERMANY.    
Main findings include:  1. The flow transitions only in higher degrees of stenosis namely 50%, 60% and  75%, where Re~1800 is the approximate threshold for transition.  2. The flow reversal stabilizes the flow field.  3. A higher pulsation frequency leads to earlier breakdown of flow – a phenomenon  that is seen mostly for lower stenoses degrees.  4. The eccentricity of the stenosis is one of the major factors for flow  transition.    
The results advocate that transition to turbulence is a possibility in  physiologic flows, and the geometry of the conduit is the most prominent factor  that results in the onset of turbulence.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75849' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
