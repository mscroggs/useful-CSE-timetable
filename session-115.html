<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Reduced-Complexity Models for Fluid Flows - Part II of II</h2><div class='index-talk' id='talk525' style='display:block'><a href='javascript:toggle_star(525)' class='star'><span class='star525'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (E103)</b> Kai Fukami, Super-Resolving Turbulent Flows with Machine Learning: a Survey <span id='bitlink-484'><small><a href='javascript:show_bit(484)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-484' style='display:none'><small><a href='javascript:hide_bit(484)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Super-Resolving Turbulent Flows with Machine Learning: a Survey</b><br />Kai Fukami<br />Monday, February 27 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-115.html'>Reduced-Complexity Models for Fluid Flows - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E103<br /><br /><small>Machine-learning-based super resolution has become a powerful tool for turbulent flows.  Super resolution reconstructs fine-scale structures from their coarse input.  This concept amounts not only to sparse reconstruction but can also be related to sub-grid scale modeling for turbulent flow simulations.  We present a case study of machine-learning-based super-resolution analysis of turbulent flows to discuss its capabilities and extensions for a range of fluid mechanics problems.  Convolutional neural network-based methods are used for the present survey of machine-learning-based super resolution.  We find that embedding scale invariance is important in the construction of machine-learning model.  Furthermore, physics-based cost function can greatly assist with the reconstruction in terms of accuracy and robustness against noisy low-resolution input.  Towards the end of the presentation, we will also discuss the challenges and outlook of machine-learning-based super-resolution reconstruction with turbulence.     </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75391' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk526' style='display:block'><a href='javascript:toggle_star(526)' class='star'><span class='star526'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (E103)</b> Shervin Bagheri, Towards Learning to Rank and Decompose Rough Surfaces Based on Drag Penalty <span id='bitlink-485'><small><a href='javascript:show_bit(485)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-485' style='display:none'><small><a href='javascript:hide_bit(485)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Towards Learning to Rank and Decompose Rough Surfaces Based on Drag Penalty</b><br />Shervin Bagheri<br />Monday, February 27 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-115.html'>Reduced-Complexity Models for Fluid Flows - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E103<br /><br /><small>Turbulent flows over irregular rough surfaces are ubiquitous in both nature and industry. Roughness increases the momentum transfer near the wall and the hydrodynamic drag on the wall. The increase of drag depends on various topographic features. Several studies have investigated the effects of statistical parameters of roughness topography on the increase of hydrodynamic drag. However, no generally applicable statistical model exists that can accurately predict drag on various rough surfaces (e.g., gaussian surfaces, positively and negatively skewed surfaces, etc.).  Moreover, many models struggle to properly " rank” the rough surfaces based on drag penalty. In practice, ranking the drag penalties from rough surfaces is generally more critical than estimating the absolute value of drag itself. This presentation will introduce attempts to “rank” rough surfaces using various machine learning techniques based on their drag penalties. In addition, we will discuss distinctive roughness structures suspected to increase drag significantly by decomposing modes of feature maps in convolutional neural networks.     
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75391' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk527' style='display:block'><a href='javascript:toggle_star(527)' class='star'><span class='star527'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (E103)</b> Luca Magri, Interpretable Nonlinear Reduced-Order Modelling with Autoencoders <span id='bitlink-486'><small><a href='javascript:show_bit(486)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-486' style='display:none'><small><a href='javascript:hide_bit(486)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Interpretable Nonlinear Reduced-Order Modelling with Autoencoders</b><br />Luca Magri<br />Monday, February 27 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-115.html'>Reduced-Complexity Models for Fluid Flows - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E103<br /><br /><small>Autoencoders are machine-learning methods that enable a reduced- order representation of data. They consist of an encoder, which compresses the data in a latent space, and a decoder, which decompresses the data back to the original space. If only linear operations are performed during the encoding and decoding phases, an autoencoder can learn the principal components of the data. On the other hand, if nonlinear activations functions are employed, an autoencoder learns a nonlinear model of the data in the latent space.    
The interpretability of the latent space, however, is not yet fully established. In this work, we physically interpret the latent space with simple tools from differential geometry. The interpretation is employed on canonical turbulent flows, i.e., the Kolmogorov flow and the minimal flow unit. The results show that the autoencoder learns the optimal submanifold in which the reduced-order dynamics is well represented. The latent variables are exploited for reducing the model's complexity whilst keeping optimal accuracy on the spatiotemporal dynamics. This work opens opportunities for extracting physical insight from the latent space and for nonlinear model reduction.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75391' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk528' style='display:block'><a href='javascript:toggle_star(528)' class='star'><span class='star528'>&star;</span></a> <b>2:50 PM&ndash;3:05 PM (E103)</b> Jean-Christophe Loiseau, Submodular Optimization for Near-Optimal Sensor and Actuator Placement <span id='bitlink-487'><small><a href='javascript:show_bit(487)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-487' style='display:none'><small><a href='javascript:hide_bit(487)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Submodular Optimization for Near-Optimal Sensor and Actuator Placement</b><br />Jean-Christophe Loiseau<br />Monday, February 27 2:50 PM&ndash;3:05 PM<br />This is the 4th talk in <a href='session-115.html'>Reduced-Complexity Models for Fluid Flows - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E103<br /><br /><small>Adequately choosing sensors to monitor a high-dimensional system is of utmost importance in numerous situations, most notably for feedback control applications. Yet, optimal sensor and actuator placement is a rapidly intractable combinatorial problem. Following an optimal design strategy, near-optimal selections can still be obtained using various convex relaxations of the otherwise combinatorial problem. One major drawback however is the limited scalability of the algorithms used to solve these convex problems in practice, or their long time to solution.  Based on the theory of modular and submodular functions, this contribution will present different algorithms for sensor selection having extremely high computational efficiency along with near-optimal guarantees. We will focus in particular on D-optimality and compare the performances of a semi-definite programming relaxation of the problem against that of the corresponding submodular relaxation and SupSub approximation.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75391' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk529' style='display:block'><a href='javascript:toggle_star(529)' class='star'><span class='star529'>&star;</span></a> <b>3:10 PM&ndash;3:25 PM (E103)</b> Kamila Zdybal, Reduced-Order Modeling with a Regression-Aware Autoencoder <span id='bitlink-488'><small><a href='javascript:show_bit(488)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-488' style='display:none'><small><a href='javascript:hide_bit(488)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Reduced-Order Modeling with a Regression-Aware Autoencoder</b><br />Kamila Zdybal<br />Monday, February 27 3:10 PM&ndash;3:25 PM<br />This is the 5th talk in <a href='session-115.html'>Reduced-Complexity Models for Fluid Flows - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E103<br /><br /><small>The first step in reduced-order modeling (ROM) workflows is finding a low-dimensional representation of a highly-dimensional system. The second step of ROM often requires training a nonlinear regression model to predict physical quantities of interest from the reduced representation. Much of the research on training ROMs thus far has tackled those two steps separately. While they both come with their challenges, a good-quality low-dimensional system representation usually facilitates building a regression model. In this work, we leverage the link between dimensionality reduction and nonlinear regression. We propose an approach where dimensionality reduction and nonlinear regression are considered jointly within an autoencoder-like neural-network architecture. The dimensionality reduction (encoding) is affected by forcing accurate regression (decoding) of the quantities of interest. We show that such a joint architecture leads to improved low-dimensional representations as the two steps communicate with each other through backpropagation. We apply our regression-aware autoencoder on test cases coming from reacting and non-reacting flow systems. The relevant quantities of interest are the important state variables and highly nonlinear source terms required by the reduced model. The proposed approach can serve as an effective replacement of standalone dimensionality-reduction techniques whenever nonlinear regression is anticipated in the downstream use.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75391' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
