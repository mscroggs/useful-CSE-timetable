<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Performance Portable Implementations of Particle-in-Cell Methods</h2><div class='index-talk' id='talk1802' style='display:block'><a href='javascript:toggle_star(1802)' class='star'><span class='star1802'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (G103)</b> Andrew Myers, Scalable, Performance Portable Particle-in-Cell Modeling with WarpX <span id='bitlink-1648'><small><a href='javascript:show_bit(1648)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1648' style='display:none'><small><a href='javascript:hide_bit(1648)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Scalable, Performance Portable Particle-in-Cell Modeling with WarpX</b><br />Andrew Myers<br />Monday, February 27 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-397.html'>Performance Portable Implementations of Particle-in-Cell Methods</a> (9:45 AM&ndash;11:25 AM)<br />G103<br /><br /><small>WarpX is an electromagnetic and electrostatic particle-in-cell code that is being developed as part of the Exascale Computing Project. Originally designed for particle accelerator modeling, WarpX has also been used for a variety of other scientific applications, including laser-plasma interaction and astrophysical plasmas. WarpX includes several advanced algorithmic options, including psuedo-spectral analytical time-domain (PSATD) solvers and the ability to operate in Lorentz-boosted reference frame. Powered by the AMReX framework, WarpX can run on a variety of CPU and GPU-based platforms and has been scaled up to some of the most powerful supercomputers in the world, such as Frontier. In this talk, I will give an overview of the WarpX project and discuss some of the optimizations that allowed WarpX to take advantage of these machines.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75875' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1803' style='display:block'><a href='javascript:toggle_star(1803)' class='star'><span class='star1803'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (G103)</b> Sunita Chandrasekaran, Leveraging Exascale Computing Resources for Particle-In-Cell on GPU (PIConGPU) <span id='bitlink-1649'><small><a href='javascript:show_bit(1649)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1649' style='display:none'><small><a href='javascript:hide_bit(1649)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Leveraging Exascale Computing Resources for Particle-In-Cell on GPU (PIConGPU)</b><br />Sunita Chandrasekaran<br />Monday, February 27 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-397.html'>Performance Portable Implementations of Particle-in-Cell Methods</a> (9:45 AM&ndash;11:25 AM)<br />G103<br /><br /><small>This talk will discuss challenges and success stories while migrating the plasma application, PIConGPU to ORNL Frontier exascale system.   PIConGPU is one of the 8 CAAR (Center for Accelerated Application Readiness) codes chosen by ORNL to stress test the hardware and software of Frontier. The talk will also cover recent results and software tools used to analyze performance while preparing PIConGPU for the Frontier system.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75875' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1804' style='display:block'><a href='javascript:toggle_star(1804)' class='star'><span class='star1804'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (G103)</b> Sam Reeve, The Cabana Particle Library and Cabana-Based PIC Applications <span id='bitlink-1650'><small><a href='javascript:show_bit(1650)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1650' style='display:none'><small><a href='javascript:hide_bit(1650)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>The Cabana Particle Library and Cabana-Based PIC Applications</b><br />Sam Reeve<br />Monday, February 27 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-397.html'>Performance Portable Implementations of Particle-in-Cell Methods</a> (9:45 AM&ndash;11:25 AM)<br />G103<br /><br /><small>We first describe the Cabana library, developed as part of the Exascale Computing Project (ECP) by the Co-design center for Particle Applications (CoPA). Cabana extends Kokkos, a library for performance portability across parallel architectures (of particular interest, Summit and Frontier), for particle applications. Cabana includes particle data structures, particle algorithms, and particle communication (MPI) capabilities, as well as structured grids with corresponding grid algorithms, grid communication, and particle-grid algorithms. Next, we detail the development of Picasso, a library built on Cabana for PIC algorithms, and PicassoMPM, a new MPM application for additive manufacturing built in collaboration with the ECP ExaAM center. In addition, use of Cabana within the XGC plasma physics application is discussed. Finally, the use of Cabana for PIC algorithmic exploration is described, including sparse grid and collision kernel examples.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75875' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1805' style='display:block'><a href='javascript:toggle_star(1805)' class='star'><span class='star1805'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (G103)</b> Maxence Thevenet, Hipace++: Progress on the Quasi-Static Particle-in-Cell Method on GPU <span id='bitlink-1651'><small><a href='javascript:show_bit(1651)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1651' style='display:none'><small><a href='javascript:hide_bit(1651)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Hipace++: Progress on the Quasi-Static Particle-in-Cell Method on GPU</b><br />Maxence Thevenet<br />Monday, February 27 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-397.html'>Performance Portable Implementations of Particle-in-Cell Methods</a> (9:45 AM&ndash;11:25 AM)<br />G103<br /><br /><small>Simulations of plasma-based accelerators with the particle-in-cell (PIC) method are notoriously expensive, as they require a nanometer-scale resolution over a meter-scale propagation distance. Due to the Courant condition, this scale discrepancy results in a very large (many millions) number of time steps for a production simulation, which can be unworkable. The quasi-static PIC method is a reduced model not subject to the same Courant condition, thus accelerating these simulations by orders of magnitude.    
Developed in the last few years, HiPACE++ is the first quasi-static PIC code fully running on GPU. In this algorithm, a 3D problem is decomposed into many 2D problems (one per longitudinal cell). HiPACE++ exploits this property to reduce the volume of communications by orders of magnitude and harness the effectiveness of single-GPU FFTs and multi-grid solves. We will discuss the main algorithm tweaks needed to fully port the quasi-static PIC method on GPU (Nvidia or AMD) based on the AMReX library.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75875' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1806' style='display:block'><a href='javascript:toggle_star(1806)' class='star'><span class='star1806'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (G103)</b> Lucas Schauer, Dynamic Parallelization of Multi-Dimensional Lagrangian Random Walk, Mass-Transfer Particle Tracking Schemes <span id='bitlink-1652'><small><a href='javascript:show_bit(1652)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1652' style='display:none'><small><a href='javascript:hide_bit(1652)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Dynamic Parallelization of Multi-Dimensional Lagrangian Random Walk, Mass-Transfer Particle Tracking Schemes</b><br />Lucas Schauer<br />Monday, February 27 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-397.html'>Performance Portable Implementations of Particle-in-Cell Methods</a> (9:45 AM&ndash;11:25 AM)<br />G103<br /><br /><small>Lagrangian particle tracking schemes allow a wide range of flow and transport processes to be simulated accurately, but a major challenge is numerically implementing the inter-particle interactions in an efficient manner. Our work improves upon a multi-dimensional, parallelized domain decomposition (DDC) strategy for mass-transfer particle tracking (MTPT) methods. We show that this can be efficiently parallelized by employing large numbers of CPU cores to accelerate run times. Current, static DDC methods fix each processor's area of responsibility for the duration of the simulation and provide ample speedup when particle density is approximately constant throughout the domain. However, this type of DDC does not guarantee proper load balance in simulations with, say, a highly heterogeneous velocity field. Our algorithm relies on particles that move within each time step, and their corresponding information must be manually moved between processors when necessary. In this work, we investigate and compare two different procedures for dynamically decomposing the domain to address work imbalance amongst processors---one being a moving geometric DDC and the other being a tree-based neighbor search. In theory, each method presents advantages over the other, which prompted this study for the method we should use moving forward. Both new techniques provide significant speedup over their serial versions, and we observe further advantages than their parallelized predecessors.  	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75875' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
