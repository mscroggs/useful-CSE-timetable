<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Theory and Applications of Derivative-Free Optimization - Part I of II</h2><div class='index-talk' id='talk730' style='display:block'><a href='javascript:toggle_star(730)' class='star'><span class='star730'>&star;</span></a> <b>4:00 PM&ndash;4:15 PM (D407)</b> Margherita Porcelli, Derivative-Free Spectral Residual Methods for Nonlinear Systems of Equations and Applications to Wheel-Rail Contact Models in Railway Systems <span id='bitlink-678'><small><a href='javascript:show_bit(678)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-678' style='display:none'><small><a href='javascript:hide_bit(678)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Derivative-Free Spectral Residual Methods for Nonlinear Systems of Equations and Applications to Wheel-Rail Contact Models in Railway Systems</b><br />Margherita Porcelli<br />Wednesday, March 1 4:00 PM&ndash;4:15 PM<br />This is the 1st talk in <a href='session-161.html'>Theory and Applications of Derivative-Free Optimization - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />D407<br /><br /><small>Spectral residual methods are derivative-free and low-cost per iteration procedures for  solving systems of nonlinear equations [La Cruz, W., Martinez, J. M., Raydan, M. 2006 Spectral residual method without gradient information for solving large-scale nonlinear systems of equations. Math. Comput. 75, 1429-1448]. They are generally coupled with a nonmonotone  linesearch strategy and compare well with Newton-based methods for large nonlinear  systems and sequences of nonlinear systems. The residual vector is used as the search  direction and the steplength is inspired by the Barzilai Borwein method [Barzilai, J., Borwein, J. 1988 Two point step gradient methods. IMA J. Numer. Anal. 8, 141-148]. Analogously  to spectral gradient methods for minimization, choosing the steplength has a crucial  impact on the performance of the procedure. In this talk we address, both theoretically  and experimentally, the steplength selection and provide results on a real application such  as a rolling contact problem modelling a wheel-rail contact in railway systems.    
    
    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75456' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk731' style='display:block'><a href='javascript:toggle_star(731)' class='star'><span class='star731'>&star;</span></a> <b>4:20 PM&ndash;4:35 PM (D407)</b> Clement W. Royer, Stochastic Blackbox Optimization Methods in the Presence of Dynamical Constraints <span id='bitlink-679'><small><a href='javascript:show_bit(679)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-679' style='display:none'><small><a href='javascript:hide_bit(679)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Stochastic Blackbox Optimization Methods in the Presence of Dynamical Constraints</b><br />Clement W. Royer<br />Wednesday, March 1 4:20 PM&ndash;4:35 PM<br />This is the 2nd talk in <a href='session-161.html'>Theory and Applications of Derivative-Free Optimization - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />D407<br /><br /><small>In simulation-based optimization, one is often faced with constraints that stem from physical processes, and are expressed under the form of differential equations. Recent interest in machine learning architectures based on differential equations has generated renewed interest for this class of problems. Indeed, it gave rise to several complex optimization formulations where the dynamics play a prominent role and the objective function can be viewed as the result of an expensive procedure, typically not directly available to the optimizer.    
In this talk, we investigate constrained optimization problems where the objective function is the result of a blackbox simulation, but the dynamics expressed in the constraints are available as a white box. We provide an algorithmic framework that is equipped with theoretical guarantees, even when the objective function cannot be accessed directly, and stochastic estimates are available instead. We also illustrate the performance of our algorithm on task involving neural architectures inspired by differential equations.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75456' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk732' style='display:block'><a href='javascript:toggle_star(732)' class='star'><span class='star732'>&star;</span></a> <b>4:40 PM&ndash;4:55 PM (D407)</b> Mickael Binois, Massively Parallel Bayesian Optimization <span id='bitlink-680'><small><a href='javascript:show_bit(680)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-680' style='display:none'><small><a href='javascript:hide_bit(680)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Massively Parallel Bayesian Optimization</b><br />Mickael Binois<br />Wednesday, March 1 4:40 PM&ndash;4:55 PM<br />This is the 3rd talk in <a href='session-161.html'>Theory and Applications of Derivative-Free Optimization - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />D407<br /><br /><small>One way to reduce the time of conducting optimization studies is to evaluate designs in parallel rather than just one-at-a-time.  For expensive-to-evaluate black-boxes, batch versions of Bayesian optimization have been proposed.   They work by building a surrogate model of the black-box that can be used to select the designs to evaluate efficiently via an infill criterion.  Still, with higher levels of parallelization becoming available, the strategies that work for a few tens of parallel evaluations become limiting, in particular due to the complexity of selecting more evaluations. It is even more crucial when the black-box is noisy, necessitating more evaluations as well as repeating experiments.  Here we propose a scalable strategy that can keep up with massive batching natively, focused on the exploration/exploitation trade-off and a portfolio allocation.   We compare the approach with related methods on noisy test functions. The results show similar or better performance than existing methods, while being orders of magnitude faster. Then we illustrate the potential on the optimization of a large-scale epidemiological simulator.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75456' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk733' style='display:block'><a href='javascript:toggle_star(733)' class='star'><span class='star733'>&star;</span></a> <b>5:00 PM&ndash;5:15 PM (D407)</b> Matt Menickelly, Derivative-Free Variance-Reduced Jacobian Sketching <span id='bitlink-681'><small><a href='javascript:show_bit(681)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-681' style='display:none'><small><a href='javascript:hide_bit(681)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Derivative-Free Variance-Reduced Jacobian Sketching</b><br />Matt Menickelly<br />Wednesday, March 1 5:00 PM&ndash;5:15 PM<br />This is the 4th talk in <a href='session-161.html'>Theory and Applications of Derivative-Free Optimization - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />D407<br /><br /><small>We consider the setting of derivative-free finite-sum minimization. Motivated in particular by problems in nuclear model calibration, we suppose that each summand function is computationally expensive, and that moreover, the summand functions can be evaluated independently. In this setting, we propose and demonstrate a novel method inspired by variance reduction methods in machine learning; such a method permits us to more judiciously select a subset of summand functions to evaluate on each iteration according to a particular probability distribution. We then combine this methodology with sketching methods, enabling judicious sketches of the problem Jacobian. Numerical results demonstrating the efficiency of our Jacobian sketching method will be presented.     
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75456' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk734' style='display:block'><a href='javascript:toggle_star(734)' class='star'><span class='star734'>&star;</span></a> <b>5:20 PM&ndash;5:35 PM (D407)</b> St&#233;phane Jacquet, Handling Hidden Constraints in Blackbox Optimization <span id='bitlink-682'><small><a href='javascript:show_bit(682)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-682' style='display:none'><small><a href='javascript:hide_bit(682)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Handling Hidden Constraints in Blackbox Optimization</b><br />St&#233;phane Jacquet<br />Wednesday, March 1 5:20 PM&ndash;5:35 PM<br />This is the 5th talk in <a href='session-161.html'>Theory and Applications of Derivative-Free Optimization - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />D407<br /><br /><small>In the context of blackbox optimization, the values of the objective function and of the constraints are given through industrial experiments of computer simulations. Sometimes, the values cannot be returned because the experiment failed or the simulation crashed. This causes to lose an evaluation in the budget for a point that brings very little information. In such case, it is said that the point hits a 'hidden constraint'. Therefore, it becomes helpful to avoid those hidden constraints and use the information given by the fact that a point is hitting or not a hidden constraint. This presentation offers different ways to handle those hidden constraints using surrogates and subproblem formulations in the algorithm MADS and shows numerical results.      </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75456' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
