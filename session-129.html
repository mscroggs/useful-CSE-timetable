<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Goal-Oriented and Context-Aware Scientific Machine Learning - Part II of II</h2><div class='index-talk' id='talk586' style='display:block'><a href='javascript:toggle_star(586)' class='star'><span class='star586'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (Forum Centre)</b> Lucia M. Yang, Sampling Strategies for Training Machine Learning Emulators of Gravity Wave Momentum Transport <span id='bitlink-544'><small><a href='javascript:show_bit(544)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-544' style='display:none'><small><a href='javascript:hide_bit(544)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Sampling Strategies for Training Machine Learning Emulators of Gravity Wave Momentum Transport</b><br />Lucia M. Yang<br />Monday, February 27 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-129.html'>Goal-Oriented and Context-Aware Scientific Machine Learning - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />Forum Centre<br /><br /><small>With the goal of developing a data-driven parameterization of gravity waves (GWP) for use in general circulation models, we train various machine learning architectures to emulate an existing GWP scheme. We diagnose the disparity between online and offline performance of the trained emulators by identifying a subspace of the phase space that is prone to large errors and sparse samples, and develop a sampling algorithm to treat biases that stem from underrepresentation. This strategy can be used for regression tasks over long-tailed (and other imbalanced) distributions.   We find that error-prone samples often have larges shears in the wind profile– this is corroborated with physical intuition as large shears indicate many breaking levels, which requires a more complex, nonlocal computation.  To remedy this, we develop a sampling strategy that performs a parameterized histogram equalization.   The sampling algorithm uses a linear mapping from the original histogram to the uniform histogram parameterized by $t \in [0,1]$. Parameters $t$ and &#8220;maximum repeat' assign each bin a new probability. The new probability is applied in two different implementations: 1) by sampling the bins to adjust the training set distribution; 2) by weighting the loss function to achieve the same effect in expectation. We find that this strategy improves the errors at the tail of the distribution except at the extreme end, while maintaining minimal loss of accuracy at the peak of the distribution.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75416' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk587' style='display:block'><a href='javascript:toggle_star(587)' class='star'><span class='star587'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (Forum Centre)</b> Raphael Pestourie, Leveraging Multifidelity in Scientific Machine Learning for Inverse Design <span id='bitlink-545'><small><a href='javascript:show_bit(545)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-545' style='display:none'><small><a href='javascript:hide_bit(545)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Leveraging Multifidelity in Scientific Machine Learning for Inverse Design</b><br />Raphael Pestourie<br />Monday, February 27 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-129.html'>Goal-Oriented and Context-Aware Scientific Machine Learning - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />Forum Centre<br /><br /><small>Wide adoption of inverse design requires global surrogate models that can simulate desired real-world outcomes for all feasible inputs. With the universal approximation theorem and fast evaluations, neural networks (NN) are promising. However, by the curse of dimensionality, with many input variables, NNs become too data greedy for practical usage. For many problems in engineering, there exist data and/or models with multiple fidelity levels. This multifidelity knowledge can be exploited to make data-efficient models that are accurate enough for inverse design. I will present a recently developed physics-enhanced deep surrogate (PEDS) that combines a NN generator, trained end-to-end with a low-fidelity PDE solver, to match costly high-fidelity data. PEDS saves at least two orders of magnitude in data needed to inverse design. The solver is inaccurate but fast and enforces inductive biases that make PEDS designs always satisfy the PDE knowledge. When only multifidelity data is available, the low-fidelity solver can be replaced by a learned operator on the low-fidelity dataset. To finish, we will present inverse design results for thermoelectrics where we compare two optimization approaches: one relying on topology optimization of the continuous relaxation of the problem, another based on derivative-free genetic algorithm.    
    
    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75416' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk588' style='display:block'><a href='javascript:toggle_star(588)' class='star'><span class='star588'>&star;</span></a> <b>2:30 PM&ndash;2:45 PM (Forum Centre)</b> Nicholas H. Nelsen, Learning the Electrical Impedance Tomography Inversion Operator <span id='bitlink-546'><small><a href='javascript:show_bit(546)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-546' style='display:none'><small><a href='javascript:hide_bit(546)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Learning the Electrical Impedance Tomography Inversion Operator</b><br />Nicholas H. Nelsen<br />Monday, February 27 2:30 PM&ndash;2:45 PM<br />This is the 3rd talk in <a href='session-129.html'>Goal-Oriented and Context-Aware Scientific Machine Learning - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />Forum Centre<br /><br /><small>Electrical impedance tomography (EIT) is a class of non-invasive imaging techniques with uses in medical and geophysical applications. EIT may be mathematically abstracted as the severely ill-posed nonlinear inverse problem of recovering the interior conductivity coefficient (the parameter, a function) of an elliptic partial differential equation from its Dirichlet-to-Neumann boundary map (the data, a linear operator). Existing methods for this problem are highly sensitive to noisy boundary measurements and tend to suffer from low accuracy. This work bypasses expensive iterative optimization or Bayesian inference solution approaches by instead directly learning the data-to-parameter solution operator of the inverse problem from training data. Theoretical analysis based on rigorous direct reconstruction algorithms for EIT establishes that the solution operator is well-approximated by a new class of neural operators suitable for operator-valued data. Numerical evidence suggests that incorporating theoretical and EIT-specific problem structure into these neural operator architectures can lead to highly accurate trained models.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75416' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk589' style='display:block'><a href='javascript:toggle_star(589)' class='star'><span class='star589'>&star;</span></a> <b>2:50 PM&ndash;3:05 PM (Forum Centre)</b> Tan Bui-Thanh, TNet: A Tikhonov Neural Network Approach to Deterministic and Bayesian Inverse Problems <span id='bitlink-547'><small><a href='javascript:show_bit(547)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-547' style='display:none'><small><a href='javascript:hide_bit(547)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>TNet: A Tikhonov Neural Network Approach to Deterministic and Bayesian Inverse Problems</b><br />Tan Bui-Thanh<br />Monday, February 27 2:50 PM&ndash;3:05 PM<br />This is the 4th talk in <a href='session-129.html'>Goal-Oriented and Context-Aware Scientific Machine Learning - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />Forum Centre<br /><br /><small>Deep Learning (DL) by design is purely data-driven and in general does not require physics. This is the strength of DL but also one of its key limitations. DL methods in their original forms are not capable of respecting the underlying mathematical models or achieving desired accuracy even in big-data regimes. On the other hand, many data-driven science and engineering problems, such as inverse problems, typically have limited experimental or observational data, and DL would overfit the data in this case. Leveraging information encoded in the underlying mathematical models not only compensates missing information in low data regimes but also provides opportunities to equip DL methods with the underlying physics and hence obtaining higher accuracy. This talk introduces a Tikhonov Network (TNet) that is capable of learning Tikhonov regularized inverse problems. We rigorously show that our TNet approach can learn information encoded in the underlying mathematical models, and thus can produce consistent or equivalent inverse solutions, while naive purely data-based counterparts cannot. Furthermore, we theoretically study the error estimate between TNet and Tikhhonov inverse solutions and under which conditions they are the same. Extension to statistical inverse problems will also be presented.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75416' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk590' style='display:block'><a href='javascript:toggle_star(590)' class='star'><span class='star590'>&star;</span></a> <b>3:10 PM&ndash;3:25 PM (Forum Centre)</b> Fatemeh Nassajian, Variable Shape Parameter Strategy for Radial Basis Function Approximation Using Neural Networks <span id='bitlink-548'><small><a href='javascript:show_bit(548)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-548' style='display:none'><small><a href='javascript:hide_bit(548)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Variable Shape Parameter Strategy for Radial Basis Function Approximation Using Neural Networks</b><br />Fatemeh Nassajian<br />Monday, February 27 3:10 PM&ndash;3:25 PM<br />This is the 5th talk in <a href='session-129.html'>Goal-Oriented and Context-Aware Scientific Machine Learning - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />Forum Centre<br /><br /><small>The choice of the shape parameter highly effects the behaviour of radial basis function (RBF) approximations, as it needs to be selected to balance between ill-condition of the interpolation matrix and high accuracy. In this work, we demonstrate how to use neural networks  to determine the shape parameters in RBFs. In particular, we construct a multilayer perceptron trained using an unsupervised learning strategy, and use it to predict shape parameters for inverse multiquadric and Gaussian kernels. We test the neural network approach in RBF interpolation tasks and in a RBF-finite difference method in one and two-space dimensions, demonstrating promising results.     </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75416' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
