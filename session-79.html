<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Advances in Deep Neural Operators - Part I of II</h2><div class='index-talk' id='talk354' style='display:block'><a href='javascript:toggle_star(354)' class='star'><span class='star354'>&star;</span></a> <b>4:00 PM&ndash;4:15 PM (Forum Centre)</b> Margaret K. Trautner, Learning Homogenized Constitutive Models in Viscoelasticity and Viscoplasticity <span id='bitlink-324'><small><a href='javascript:show_bit(324)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-324' style='display:none'><small><a href='javascript:hide_bit(324)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Learning Homogenized Constitutive Models in Viscoelasticity and Viscoplasticity</b><br />Margaret K. Trautner<br />Wednesday, March 1 4:00 PM&ndash;4:15 PM<br />This is the 1st talk in <a href='session-79.html'>Advances in Deep Neural Operators - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />Forum Centre<br /><br /><small>The macroscopic behavior of materials is governed in part by small-scale rapidly-varying material properties. Fully resolving these features within the balance laws thus involves expensive fine-scale computations which need to be conducted on macroscopic scales. The theory of homogenization provides an approach to derive effective macroscopic equations which eliminates the small scales by exploiting scale separation. An accurate homogenized model avoids the computationally-expensive task of numerically solving the underlying balance laws at a fine scale.    
In simple settings the homogenization produces an explicit formula for a macroscopic constitutive model, but in more complex settings it may only define the constitutive model implicitly. In these complex settings machine learning can be used to learn the constitutive model from localized fine-scale simulations. In the case of one-dimensional viscoelasticity, the linearity of the model allows for a complete analysis. For this case, we derive a homogenized constitutive model and develop a theory to prove that the model may be approximated by a recurrent   neural network (RNN) model that captures the memory; this may be thought of as discovering appropriate internal variables. Simulations are presented which validate the theory, and additional numerical experiments demonstrate extension of the methodology to higher dimensions and to nonlinear viscoplasticity.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75334' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk355' style='display:block'><a href='javascript:toggle_star(355)' class='star'><span class='star355'>&star;</span></a> <b>4:20 PM&ndash;4:35 PM (Forum Centre)</b> Saad Qadeer, Machine-Learning-Based Spectral Methods for Partial Differential Equations <span id='bitlink-325'><small><a href='javascript:show_bit(325)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-325' style='display:none'><small><a href='javascript:hide_bit(325)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Machine-Learning-Based Spectral Methods for Partial Differential Equations</b><br />Saad Qadeer<br />Wednesday, March 1 4:20 PM&ndash;4:35 PM<br />This is the 2nd talk in <a href='session-79.html'>Advances in Deep Neural Operators - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />Forum Centre<br /><br /><small>Spectral methods are an important part of scientific computing's arsenal for solving partial differential equations (PDEs). However, their applicability and effectiveness depend crucially on the choice of basis functions used to expand the solution of a PDE. The last decade has seen the emergence of deep learning as a strong contender in providing efficient representations of complex functions. In the current work, we present an approach for combining deep neural networks with spectral methods to solve PDEs. In particular, we use a deep learning technique known as the Deep Operator Network (DeepONet) to identify candidate functions on which to expand the solution of PDEs. We have devised an approach that uses the candidate functions provided by the DeepONet as a starting point to construct a set of functions that have the following properties: (1) they constitute a basis, (2) they are orthonormal, and (3) they are hierarchical, i.e., akin to Fourier series or orthogonal polynomials. We have exploited the favorable properties of our custom-made basis functions to both study their approximation capability and use them to expand the solution of linear and nonlinear time-dependent PDEs. The proposed approach advances the state of the art and versatility of spectral methods and, more generally, promotes the synergy between traditional scientific computing and machine learning.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75334' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk356' style='display:block'><a href='javascript:toggle_star(356)' class='star'><span class='star356'>&star;</span></a> <b>4:40 PM&ndash;4:55 PM (Forum Centre)</b> Zhi-Qin Xu, Data in Deep Operator Learning <span id='bitlink-326'><small><a href='javascript:show_bit(326)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-326' style='display:none'><small><a href='javascript:hide_bit(326)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Data in Deep Operator Learning</b><br />Zhi-Qin Xu<br />Wednesday, March 1 4:40 PM&ndash;4:55 PM<br />This is the 3rd talk in <a href='session-79.html'>Advances in Deep Neural Operators - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />Forum Centre<br /><br /><small>In this talk, I will show the importance of data in deep operator learning. First, I will show the accuracy of a MOD-Net (Model-operator-data) with physics-informed loss to learn the operator of a PDE could be greatly improved by using few data points computed by traditional numerical schemes on coarse-grained grids. Second, I will use data-driven DeepONet to solve an inverse problem of fractional problem. Finally, I will show a good sampling scheme is extremely important for learning the operator of a stiff ODE system, such as combustion chemical system, where our neural network model is accurate and efficient in many different working conditions for large reaction systems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75334' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk357' style='display:block'><a href='javascript:toggle_star(357)' class='star'><span class='star357'>&star;</span></a> <b>5:00 PM&ndash;5:15 PM (Forum Centre)</b> Jinchao Xu, Convergence Analysis of Finite Neuron Method and Its Training Algorithms <span id='bitlink-327'><small><a href='javascript:show_bit(327)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-327' style='display:none'><small><a href='javascript:hide_bit(327)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Convergence Analysis of Finite Neuron Method and Its Training Algorithms</b><br />Jinchao Xu<br />Wednesday, March 1 5:00 PM&ndash;5:15 PM<br />This is the 4th talk in <a href='session-79.html'>Advances in Deep Neural Operators - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />Forum Centre<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75334' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk358' style='display:block'><a href='javascript:toggle_star(358)' class='star'><span class='star358'>&star;</span></a> <b>5:20 PM&ndash;5:35 PM (Forum Centre)</b> Remy Hosseinkhan Boucher, Exploration Strategies for Control of Chaotic Dynamical Systems Using Reinforcement Learning <span id='bitlink-328'><small><a href='javascript:show_bit(328)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-328' style='display:none'><small><a href='javascript:hide_bit(328)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Exploration Strategies for Control of Chaotic Dynamical Systems Using Reinforcement Learning</b><br />Remy Hosseinkhan Boucher<br />Wednesday, March 1 5:20 PM&ndash;5:35 PM<br />This is the 5th talk in <a href='session-79.html'>Advances in Deep Neural Operators - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />Forum Centre<br /><br /><small>Predicting and controlling the future state of a complex physical system is a cornerstone in a wide range of situations. This aim however often remains difficult to achieve. In particular, for a high-dimensional multiscale nonlinear system, a model is not necessarily available or usable due to real-time control constraints which drastically limit the affordable computational complexity. Data-driven control circumvents the need for an a priori model and constitutes an attractive approach.    
In this talk, we will focus on a Reinforcement Learning strategy fully leveraging its theoretical foundation in optimal control theory while accounting for challenges of practical situations such as weak observability. We identify the multiple factors and their influence on the policy identification without relying on physics constraints or prior knowledge. In particular, we develop consistent exploration strategies allowing to carefully sample new observations in order to improve knowledge of the system. As a result, better control policies could be obtained with fewer interactions with the system, resulting in a faster learning. To achieve this, we rely on tools from information theory and optimal design of experiments to quantify the knowledge gathers during the process.  Our framework will be illustrated on simplified models of turbulent flows such as the Kuramoto-Sivashinsky equations.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75334' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
