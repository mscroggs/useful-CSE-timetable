<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Randomized Solvers in Large-Scale Scientific Computing - Part I of II</h2><div class='index-talk' id='talk1641' style='display:block'><a href='javascript:toggle_star(1641)' class='star'><span class='star1641'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (G111)</b> Julia Schleu&#223;, Generating Problem-Adapted Basis Functions Parallel in Time via Random Sampling <span id='bitlink-1506'><small><a href='javascript:show_bit(1506)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1506' style='display:none'><small><a href='javascript:hide_bit(1506)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Generating Problem-Adapted Basis Functions Parallel in Time via Random Sampling</b><br />Julia Schleu&#223;<br />Tuesday, February 28 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-363.html'>Randomized Solvers in Large-Scale Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G111<br /><br /><small>To tackle time-dependent partial differential equations (PDEs) with coefficients that are rough in both space and time, we construct reduced ansatz functions defined in space that can be combined with time stepping schemes, e.g., within model order reduction methods. As a key new contribution, we propose to construct these ansatz functions in an embarrassingly parallel and local manner in time by selecting important points in time and only performing local computations on the corresponding local time intervals.    
In detail, we perform several simulations of the PDE for only few time steps in parallel, starting at different, randomly drawn start time points, prescribing random initial conditions. Applying a singular value decomposition to a subset of the so obtained snapshots yields the reduced ansatz functions. To select suitable start points in time, we suggest using data-driven sampling strategies from randomized numerical linear algebra such as leverage score sampling. By solving the PDE locally in time with random initial conditions, we construct local ansatz spaces in time that converge provably at a quasi-optimal rate and allow for local error control.     
Numerical experiments demonstrate that the proposed approach can outperform existing methods like the proper orthogonal decomposition even in a sequential setting and is well capable of approximating advection-dominated problems.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75797' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1642' style='display:block'><a href='javascript:toggle_star(1642)' class='star'><span class='star1642'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (G111)</b> Valentin Braez, Randomized Maximum Likelihood via High-Dimensional Bayesian Optimization <span id='bitlink-1507'><small><a href='javascript:show_bit(1507)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1507' style='display:none'><small><a href='javascript:hide_bit(1507)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Randomized Maximum Likelihood via High-Dimensional Bayesian Optimization</b><br />Valentin Braez<br />Tuesday, February 28 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-363.html'>Randomized Solvers in Large-Scale Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G111<br /><br /><small>Posterior sampling for high-dimensional Bayesian inverse problems where the log-likelihood has a low-dimensional structure, known as an active subspace, is a problem commonly faced in real-world applications. Existing approaches assume that the computational budget is sufficient to estimate the active subspace, either via gradient-based methods or using a large number of simulator evaluations. Here we tackle the more challenging (and practically relevant) case where we do not have sufficient computational budget to satisfactorily estimate the active subspace. We develop a high-dimensional Bayesian optimization approach to solve the Randomized Maximum Likelihood (RML) problem. RML is an approximate posterior sampling methodology based on multi-objective optimization, first developed for petroleum engineering applications. By sharing data between the different objective functions, we are able to implement RML at a greatly reduced computational cost compared to existing methods, allowing us to efficiently sample from the posterior distribution of the inverse problem. We demonstrate the benefits of this approach in comparison to alternative optimization methods on a variety of synthetic and real-world problems, including medical and fluid dynamics applications. Furthermore, we show that the samples produced by our method cover well the high-posterior density regions in all of the experiments.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75797' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1643' style='display:block'><a href='javascript:toggle_star(1643)' class='star'><span class='star1643'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (G111)</b> Martin Eigel, Convergence of an Empirical Galerkin Method for Parametric PDEs <span id='bitlink-1508'><small><a href='javascript:show_bit(1508)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1508' style='display:none'><small><a href='javascript:hide_bit(1508)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Convergence of an Empirical Galerkin Method for Parametric PDEs</b><br />Martin Eigel<br />Tuesday, February 28 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-363.html'>Randomized Solvers in Large-Scale Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G111<br /><br /><small>Weighted least squares methods have been examined thouroughly to obtain quasi-optimal convergence results for a chosen (polynomial) basis of a linear space. A focus in the analysis lies on the construction of optimal sampling measures and the derivation of a sufficient sample complexity for stable reconstructions. When considering holomorphic functions such as solutions of common parametric PDEs, the anisotropic sparsity they exhibit can be exploited to achieve improved results adapted to the considered problem. In particular, the sparsity of the data transfers to the solution sparsity in terms of polynomial chaos coefficients.    
When using nonlinear model classes, it turns out that the known results cannot be used directly. To obtain comparable a priori rates, we introduce a new weighted version of Stechkin's lemma. This enables to obtain optimal complexity results for a model class of low-rank tensor train  networks. We also show that the solution sparsity results in sparse component tensors. Numerical examples illustrate the theoretical findings and demonstrate a remarkable performance of the derived RALS algorithm in comparison to existing state-of-the-art algorithms.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75797' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1644' style='display:block'><a href='javascript:toggle_star(1644)' class='star'><span class='star1644'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (G111)</b> Tommaso Taddei, Randomized Local Model Order Reduction for Nonlinear Partial Differential Equations <span id='bitlink-1509'><small><a href='javascript:show_bit(1509)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1509' style='display:none'><small><a href='javascript:hide_bit(1509)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Randomized Local Model Order Reduction for Nonlinear Partial Differential Equations</b><br />Tommaso Taddei<br />Tuesday, February 28 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-363.html'>Randomized Solvers in Large-Scale Scientific Computing - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />G111<br /><br /><small>Applications that require multiple simulation requests or a real-time simulation response are ubiquitous in science and engineering. Model order reduction methods in which the problem is (approximately) solved in a carefully chosen subspace of the high-dimensional discretization space have been developed to tackle such problems. However, for many large-scale problems, and especially problems that exhibit multiscale features, full order solves are not affordable in a reasonable time frame. Localized model order reduction methods decompose the global computational domain into subdomains, build local reduced models from solutions of the partial differential equation (PDE) on the subdomains, and use some coupling to compute a global reduced approximation. For the local model order reduction methods that we will present in this talk we can control the global approximation error even if the global computational domain is unknown when the local reduced models are constructed, facilitating a LEGO brick like assembly of the global computational domain, imperative for many applications such as digital twins. While there has been a significant progress in recent years for the construction of local reduced models for linear PDEs, very few results have been obtained so far for nonlinear PDEs. In this talk, we will show how randomized methods and their probabilistic numerical analysis can be exploited for the construction and numerical analysis of local reduced models for nonlinear PDEs.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75797' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
