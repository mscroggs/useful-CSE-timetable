<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Deep Learning-Based Latent-Space Models for Scientific Computing</h2><div class='index-talk' id='talk632' style='display:block'><a href='javascript:toggle_star(632)' class='star'><span class='star632'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D406)</b> Toby van Gastelen, Structure Preserving Machine Learning: Energy-Conserving Neural Network for Turbulence Closure Modelling <span id='bitlink-588'><small><a href='javascript:show_bit(588)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-588' style='display:none'><small><a href='javascript:hide_bit(588)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Structure Preserving Machine Learning: Energy-Conserving Neural Network for Turbulence Closure Modelling</b><br />Toby van Gastelen<br />Wednesday, March 1 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-139.html'>Deep Learning-Based Latent-Space Models for Scientific Computing</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>In turbulence modelling, and more particularly in the LES framework, we are concerned with finding a suitable closure model to represent the effect of the unresolved subgrid-scales on the larger/resolved scales. In recent years, the scientific computing community has started to gravitate towards machine learning techniques to attempt to solve this issue. However, stability and abidance by physical structure laws of the resulting closure models is still an open problem.    
We apply a spatial averaging filter to a high-resolution reference simulation to reduce the degrees of freedom of the system and derive a new kinetic energy conservation condition. We then suggest a data-driven compression to represent the subgrid-scale content as latent variables, living on the coarse grid, in order to comply with our new conservation condition. Finally, a skew-symmetric energy-conserving convolutional neural network architecture is introduced that can be enhanced with dissipative terms to account for viscous flows. Combined with a structure-preserving discretization this framework is used to evolve both the filtered solution and the latent subgrid-scale representation in time in a structure-preserving fashion. This yields stability while still allowing for backscatter.    
We apply the methodology to both the viscous Burgers' equation and Korteweg-De Vries equation in 1-D and show increased accuracy and stability as compared to a standard convolutional neural network.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75430' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk633' style='display:block'><a href='javascript:toggle_star(633)' class='star'><span class='star633'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D406)</b> Lasse Hjuler Christiansen, Efficient Synthesis of Antenna Arrays Using Latent-Space Modelling <span id='bitlink-589'><small><a href='javascript:show_bit(589)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-589' style='display:none'><small><a href='javascript:hide_bit(589)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Efficient Synthesis of Antenna Arrays Using Latent-Space Modelling</b><br />Lasse Hjuler Christiansen<br />Wednesday, March 1 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-139.html'>Deep Learning-Based Latent-Space Models for Scientific Computing</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>A popular antenna type is the phased-array antenna, an antenna type characterized by a large quantity of individually controllable antennas working in unison. Typically, the applications impose strict requirements on the radiation pattern from the array antenna, which require detailed optimisation of the antenna parameters. A major drawback of phased-array antenna design is the array synthesis problem, i.e., determining the phases of the array such that the requirements are satisfied. The synthesis problem poses several major challenges that are tied to, e.g., conflicting objectives of the underlying optimisation, many local optima, and the need to carry out computationally costly online simulations. To address these challenges, this work demonstrates how latent-space modelling by custom-tailored autoencoders can augment and improve state-of-the-art, model-based synthesis of phased arrays. We explore an encoder-decoder structure that combines highly accurate simulation-based synthetic data with application-tailored models that are embedded into the network architectures, thereby guiding the latent space to reflect the real-world antenna problem, allowing low-rank subspaces to be extracted from the data. Numerical results demonstrate that the autoencoder has a significant potential to accelerate array synthesis to near real-time by generating good starting guesses for specialized optimizers.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75430' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk2060' style='display:block'><a href='javascript:toggle_star(2060)' class='star'><span class='star2060'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D406)</b> Wouter Edeling, Using Neural Networks to Find Low-Dimensional Active Subspaces <span id='bitlink-1885'><small><a href='javascript:show_bit(1885)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1885' style='display:none'><small><a href='javascript:hide_bit(1885)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Using Neural Networks to Find Low-Dimensional Active Subspaces</b><br />Wouter Edeling<br />Wednesday, March 1 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-139.html'>Deep Learning-Based Latent-Space Models for Scientific Computing</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>The deep active subspace method is a neural-network based tool for the propagation of uncertainty through computational models with high-dimensional input spaces. Unlike the original active subspace method, it does not require access to the gradient of the model. It relies on an orthogonal projection matrix constructed with Gram-Schmidt orthogonalization, which is used to linearly project the (high-dimensional) input space to a low-dimensional active subspace. This matrix is incorporated into a neural network as the weight matrix of the first hidden layer, and optimized using back propagation to identify the active subspace of the input. We propose several theoretical extensions, starting with a new analytic relation for the derivatives of Gram-Schmidt vectors, which are required for back propagation. We also strengthen the connection between deep active subspaces and the original active subspace method, and study the use of vector-valued model outputs, which is difficult in the case of the original active subspace method. Additionally, we extract more traditional global sensitivity indices from the neural network to identify important inputs, and compare the resulting reduction of the input space to the dimension of the identified active subspace. Finally, we will assess the performance of the deep active subspace method on (epidemiological) problems with high dimensional input spaces, including an HIV model with 27 inputs and a COVID19 model with a 51-dimensional input space.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75430' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk635' style='display:block'><a href='javascript:toggle_star(635)' class='star'><span class='star635'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D406)</b> Vinicius L. Santos Silva, Generative Network-Based Reduced Order Model for Data Assimilation and Uncertainty Quantification in the Latent Space <span id='bitlink-590'><small><a href='javascript:show_bit(590)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-590' style='display:none'><small><a href='javascript:hide_bit(590)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Generative Network-Based Reduced Order Model for Data Assimilation and Uncertainty Quantification in the Latent Space</b><br />Vinicius L. Santos Silva<br />Wednesday, March 1 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-139.html'>Deep Learning-Based Latent-Space Models for Scientific Computing</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>The production of numerous high-fidelity simulations has been a key aspect of research for many problems in computational physics. The computational resources and time required to generate these simulations can be so large and impractical. With several successes of generative models, we propose a new method in which generative neural networks within a reduced-order model (ROM) framework are used for prediction, data assimilation and uncertainty quantification. A method has been developed which enables a generative network to perform time series prediction and data assimilation by training it with unconditional simulations of a discretized partial differential equation (PDE) model. After training, the generative model can be used to predict the spatio-temporal evolution of the physical states and observed data can be assimilated. We also describe the process required in order to quantify uncertainty, during which no additional simulations of the high-fidelity numerical PDE model are required. These methods work in the latent space (reduced space), which improves efficiency, also they take advantage of the adjoint-like capabilities of neural networks and the ability to simulate forwards and backwards in time. The results show that the proposed Generative Network-Based ROM can efficiently quantify uncertainty and accurately match the observed data, using only few unconditional simulations of the high-fidelity numerical PDE model.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75430' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk636' style='display:block'><a href='javascript:toggle_star(636)' class='star'><span class='star636'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (D406)</b> Maude Girardin, Error Assessment for a Finite Elements - Neural Networks Approach Applied to Parametric PDEs <span id='bitlink-591'><small><a href='javascript:show_bit(591)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-591' style='display:none'><small><a href='javascript:hide_bit(591)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Error Assessment for a Finite Elements - Neural Networks Approach Applied to Parametric PDEs</b><br />Maude Girardin<br />Wednesday, March 1 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-139.html'>Deep Learning-Based Latent-Space Models for Scientific Computing</a> (9:45 AM&ndash;11:25 AM)<br />D406<br /><br /><small>We consider a parametric PDE   \begin{align*}      \mathcal{F}(u(x;\mu);\mu)= 0 \ \ &\ x \in \Omega,\ \ \mu \in \mathcal{P},  \end{align*}  where $\Omega$ denotes the physical domain and $\mathcal{P}$ the parameters domain. During an offline phase, time consuming approximations $u_h(\cdot;\mu_i)$ of $u(\cdot;\mu_i)$ are computed using a finite element mesh with size $h$, for given $\mu_i$, $i=1, \ldots, N$. Then, the parameter-to-solution map $\mu \mapsto u_h(\cdot;\mu)$ is approximated using a deep neural network, producing an approximation $u_{h,\mathcal{N}}(\cdot;\mu)$ of $u_h(\cdot;\mu)$. The evaluation of the neural network, performed in an online phase, is instantaneous, or at least much faster than a single numerical simulation.  We next decompose the $L^2$ error in $\Omega \times \mathcal{P}$ into  \begin{align*}      \vert \vert u-u_{h,\mathcal{N}}\vert \vert _{L^2(\Omega \times \mathcal{P})}\leq \vert \vert u-u_{h}\vert \vert _{L^2(\Omega \times \mathcal{P})}+\vert \vert u_h-u_{h,\mathcal{N}}\vert \vert _{L^2(\Omega \times \mathcal{P})}.  \end{align*}  Both error terms can be estimated using Monte-Carlo type estimates over the parameters space and an a posteriori error estimator in the physical space for the first term. In the presentation, we discuss in more detail the estimation of these two terms and show numerical experiments both for a model problem and a more complex one.    
	    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75430' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
