<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part II of II</h2><div class='index-talk' id='talk1475' style='display:block'><a href='javascript:toggle_star(1475)' class='star'><span class='star1475'>&star;</span></a> <b>2:15 PM&ndash;2:30 PM (D403)</b> Stefano Zampini, Device Accelerated Second-Order Solvers for Machine Learning Training with Petsc <span id='bitlink-1357'><small><a href='javascript:show_bit(1357)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1357' style='display:none'><small><a href='javascript:hide_bit(1357)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Device Accelerated Second-Order Solvers for Machine Learning Training with Petsc</b><br />Stefano Zampini<br />Tuesday, February 28 2:15 PM&ndash;2:30 PM<br />This is the 1st talk in <a href='session-325.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part II of II</a> (2:15 PM&ndash;3:55 PM)<br />D403<br /><br /><small>Deep learning and neural networks have had a tremendous impact in areas such as computer vision, natural language processing, and speech recognition. Such a widely recognized success has recently driven developments and applications of similar techniques in the context of computational science and engineering, for example as functional representation of the solution of partial differential equations, or when learning parameters to observable maps.    
CSE differs from the traditional contexts in which these machine learning techniques operate, due to a possibly richer training dataset available. Building on this observation, and in the quest of constructing higher-fidelity representations of the physical models to be represented, in this talk we will present preliminary results using second-order methods as implemented in PETSc to train some of the models that have been recently proposed in the literature, with applications coming from function interpolation, partial differential equations, Bayesian frameworks, and molecular dynamics.    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75728' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1476' style='display:block'><a href='javascript:toggle_star(1476)' class='star'><span class='star1476'>&star;</span></a> <b>2:35 PM&ndash;2:50 PM (D403)</b> Li Luo, A Nonlinear Preconditioning Strategy Based on Residual Learning for PDEs <span id='bitlink-1358'><small><a href='javascript:show_bit(1358)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1358' style='display:none'><small><a href='javascript:hide_bit(1358)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A Nonlinear Preconditioning Strategy Based on Residual Learning for PDEs</b><br />Li Luo<br />Tuesday, February 28 2:35 PM&ndash;2:50 PM<br />This is the 2nd talk in <a href='session-325.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part II of II</a> (2:15 PM&ndash;3:55 PM)<br />D403<br /><br /><small>We talk about a nonlinearly preconditioned inexact Newton method for solving highly nonlinear system of algebraic equations from the discretization of PDEs. From a large number of numerical experiments, we observe that when the inexact Newton stagnates or fails to converge, the space of residuals often contains a subspace that is difficult to reach by the usual Newton direction. We introduce a learning technique to identify this subspace and then speedup the convergence.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75728' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1477' style='display:block'><a href='javascript:toggle_star(1477)' class='star'><span class='star1477'>&star;</span></a> <b>2:55 PM&ndash;3:10 PM (D403)</b> Juncai He, MgNet: Algorithms and Applications in Numerical PDEs <span id='bitlink-1359'><small><a href='javascript:show_bit(1359)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1359' style='display:none'><small><a href='javascript:hide_bit(1359)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>MgNet: Algorithms and Applications in Numerical PDEs</b><br />Juncai He<br />Tuesday, February 28 2:55 PM&ndash;3:10 PM<br />This is the 3rd talk in <a href='session-325.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part II of II</a> (2:15 PM&ndash;3:55 PM)<br />D403<br /><br /><small>In this talk, we will first demonstrate how a new type of convolutional neural network, known as MgNet, can be derived by making minor modifications to a classic geometric multigrid method. Then, we present an approximation result on how to understand MgNet. Finally, we will show some recent applications of MgNet in forecasting problems and numerical partial differential equations.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75728' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1478' style='display:block'><a href='javascript:toggle_star(1478)' class='star'><span class='star1478'>&star;</span></a> <b>3:15 PM&ndash;3:30 PM (D403)</b> Axel Klawonn, A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel Training Applied to Image Recognition Problems <span id='bitlink-1360'><small><a href='javascript:show_bit(1360)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1360' style='display:none'><small><a href='javascript:hide_bit(1360)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A Domain Decomposition-Based CNN-DNN Architecture for Model Parallel Training Applied to Image Recognition Problems</b><br />Axel Klawonn<br />Tuesday, February 28 3:15 PM&ndash;3:30 PM<br />This is the 4th talk in <a href='session-325.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part II of II</a> (2:15 PM&ndash;3:55 PM)<br />D403<br /><br /><small>Deep neural networks (DNNs) and, in particular, convolutional neural networks (CNNs) have brought significant advances in a wide range of modern computer application problems.  However, the increasing availability of large amounts of datasets as well as the increasing available computational power of modern computers lead to a steady growth in the complexity and size of DNN and CNN models, respectively, and thus, to  longer training times. Hence, various methods and attempts have been developed to accelerate and parallelize the training of complex network architectures. In this talk, a novel domain decomposition-based CNN-DNN architecture  is presented which naturally supports a model parallel training strategy. Experimental results for different 2D image classification problems are shown as well as for a face recognition problem, and for a classification problem for 3D computer tomography (CT) scans.  The results show that the proposed approach can significantly accelerate the required training time compared to the global model and, additionally, can also help to improve the accuracy of the underlying classification problem.     </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75728' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
