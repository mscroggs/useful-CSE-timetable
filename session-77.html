<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Butterfly Factorizations: Algorithms, Applications, and Theory - Part I of II</h2><div class='index-talk' id='talk346' style='display:block'><a href='javascript:toggle_star(346)' class='star'><span class='star346'>&star;</span></a> <b>4:45 PM&ndash;6:25 PM (D304)</b> Michael O'Neil, Butterfly Algorithms: An Overview <span id='bitlink-316'><small><a href='javascript:show_bit(316)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-316' style='display:none'><small><a href='javascript:hide_bit(316)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Butterfly Algorithms: An Overview</b><br />Michael O'Neil<br />Thursday, March 2 4:45 PM&ndash;6:25 PM<br />This is the 1st talk in <a href='session-77.html'>Butterfly Factorizations: Algorithms, Applications, and Theory - Part I of II</a> (4:45 PM&ndash;6:25 PM)<br />D304<br /><br /><small>The so-called “butterfly matrix” or “butterfly operator” is widely known in the signals processing community from its appearance in algorithms such as the Fast Fourier Transform. The basic idea is to split a signal into two pieces, apply separate linear transformations to each piece, and then linearly re-combine the output. The matrix corresponding to the Discrete Fourier Transform can be analytically factored into a product of log(N) block-diagonal butterfly matrices. Generalizations of such matrix factorizations yield what are now known as “butterfly factorizations”, and have found applications in PDE, integral equations, signals processing, special function transforms, etc. They often allow for the compression of an N x N matrix into a product of log(N) matrices, each with O(N) sparsity. This talk will give a brief overview of such factorizations and applications, modern developments, and open problems as an introduction to the corresponding mini-symposium.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75326' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk347' style='display:block'><a href='javascript:toggle_star(347)' class='star'><span class='star347'>&star;</span></a> <b>4:45 PM&ndash;6:25 PM (D304)</b> L&#233;on Zheng, Efficient Identification of Butterfly Sparse Matrix Factorizations <span id='bitlink-317'><small><a href='javascript:show_bit(317)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-317' style='display:none'><small><a href='javascript:hide_bit(317)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Efficient Identification of Butterfly Sparse Matrix Factorizations</b><br />L&#233;on Zheng<br />Thursday, March 2 4:45 PM&ndash;6:25 PM<br />This is the 2nd talk in <a href='session-77.html'>Butterfly Factorizations: Algorithms, Applications, and Theory - Part I of II</a> (4:45 PM&ndash;6:25 PM)<br />D304<br /><br /><small>Large neural networks perform well in many domains, but they suffer from long training and inference time. Introducing sparsity in these deep models by enforcing most entries in their weight matrices to zero can reduce their complexity. However, learning such sparse networks is difficult. Motivated by recent works showing the expressivity of the butterfly structure for neural network design, a promising approach is to promote sparsity during training by approximating any dense weight matrices with a product of sparse butterfly factors. Our main contribution is to show that any butterfly factorization $\mathbf{Z} = \mathbf{X}^{(1)} \ldots \mathbf{X}^{(J)}$ where the factors $\mathbf{X}^{(\ell)}$ of size $N \times N$ follow the butterfly constraint is essentially unique, and we provide an efficient hierarchical algorithm that can recover these factors up to unavoidable scaling ambiguities from the product $\mathbf{Z}$. Our method consists in recursively factorizing the considered matrix into two factors, by relying on a non-trivial application of the singular value decomposition to compute best rank-one approximations of specific submatrices, instead of iterative gradient descent steps. The complexity of our algorithm is only of $O(N^2)$, and it enables fast $O(N \log N)$ matrix-vector multiplication. Hence our work provides a first step toward the construction of an efficient proximal operator associated to the butterfly structure for sparse neural network training.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75326' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk348' style='display:block'><a href='javascript:toggle_star(348)' class='star'><span class='star348'>&star;</span></a> <b>4:45 PM&ndash;6:25 PM (D304)</b> Matthew Li, Wide-Band Butterfly Networks for Wave-Based Inverse Problems in the Super-Resolution Regime <span id='bitlink-318'><small><a href='javascript:show_bit(318)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-318' style='display:none'><small><a href='javascript:hide_bit(318)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Wide-Band Butterfly Networks for Wave-Based Inverse Problems in the Super-Resolution Regime</b><br />Matthew Li<br />Thursday, March 2 4:45 PM&ndash;6:25 PM<br />This is the 3rd talk in <a href='session-77.html'>Butterfly Factorizations: Algorithms, Applications, and Theory - Part I of II</a> (4:45 PM&ndash;6:25 PM)<br />D304<br /><br /><small>We propose an end-to-end deep learning framework that comprehensively solves the inverse wave scattering problem across all length scales. Our framework consists of the newly introduced wide-band butterfly network coupled with a simple training procedure which dynamically injects noise during training. While our trained network provides competitive results in classical imaging regimes, most notably it also succeeds in the super-resolution regime where other comparable methods fail. This encompasses both (i) reconstruction of scatterers with sub-wavelength geometric features, and (ii) accurate imaging when two or more scatterers are separated by less than the classical diffraction limit. We demonstrate these properties are retained even in the presence of strong noise and extend to scatterers not previously seen in the training set. In addition, our network is straightforward to train requiring no restarts and has an online runtime that is an order of magnitude faster than optimization-based algorithms. We perform experiments with a variety of wave scattering mediums and we demonstrate that our proposed framework outperforms both classical inversion and competing network architectures that specialize in oscillatory wave scattering data.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75326' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk349' style='display:block'><a href='javascript:toggle_star(349)' class='star'><span class='star349'>&star;</span></a> <b>4:45 PM&ndash;6:25 PM (D304)</b> Fruzsina Agocs, A Fast and Arbitrarily High-Order Solver for Highly Oscillatory ODEs <span id='bitlink-319'><small><a href='javascript:show_bit(319)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-319' style='display:none'><small><a href='javascript:hide_bit(319)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>A Fast and Arbitrarily High-Order Solver for Highly Oscillatory ODEs</b><br />Fruzsina Agocs<br />Thursday, March 2 4:45 PM&ndash;6:25 PM<br />This is the 4th talk in <a href='session-77.html'>Butterfly Factorizations: Algorithms, Applications, and Theory - Part I of II</a> (4:45 PM&ndash;6:25 PM)<br />D304<br /><br /><small>Oscillatory systems are ubiquitous in physics: they arise in celestial and quantum mechanics, electrical circuits, molecular dynamics, and beyond. Yet even in the simplest case, when the frequency of oscillations changes slowly but is large, the vast majority of numerical methods struggle to solve such equations. Methods based on approximating the solution with polynomials are forced to take O(k) timesteps, where k is the characteristic frequency of oscillations. This scaling can generate unacceptable computational costs when the ODE in question needs to be solved billions of times, e.g. as the forward modelling step of Bayesian parameter estimation. In this talk I will introduce an efficient method for solving 2nd order, linear ODEs with highly oscillatory solutions. The solver employs two methods: in regions where the solution varies slowly, it uses a spectral method based on Chebyshev nodes and with an adaptive stepsize, but in the highly oscillatory phase it automatically switches over to an asymptotic method. The asymptotic method constructs a nonoscillatory phase function solution of the Riccati equation associated with the ODE. In the talk I will present how the method fits in the landscape of oscillatory solvers, the theoretical underpinnings of the asymptotic solver, a summary of the switching and stepsize-update algorithms, results from numerical experiments, and a brief error analysis.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75326' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
