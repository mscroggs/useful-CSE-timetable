<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Acceleration Methods for Scientific and Machine Learning Applications - Part II of II</h2><div class='index-talk' id='talk409' style='display:block'><a href='javascript:toggle_star(409)' class='star'><span class='star409'>&star;</span></a> <b>11:30 AM&ndash;11:45 AM (G106)</b> Yangyang Xu, Distributed Stochastic Inertial-Accelerated Methods with Delayed Derivatives for Nonconvex Problems <span id='bitlink-378'><small><a href='javascript:show_bit(378)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-378' style='display:none'><small><a href='javascript:hide_bit(378)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Distributed Stochastic Inertial-Accelerated Methods with Delayed Derivatives for Nonconvex Problems</b><br />Yangyang Xu<br />Friday, March 3 11:30 AM&ndash;11:45 AM<br />This is the 1st talk in <a href='session-91.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G106<br /><br /><small>Stochastic gradient methods (SGMs) are predominant approaches for stochastic optimization. On smooth nonconvex problems, acceleration techniques have been applied to improve the convergence of SGMs. However, little exploration has been made on applying acceleration to a stochastic subgradient method (SsGM) for nonsmooth nonconvex problems. Also, few efforts have been made to analyze an (accelerated) SsGM with delayed derivatives. The information delay naturally happens in a distributed system.    
In this talk, I will present an inertial proximal SsGM for nonsmooth nonconvex stochastic optimization. Our method has guaranteed convergence even with delayed derivatives in a distributed environment. Convergence rate results are established to three problem classes: weakly-convex nonsmooth problems with a convex regularizer, composite nonconvex problems with a nonsmooth convex regularizer, and smooth nonconvex problems. In a distributed environment, the convergence rate of the proposed method will be slowed down by the information delay. Nevertheless, the slow-down effect will decay with the number of iterations for the latter two problem classes. We test the proposed method on three applications. The numerical results clearly demonstrate the advantages of using the inertial-based acceleration. Furthermore, we observe higher parallelization speed-up in asynchronous updates over the synchronous counterpart, though the former uses delayed derivatives.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75360' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk410' style='display:block'><a href='javascript:toggle_star(410)' class='star'><span class='star410'>&star;</span></a> <b>11:50 AM&ndash;12:05 AM (G106)</b> Fei Xue, Inexact Anderson Acceleration <span id='bitlink-379'><small><a href='javascript:show_bit(379)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-379' style='display:none'><small><a href='javascript:hide_bit(379)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Inexact Anderson Acceleration</b><br />Fei Xue<br />Friday, March 3 11:50 AM&ndash;12:05 AM<br />This is the 2nd talk in <a href='session-91.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G106<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75360' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk411' style='display:block'><a href='javascript:toggle_star(411)' class='star'><span class='star411'>&star;</span></a> <b>12:10 AM&ndash;12:25 AM (G106)</b> Vivak Patel, Probabilistic Analysis of the Convergence of Stochastic Gradient Descent <span id='bitlink-380'><small><a href='javascript:show_bit(380)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-380' style='display:none'><small><a href='javascript:hide_bit(380)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Probabilistic Analysis of the Convergence of Stochastic Gradient Descent</b><br />Vivak Patel<br />Friday, March 3 12:10 AM&ndash;12:25 AM<br />This is the 3rd talk in <a href='session-91.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G106<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75360' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk412' style='display:block'><a href='javascript:toggle_star(412)' class='star'><span class='star412'>&star;</span></a> <b>12:30 AM&ndash;12:45 AM (G106)</b> Yunhui He, On the Linear Asymptotic Convergence Speed of Anderson Acceleration Applied to Tensor Decomposition and ADMM <span id='bitlink-381'><small><a href='javascript:show_bit(381)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-381' style='display:none'><small><a href='javascript:hide_bit(381)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>On the Linear Asymptotic Convergence Speed of Anderson Acceleration Applied to Tensor Decomposition and ADMM</b><br />Yunhui He<br />Friday, March 3 12:30 AM&ndash;12:45 AM<br />This is the 4th talk in <a href='session-91.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G106<br /><br /><small>In this talk, we consider nonlinear convergence acceleration methods for fixed-point iteration, including Anderson acceleration (AA) and nonlinear GMRES (NGMRES). We focus on fixed-point methods that converge asymptotically linearly and that solve an underlying fully smooth and non-convex optimization problem. It is often observed that AA and NGMRES substantially improve the asymptotic convergence behavior of the fixed-point iteration, but this improvement has not been quantified theoretically. We investigate this problem under simplified conditions. We consider stationary versions of AA and NGMRES, and determine coefficients that result in optimal asymptotic convergence factors, given knowledge of the spectrum of the fixed-point operator at the fixed point. This allows us to understand and quantify the asymptotic convergence improvement that can be provided by nonlinear convergence acceleration. Our results are illustrated numerically for a class of test problems from canonical tensor decomposition, comparing steepest descent and alternating least squares (ALS) as the fixed-point iterations that are accelerated by AA and NGMRES. Our numerical tests show that both approaches allow us to estimate asymptotic convergence speed for nonstationary AA and NGMRES with finite window size. We also apply Anderson acceleration to improve the asymptotic linear convergence speed of the Alternating Direction Method of Multipliers (ADMM) when ADMM by itself converges linearly.  Numerical results also indicate that the optimal linear convergence factor of the stationary AA methods gives a useful estimate for the asymptotic linear convergence speed of the non-stationary AA method that is used in practice.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75360' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk413' style='display:block'><a href='javascript:toggle_star(413)' class='star'><span class='star413'>&star;</span></a> <b>12:50 AM&ndash;1:05 AM (G106)</b> Roummel F. Marcia, Shape-Changing Trust-Region Methods Using Multipoint Symmetric Secant Matrices <span id='bitlink-382'><small><a href='javascript:show_bit(382)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-382' style='display:none'><small><a href='javascript:hide_bit(382)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Shape-Changing Trust-Region Methods Using Multipoint Symmetric Secant Matrices</b><br />Roummel F. Marcia<br />Friday, March 3 12:50 AM&ndash;1:05 AM<br />This is the 5th talk in <a href='session-91.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part II of II</a> (11:30 AM&ndash;1:10 PM)<br />G106<br /><br /><small>We consider methods for large-scale and nonconvex unconstrained optimization. We propose a new trust-region method whose subproblem is defined using a so-called “shape-changing” norm together with densely-initialized multipoint symmetric secant (MSS) matrices to approximate the Hessian. Shape-changing norms and dense initializations have been successfully used in the context of traditional quasi-Newton methods, but have yet to be explored in the case of MSS methods. Numerical results suggest that trust-region methods that use densely-initialized MSS matrices together with shape-changing norms outperform MSS with other trust-region methods.	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75360' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
