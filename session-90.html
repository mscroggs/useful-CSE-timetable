<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Acceleration Methods for Scientific and Machine Learning Applications - Part I of II</h2><div class='index-talk' id='talk404' style='display:block'><a href='javascript:toggle_star(404)' class='star'><span class='star404'>&star;</span></a> <b>9:20 AM&ndash;9:35 AM (G106)</b> Huan He, An Nonlinear Acceleration Method That Exploits Symmetry of Hessian <span id='bitlink-373'><small><a href='javascript:show_bit(373)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-373' style='display:none'><small><a href='javascript:hide_bit(373)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>An Nonlinear Acceleration Method That Exploits Symmetry of Hessian</b><br />Huan He<br />Friday, March 3 9:20 AM&ndash;9:35 AM<br />This is the 1st talk in <a href='session-90.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G106<br /><br /><small>Nonlinear acceleration methods are powerful techniques to speed up fixed-point iterations. However, many acceleration methods require storing a large number of previous iterates and this can become impractical if computational resources are limited. In this work, we propose a nonlinear Truncated Generalized Conjugate Residual method (nlTGCR) whose goal is to exploit the symmetry of the Hessian to reduce memory usage. The proposed method can be interpreted as either an inexact Newton or a quasi-Newton method. We  show that,   with the help of global  strategies like residual check techniques, nlTGCR can converge globally for general nonlinear problems and that under mild conditions, nlTGCR is able to achieve superlinear  convergence.  We further analyze the convergence of nlTGCR in a stochastic setting. Numerical results demonstrate the superiority of nlTGCR when compared with several other competitive baseline  approaches on a few problems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75359' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk405' style='display:block'><a href='javascript:toggle_star(405)' class='star'><span class='star405'>&star;</span></a> <b>9:40 AM&ndash;9:55 AM (G106)</b> Florian Schaefer, Competitive Gradient Descent Algorithms <span id='bitlink-374'><small><a href='javascript:show_bit(374)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-374' style='display:none'><small><a href='javascript:hide_bit(374)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Competitive Gradient Descent Algorithms</b><br />Florian Schaefer<br />Friday, March 3 9:40 AM&ndash;9:55 AM<br />This is the 2nd talk in <a href='session-90.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G106<br /><br /><small>Competitive gradient descent and related method extend gradient descent to multi-agent optimization by solving, at each iteration, for the Nash equilibrium of a local, multilinear approximation of the agents' loss functions.     
This talk presents extensions and applications of competitive gradient descent, including mirror-descent variants based on ideas from information geometry and competitive physics-informed networks (CPINNs) that achieve order-of-magnitude improvements in accuracy compared to PINNs.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75359' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk406' style='display:block'><a href='javascript:toggle_star(406)' class='star'><span class='star406'>&star;</span></a> <b>10:00 AM&ndash;10:15 AM (G106)</b> Fred Roosta, Newton-MR Algorithms with Complexity Guarantees for Non-Convex Optimization <span id='bitlink-375'><small><a href='javascript:show_bit(375)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-375' style='display:none'><small><a href='javascript:hide_bit(375)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Newton-MR Algorithms with Complexity Guarantees for Non-Convex Optimization</b><br />Fred Roosta<br />Friday, March 3 10:00 AM&ndash;10:15 AM<br />This is the 3rd talk in <a href='session-90.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G106<br /><br /><small>Classically, the conjugate gradient (CG) method has been the dominant solver in most inexact Newton-type methods for unconstrained optimization. In this talk, we consider replacing CG with the minimum residual method (MINRES), which is often used for symmetric but possibly indefinite linear systems. We show that MINRES has an inherent ability to detect negative-curvature directions. Equipped with this advantage, we discuss algorithms, under the general name of Newton-MR, which can be used for optimization of general non-convex objectives, and that come with favorable complexity guarantees. We also give numerical examples demonstrating the performance of these methods for various non-convex problems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75359' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk407' style='display:block'><a href='javascript:toggle_star(407)' class='star'><span class='star407'>&star;</span></a> <b>10:20 AM&ndash;10:35 AM (G106)</b> Kevin Scaman, Non-Convex Stochastic Gradient Descent <span id='bitlink-376'><small><a href='javascript:show_bit(376)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-376' style='display:none'><small><a href='javascript:hide_bit(376)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Non-Convex Stochastic Gradient Descent</b><br />Kevin Scaman<br />Friday, March 3 10:20 AM&ndash;10:35 AM<br />This is the 4th talk in <a href='session-90.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G106<br /><br /><small> </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75359' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk408' style='display:block'><a href='javascript:toggle_star(408)' class='star'><span class='star408'>&star;</span></a> <b>10:40 AM&ndash;10:55 AM (G106)</b> Oren E. Livne, Numerical Optimization Algorithm for Estimating a Conditioned Symmetric Positive Definite Matrix Under Constraints <span id='bitlink-377'><small><a href='javascript:show_bit(377)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-377' style='display:none'><small><a href='javascript:hide_bit(377)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Numerical Optimization Algorithm for Estimating a Conditioned Symmetric Positive Definite Matrix Under Constraints</b><br />Oren E. Livne<br />Friday, March 3 10:40 AM&ndash;10:55 AM<br />This is the 5th talk in <a href='session-90.html'>Acceleration Methods for Scientific and Machine Learning Applications - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />G106<br /><br /><small>We present RCO (Regularized Cholesky Optimization): a numerical algorithm for finding a symmetric Positive Definite (PD) $n \times n$ matrix with a bounded condition number that minimizes an objective function. This task arises when estimating a covariance matrix from noisy data or due to model constraints, which can cause spurious small negative eigenvalues. A special case is the problem of finding the nearest well-conditioned PD matrix to a given matrix. RCO explicitly optimizes the entries of the Cholesky factor. This requires solving a regularized nonlinear optimization problem, for which we apply Newton-CG and exploit the Hessian's sparsity. The regularization parameter is determined via numerical continuation with an accuracy-conditioning trade-off criterion.    
We apply RCO to our motivating educational measurement application of estimating the covariance matrix of an Empirical Best Linear Prediction (EBLP) of school growth scores. RCO outperformed general-purpose near-PD algorithms (Higham's method; Tanaka-Nakata) in terms of the EBLP estimate bias and accuracy of its mean squared error. We present results for two empirical datasets: a large urban school district dataset and state dataset.    
For finding the nearest PD matrix, RCO yields similar results to near-PD methods. While all methods admit an $O(n^3)$ runtime complexity, RCO's constant is $100 \times$ larger, thus it is best suited for general objective functions as opposed to this particular task.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75359' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
