<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Operator Learning in the Physical and Data Sciences - Part I of II</h2><div class='index-talk' id='talk200' style='display:block'><a href='javascript:toggle_star(200)' class='star'><span class='star200'>&star;</span></a> <b>9:20 AM&ndash;9:35 AM (E108)</b> Thomas O'Leary-Roseberry, Learning High-Dimensional Parametric Derivatives with Neural Operators <span id='bitlink-187'><small><a href='javascript:show_bit(187)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-187' style='display:none'><small><a href='javascript:hide_bit(187)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Learning High-Dimensional Parametric Derivatives with Neural Operators</b><br />Thomas O'Leary-Roseberry<br />Friday, March 3 9:20 AM&ndash;9:35 AM<br />This is the 1st talk in <a href='session-44.html'>Operator Learning in the Physical and Data Sciences - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />E108<br /><br /><small>In this talk we will present efficient strategies for learning high-dimensional derivative information via neural operators. By exploiting low-dimensional information of a high-dimensional map, if it exists, one can both generate and learn high dimensional derivative information where the dominant computational costs can be made independent of the discretization dimensions. Numerical results demonstrate that this additional derivative information improves the function approximation, and additionally neural operators that are not trained on derivative information are unlikely to produce reliable derivatives with respect to high-dimensional parameters. Numerical examples will demonstrate how these derivative informed neural operators can be used to accelerate the solutions of stochastic optimization problems and high-dimensional inference problems.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75264' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk201' style='display:block'><a href='javascript:toggle_star(201)' class='star'><span class='star201'>&star;</span></a> <b>9:40 AM&ndash;9:55 AM (E108)</b> Jakob Zech, Expression Rate Bounds for Neural and GPC Operator Surrogates <span id='bitlink-188'><small><a href='javascript:show_bit(188)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-188' style='display:none'><small><a href='javascript:hide_bit(188)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Expression Rate Bounds for Neural and GPC Operator Surrogates</b><br />Jakob Zech<br />Friday, March 3 9:40 AM&ndash;9:55 AM<br />This is the 2nd talk in <a href='session-44.html'>Operator Learning in the Physical and Data Sciences - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />E108<br /><br /><small>Approximation rates are analyzed for deep surrogates of maps between infinite-dimensional function spaces, which arise e.g. as data-to-solution maps for linear and nonlinear partial differential equations. Such surrogates may be used to speed up computations in parameter estimation problems in engineering. We study in particular deep neural surrogate operators for holomorphic maps between separable Hilbert spaces, where the operator inputs are parametrized by stable, affine representation systems such as frames. Additionally, we discuss an interpolation based alternative to this framework, that allows for a deterministic construction and therefore does not require training of the network weights. Algebraic and dimension independent convergence rates are established in both cases.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75264' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk202' style='display:block'><a href='javascript:toggle_star(202)' class='star'><span class='star202'>&star;</span></a> <b>10:00 AM&ndash;10:15 AM (E108)</b> Katiana Kontolati, Transfer and Multi-Task Learning in Physics-Based Applications with Deep Neural Operators <span id='bitlink-189'><small><a href='javascript:show_bit(189)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-189' style='display:none'><small><a href='javascript:hide_bit(189)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Transfer and Multi-Task Learning in Physics-Based Applications with Deep Neural Operators</b><br />Katiana Kontolati<br />Friday, March 3 10:00 AM&ndash;10:15 AM<br />This is the 3rd talk in <a href='session-44.html'>Operator Learning in the Physical and Data Sciences - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />E108<br /><br /><small>Traditional machine learning algorithms are learned in isolation, i.e., a predictive model is trained for a single task. In cases where multiple tasks are considered, this learning approach can be computationally prohibitive. Furthermore, when only few and insufficient labeled data are available for a given task, training a model from scratch might lead to overfitting. Transfer learning allows us to leverage information from a model trained on a source domain with sufficient labeled data and transfer it to a different but closely related target domain for which only a small number of data is available. We propose a new TL framework for task-specific learning of partial differential equations (PDEs) under multiple domains that are heterogeneous but subtly correlated, based on the deep operator network (DeepONet). After the training of a source operator regression model, additional given tasks are learned via the fine-tuning of task-specific layers based on a hybrid loss function that allows for the matching of individual target samples while also preserving the global properties of the conditional distribution of target data. The task-specific training is based on the conditional embedding operator theory where conditional target distributions are embedded onto a reproducing kernel Hilbert space. We demonstrate the advantages of our approach for various TL scenarios involving nonlinear PDEs under diverse conditions due to shift in the geometric domain and model dynamics.     </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75264' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk203' style='display:block'><a href='javascript:toggle_star(203)' class='star'><span class='star203'>&star;</span></a> <b>10:20 AM&ndash;10:35 AM (E108)</b> Zachary Morrow, RaISE: A Framework to Characterize Surrogate Models in Scientific Machine Learning <span id='bitlink-190'><small><a href='javascript:show_bit(190)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-190' style='display:none'><small><a href='javascript:hide_bit(190)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>RaISE: A Framework to Characterize Surrogate Models in Scientific Machine Learning</b><br />Zachary Morrow<br />Friday, March 3 10:20 AM&ndash;10:35 AM<br />This is the 4th talk in <a href='session-44.html'>Operator Learning in the Physical and Data Sciences - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />E108<br /><br /><small>We present a novel and unifying framework of characterizing surrogate models for the emerging field of scientific machine learning (SciML). In this context, the computational cost of data collection dominates the cost of surrogate construction or evaluation. Accordingly, we define  robustness,  scalability, and  efficiency in terms of  accuracy. These three concepts then collectively inform a user-specific notion of  interpretability. We apply this framework to methods of both function and operator approximation in order to (i) create a comprehensive and intuitively accessible catalogue for SciML and (ii) demonstrate the utility of our framework in practice.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75264' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1582' style='display:block'><a href='javascript:toggle_star(1582)' class='star'><span class='star1582'>&star;</span></a> <b>10:40 AM&ndash;10:55 AM (E108)</b> Spencer H. Bryngelson, Super-Spectral Operator Recovery via the Fast Macroscopic Forcing Method <span id='bitlink-1453'><small><a href='javascript:show_bit(1453)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1453' style='display:none'><small><a href='javascript:hide_bit(1453)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Super-Spectral Operator Recovery via the Fast Macroscopic Forcing Method</b><br />Spencer H. Bryngelson<br />Friday, March 3 10:40 AM&ndash;10:55 AM<br />This is the 5th talk in <a href='session-44.html'>Operator Learning in the Physical and Data Sciences - Part I of II</a> (9:20 AM&ndash;11:00 AM)<br />E108<br /><br /><small>The macroscopic forcing method (MFM) was introduced by Mani and Park in 2021. It recovers lower-dimensional operators by successively forcing a high-dimensional direct numerical simulation. The MFM has already successfully recovered RANS-like turbulence models for fluid flows. Standard algorithms for MFM apply forcings to each coarse-scale degree of freedom and conduct a fine-scale simulation, which is expensive. We present an algorithm that is cheaper and more general. It applies sparse reconstruction to expose local features in the differential operator and reconstructs the coarse one in only a few matrix-vector products. For non-local operators, we prepend this approach by peeling long-range effects with dense matrix-vector products to expose a more local operator. We demonstrate the algorithm's performance on scalar transport and channel flow problems.  	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75264' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
