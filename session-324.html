<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part I of II</h2><div class='index-talk' id='talk1471' style='display:block'><a href='javascript:toggle_star(1471)' class='star'><span class='star1471'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D403)</b> Janine Weber, Learning the Constraints in Adaptive Feti-Dp Domain Decomposition Methods <span id='bitlink-1353'><small><a href='javascript:show_bit(1353)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1353' style='display:none'><small><a href='javascript:hide_bit(1353)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Learning the Constraints in Adaptive Feti-Dp Domain Decomposition Methods</b><br />Janine Weber<br />Tuesday, February 28 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-324.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>Domain decomposition methods (DDMs) are highly scalable, iterative solvers for the solution of large systems of linear equations, e.g., arising from the discretization of PDEs.  The convergence rate of classic DDMs in general deteriorates severly for coefficient distributions with large contrasts in the coefficient function.  To retain the robustness for such problems, the coarse space of the DDM can be enriched by additional coarse basis functions, often obtained by solving local generalized eigenvalue problems.  However, the set-up and the solution of these eigenvalue problems typically takes up a significant part of the total time to solution. Additionally, for many realistic problems, only the solution of a small number of the eigenvalue problems is necessary to design a robust algorithm. In general, it is difficult to predict a priori which of the eigenvalue problems are needed.  Using a neural network model we can predict the geometric location where eigenvalue problems have to be solved, often reducing its number significantly (joint work of the authors with Alexander Heinlein). To obtain such an a priori classification, we use a mesh-independent sampling strategy which is comparable to an image recognition problem.  In a next step, we train a surrogate model which directly learns the necessary coarse basis functions themselves using again an image representation of the underlying coefficient function.   Numerical results indicate the robustness of this approach.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75727' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1472' style='display:block'><a href='javascript:toggle_star(1472)' class='star'><span class='star1472'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D403)</b> Alena Kopanicakova, Towards Large-Scale Training of Deep Neural Networks Using Domain-Decomposition Methods <span id='bitlink-1354'><small><a href='javascript:show_bit(1354)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1354' style='display:none'><small><a href='javascript:hide_bit(1354)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Towards Large-Scale Training of Deep Neural Networks Using Domain-Decomposition Methods</b><br />Alena Kopanicakova<br />Tuesday, February 28 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-324.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>Deep neural networks (DNNs) are routinely used in a wide range of application areas and scientific fields, as they allow to efficiently predict the behavior of complex systems. However, before the DNNs can be effectively used for the prediction, their parameters have to be determined during the training process. Traditionally, the training process is associated with the minimization of a loss function, which is commonly performed using variants of the stochastic gradient (SGD) method. Although SGD and its variants have a low computational cost per iteration, their convergence properties tend to deteriorate with increasing network size. In this talk, we aim to alleviate the training cost of DNNs by leveraging the nonlinear domain decomposition strategies. The subdomains will be constructed by exploiting two complementary approaches, namely the decomposition of the data and the parameter space. This will give rise to two new classes of training methods, convergence properties of which will be analyzed using a series of numerical experiments. Comparison with state-of-the-art optimizers will be also performed, demonstrating the efficiency of our novel training methods. Moreover, we will discuss our parallelization strategy, its shortcomings, and possible future directions.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75727' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1473' style='display:block'><a href='javascript:toggle_star(1473)' class='star'><span class='star1473'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D403)</b> Alexander Heinlein, Domain Decomposition Training Strategies for Physics-Informed Neural Networks <span id='bitlink-1355'><small><a href='javascript:show_bit(1355)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1355' style='display:none'><small><a href='javascript:hide_bit(1355)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Domain Decomposition Training Strategies for Physics-Informed Neural Networks</b><br />Alexander Heinlein<br />Tuesday, February 28 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-324.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>Physics-informed neural networks (PINNs) are a solution method for solving boundary value problems based on differential equations (PDEs). The key idea of PINNs is to incorporate the residual of the PDE as well as boundary conditions into the loss function of the neural network. This provides a simple and mesh-free approach for solving problems relating to PDEs. However, a key limitation of PINNs is their lack of accuracy and efficiency when solving problems with larger domains and more complex, multi-scale solutions.    
In a more recent approach, Finite Basis Physics-Informed Neural Networks  (FBPINNs), the authors use ideas from domain decomposition to accelerate the learning process of PINNs and improve their accuracy in this setting. In this talk, we show how Schwarz-like additive, multiplicative, and hybrid iteration methods for training FBPINNs can be developed. Furthermore, we will present numerical experiments on the influence on convergence and accuracy of these different variants.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75727' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1474' style='display:block'><a href='javascript:toggle_star(1474)' class='star'><span class='star1474'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D403)</b> Eric C. Cyr, Combing Layer-Parallel Training with Multilevel Optimization Techniques <span id='bitlink-1356'><small><a href='javascript:show_bit(1356)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1356' style='display:none'><small><a href='javascript:hide_bit(1356)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Combing Layer-Parallel Training with Multilevel Optimization Techniques</b><br />Eric C. Cyr<br />Tuesday, February 28 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-324.html'>On the Mutual Benefit of  Machine Learning and Domain Decomposition/Multilevel Methods - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D403<br /><br /><small>Training of deep neural networks is computationally costly due to slow convergence of optimizers like stochastic gradient descent, coupled with limited parallelism in terms of number of layers. The later point implies that there is an upper bound on the strong scaling of these methods. However, recent advances have pursued multilevel approaches that tackle each of these issues independently. Convergence has been accelerated using algorithms based on the Multigrid Optimization (MGOpt) framework. These approaches in combination with a trust-region based search direction can reduce the number of epochs required to train. On the other hand, a Layer-Parallel scheme that applies multigrid-in-time (MGRIT) approaches for both forward and back propagation has shown order of magnitude level parallel speedups for various neural network architectures. The proposed MGOpt algorithm and the MGRIT strategy both use an identical coarsening in time algorithm. This talk will consider leveraging this similarity to improve the parallel performance of the MGOpt approach using a Layer-Parallel decomposition. To demonstrate the performance of this methodology we will apply our approach to standard problems from the deep learning literature.       
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75727' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
