<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Forward Alternatives to Back-Propagation in ML and Science</h2><div class='index-talk' id='talk1841' style='display:block'><a href='javascript:toggle_star(1841)' class='star'><span class='star1841'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (G101)</b> Krishnan Raghavan, The Pitfalls of Backpropagation &#8211; Some Perspectives and Alternatives <span id='bitlink-1684'><small><a href='javascript:show_bit(1684)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1684' style='display:none'><small><a href='javascript:hide_bit(1684)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>The Pitfalls of Backpropagation &#8211; Some Perspectives and Alternatives</b><br />Krishnan Raghavan<br />Tuesday, February 28 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-406.html'>Forward Alternatives to Back-Propagation in ML and Science</a> (9:45 AM&ndash;11:25 AM)<br />G101<br /><br /><small>The ubiquity of neural networks has pushed the boundaries of human understanding in many areas such as science, healthcare, finance. The key reason for this popularity is the availability of GPU infrastructure to enable fast matrix calculations  necessary to perform back-propagation—the workhorse behind the design and development of neural network. While backpropagation has been proven effective, it is not without its pitfalls. In this talk, we will discuss the many challenges that arise during the design and development of neural networks through backpropagation—especially in science application. We will then provide alternatives to backpropagation that are both theoretically sound and can be easily amalgamated into the current GPU infrastructure for fast and efficient training of neural networks. We will show that, our alternatives can provide performance equivalent to backpropagation when gradients are available with the added advantage of being able to address scenarios where the gradients may be inaccurate. We will demonstrate the efficiency of these alternatives in GPU infrastructures and in AI accelerators and derives perspectives for future research.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75887' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1842' style='display:block'><a href='javascript:toggle_star(1842)' class='star'><span class='star1842'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (G101)</b> Arvind Sundaram, Randomized Dimensionality Reduction Techniques for Automatic Differentiation <span id='bitlink-1685'><small><a href='javascript:show_bit(1685)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1685' style='display:none'><small><a href='javascript:hide_bit(1685)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Randomized Dimensionality Reduction Techniques for Automatic Differentiation</b><br />Arvind Sundaram<br />Tuesday, February 28 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-406.html'>Forward Alternatives to Back-Propagation in ML and Science</a> (9:45 AM&ndash;11:25 AM)<br />G101<br /><br /><small>Recent research has shown the power of randomized dimensionality reduction techniques in creating accuracy preserving reduction of general nonlinear models. The reduction is in the form of active subspaces, described by linear transformations of the model parameters and responses. Theoretical results prove that the errors resulting from the reduction can be upper bounded with high probability. This talk shows how the resulting active subspaces can be leveraged to reduce the computational burden associated with Automatic Differentiation by confining the differentiation to a number of pseudo parameters and responses as defined by the dimensionality reduction algorithm.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75887' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1843' style='display:block'><a href='javascript:toggle_star(1843)' class='star'><span class='star1843'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (G101)</b> Tim Gymnich, Efficient Batched Forward-Mode Derivatives with Compiler-Based Automatic Differentiation <span id='bitlink-1686'><small><a href='javascript:show_bit(1686)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1686' style='display:none'><small><a href='javascript:hide_bit(1686)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Efficient Batched Forward-Mode Derivatives with Compiler-Based Automatic Differentiation</b><br />Tim Gymnich<br />Tuesday, February 28 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-406.html'>Forward Alternatives to Back-Propagation in ML and Science</a> (9:45 AM&ndash;11:25 AM)<br />G101<br /><br /><small>Derivatives are becoming ever more central to modern approaches in the computational sciences. From gradient-based optimization over uncertainty quantification to the incorporation of machine learning approaches into our simulation workflows, the ability to obtain gradients of our simulation code determines our ability to access this exceedingly large toolbox. But for a large number of our simulations the rewriting of entire simulations in differentiable domain-specific languages such as JAX or PyTorch is simply infeasible. Compiler-based automatic differentiation with Enzyme enables the synthesization of gradients even for these simulations in any language which utilizes LLVM in their compiler such as e.g. C/C++, Julia, Fortran, Rust, and Swift. This tight integration into the compiler enables Enzyme to synthesize much more efficient gradients by operating on representations already optimized by the compiler.  In this talk we present key extensions to Enzyme which enable the auto-batching of operations inside of Enzyme to convert scalar functions into vectorized functions for more efficient gradients in downstream applications.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75887' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1844' style='display:block'><a href='javascript:toggle_star(1844)' class='star'><span class='star1844'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (G101)</b> Leila Ghaffari, Forward-Mode Enzyme in Developing Constitutive Models with Ratel <span id='bitlink-1687'><small><a href='javascript:show_bit(1687)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1687' style='display:none'><small><a href='javascript:hide_bit(1687)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Forward-Mode Enzyme in Developing Constitutive Models with Ratel</b><br />Leila Ghaffari<br />Tuesday, February 28 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-406.html'>Forward Alternatives to Back-Propagation in ML and Science</a> (9:45 AM&ndash;11:25 AM)<br />G101<br /><br /><small>Ratel is a new, open-source package built on libCEED and PETSc capable of solving complex solid mechanics problems without sacrificing computational performance. Computing derivatives is essential to the algorithms employed within Ratel. However, deriving and implementing derivatives of some constitutive models could be cumbersome hence exploiting automatic differentiation (AD) tools could simplify the implementation or provide verification results for hand-coded derivatives. Enzyme is a new LLVM plugin with GPU support that provides split forward and reverse mode AD on LLVM intermediate representation (IR). We explore the applicability and performance of Enzyme in computing tensor gradients in some of our elasticity experiments in Ratel.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75887' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1845' style='display:block'><a href='javascript:toggle_star(1845)' class='star'><span class='star1845'>&star;</span></a> <b>11:05 AM&ndash;11:20 AM (G101)</b> Markus Towara, Parallel Adjoint Taping Using MPI <span id='bitlink-1688'><small><a href='javascript:show_bit(1688)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1688' style='display:none'><small><a href='javascript:hide_bit(1688)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Parallel Adjoint Taping Using MPI</b><br />Markus Towara<br />Tuesday, February 28 11:05 AM&ndash;11:20 AM<br />This is the 5th talk in <a href='session-406.html'>Forward Alternatives to Back-Propagation in ML and Science</a> (9:45 AM&ndash;11:25 AM)<br />G101<br /><br /><small>Automatic Differentiation (AD) allows to efficiently and accurately calculate derivatives of expressions stated as computer code.  The adjoint data flow reversal of long evolutionary calculations (e.g. loops), where each iteration depends on a set of parameters and the output of the previous iterate, is a common occurrence in computational engineering (e.g. computational fluid dynamics simulation), physics (e.g. molecular dynamics) and computational finance (e.g. Monte Carlo paths).  For the extreme case of a scalar state, the execution as well as adjoint control flow reversal are inherently serial operations, as there is no spatial dimension to parallelize.  We propose a method, focusing on programs with such a structure, that exploits the run time difference typically exhibited by AD tools between pure function evaluation and evaluation with additional calculation of local derivative information (a process frequently called pre-accumulation), which is then later used by the data flow reversal.    
Additional parallelism is introduced into the computation by distributing the aforementioned calculation of local derivatives onto multiple processes.  A reference C++ implementation using MPI is presented, which allows us to reverse an OpenFOAM simulation.    
The proposed method is most beneficial for operator overloading AD tools, however the concepts are also applicable to source-to-source transformation and handwritten adjoints, or a hybrid of all approaches.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75887' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
