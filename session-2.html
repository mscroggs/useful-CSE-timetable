<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Progress and Challenges in Extreme Scale Computing and Big Data</h2><div class='index-talk' id='talk8' style='display:block'><a href='javascript:toggle_star(8)' class='star'><span class='star8'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D304)</b> Michael A. Heroux, Extreme-Scale CSE Challenges: Progress via Community Solutions <span id='bitlink-8'><small><a href='javascript:show_bit(8)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-8' style='display:none'><small><a href='javascript:hide_bit(8)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Extreme-Scale CSE Challenges: Progress via Community Solutions</b><br />Michael A. Heroux<br />Monday, February 27 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-2.html'>Progress and Challenges in Extreme Scale Computing and Big Data</a> (9:45 AM&ndash;11:25 AM)<br />D304<br /><br /><small>Extreme-scale CSE projects involve many people and skill sets, and complex hardware and software environments.  Technical excellence is essential in these environments, but success also hinges on collaboration, complementarity, and coordination of efforts within and across teams and communities.    
In this talk, we discuss efforts in the Exascale Computing Project to produce community-driven ecosystems and team structures that support our abilities to holistically pursue the goals of extreme-scale computational science.  We highlight ongoing efforts to create hierarchical software organizations and stacks, and emerging efforts to improve scientific software development and use via the application of social and cognitive sciences to software teams and communities.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75178' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk9' style='display:block'><a href='javascript:toggle_star(9)' class='star'><span class='star9'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D304)</b> Kengo Nakajima, h3-Open-BDEC: Innovative Software Infrastructure for Scientific Computing in the Exascale Era by Integrations of (Simulation + Data + Learning) <span id='bitlink-9'><small><a href='javascript:show_bit(9)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-9' style='display:none'><small><a href='javascript:hide_bit(9)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>h3-Open-BDEC: Innovative Software Infrastructure for Scientific Computing in the Exascale Era by Integrations of (Simulation + Data + Learning)</b><br />Kengo Nakajima<br />Monday, February 27 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-2.html'>Progress and Challenges in Extreme Scale Computing and Big Data</a> (9:45 AM&ndash;11:25 AM)<br />D304<br /><br /><small>We propose an innovative method for computational science for sustainable promotion of scientific discovery by supercomputers in the Exascale Era by combining (Simulation + Data + Learning (S+D+L)). In May 2021, we started operation of the Wisteria/BDEC-01 system with 33+PF at University of Tokyo. It is a Hierarchical, Hybrid, Heterogeneous (h3) system, which consists of computing nodes for CSE with A64FX and those for Data Analytics/AI with NVIDIA A100 GPU’s. We develop a software platform “h3-Open-BDEC” for integration of (S+D+L) and evaluate the effects of integration of (S+D+L) on the Wisteria system. The h3-Open-BDEC is designed for extracting the maximum performance of the supercomputers with minimum energy consumption focusing on (1) innovative method for numerical analysis with high-performance/high-reliability/power-saving based on the new principle of computing by adaptive precision, accuracy verification and automatic tuning, (2) Hierarchical Data Driven Approach (hDDA) based on machine learning, and (3) Software for heterogeneous systems, such as Wisteria/BDEC-01. Integration of (S+D+L) by h3-Open-BDEC enables significant reduction of computations and power consumption, compared to those by conventional simulations.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75178' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk10' style='display:block'><a href='javascript:toggle_star(10)' class='star'><span class='star10'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D304)</b> Matthias M&#252;ller, Combining HPC, AI and RDM: Challenges and Approaches <span id='bitlink-10'><small><a href='javascript:show_bit(10)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-10' style='display:none'><small><a href='javascript:hide_bit(10)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Combining HPC, AI and RDM: Challenges and Approaches</b><br />Matthias M&#252;ller<br />Monday, February 27 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-2.html'>Progress and Challenges in Extreme Scale Computing and Big Data</a> (9:45 AM&ndash;11:25 AM)<br />D304<br /><br /><small>The availability of large amount of digital data, computational power combined with new approaches of data analysis and so called artificial intelligence created the potential of new scientific discoveries in almost all scientific fields. However, to realize the potential a number of challenges need to be addressed. The amount, quality and accessibility of data has to be suitable. In short, the FAIR principles need to be implemented in a proper manner and the meta data has to be suitable and sufficient for the selected machine learning method.   Many of the challenges can only be addressed if the IT infrastructure offers suitable and comprehensive support in the fields of HPC, AI, and RDM. This talk will present the approach taken by RWTH Aachen University and its partners to address the challenges.     
The approach includes  1. Engagement in several projects of the German Initiative for Research Data Management (NFDI) with a focus on engineering.   2. Creation of a data integration platform Coscine implementing the FAIR principles and FAIR Digital Object interfaces.  3. Process Mining technologies to discover similarities between data sets and reveal otherwise undetected workflows   4. Data stewards in large, long time collaborative projects (CRCs)  5. Integration of HPC and RDM platforms to exploit data analytics capabilities of HPC.    
Besides the scientific content the talk will cover the technical, financial, and human resources of the activities.     
    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75178' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk11' style='display:block'><a href='javascript:toggle_star(11)' class='star'><span class='star11'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D304)</b> Serge G. Petiton, On the Road to Brain-Scale Datasets and Applications for Post-Exascale Supercomputers <span id='bitlink-11'><small><a href='javascript:show_bit(11)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-11' style='display:none'><small><a href='javascript:hide_bit(11)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>On the Road to Brain-Scale Datasets and Applications for Post-Exascale Supercomputers</b><br />Serge G. Petiton<br />Monday, February 27 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-2.html'>Progress and Challenges in Extreme Scale Computing and Big Data</a> (9:45 AM&ndash;11:25 AM)<br />D304<br /><br /><small>Several machine learning methods, such as Graph Convolutional Networks, would manipulate graphs with dozens of billions of nodes. Such brain-scale datasets are represented by very large sparse adjacency matrices, often associated with Laplacian or transition matrices. Moreover, rectangular skinny matrices may store features of nodes. Analyses of those data, such as rankings, homophily evaluations, and clusters detections, for example, are often required by those applications and methods. Moreover, the highly hierarchical architectures of the supercomputers allowing such computation, from multicore chips having network on chip, to distributed nodes and data storage units, are not always well-adapted for such computation manipulating non-structured data. Understanding the behaviors of such methods is important and lead to large number of experiments, asking to have fast accesses to many different datasets. In this talk, we first present two dataset generators, directly computed in parallel, allowing to experiment very large problems without any I/O. We introduce an open source software which generates very large datasets inspired from an easy brain structure, experimented on distributed platform and supercomputers. Then, as an example of applications on such graphs, we present results of PageRank methods experimented on several supercomputers. We conclude with analyses of obtained performances with respect of the dataset structures and of the supercomputer architectures    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75178' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
