<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Deep Learning for Sequence Modeling - Part II of II</h2><div class='index-talk' id='talk886' style='display:block'><a href='javascript:toggle_star(886)' class='star'><span class='star886'>&star;</span></a> <b>1:50 PM&ndash;2:05 PM (E107)</b> Pedram Hassanzadeh, Long-Time Stability of Deep Learning-Based Forecast Models of Multi-Scale Chaos <span id='bitlink-815'><small><a href='javascript:show_bit(815)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-815' style='display:none'><small><a href='javascript:hide_bit(815)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Long-Time Stability of Deep Learning-Based Forecast Models of Multi-Scale Chaos</b><br />Pedram Hassanzadeh<br />Wednesday, March 1 1:50 PM&ndash;2:05 PM<br />This is the 1st talk in <a href='session-194.html'>Deep Learning for Sequence Modeling - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E107<br /><br /><small>There is growing interest in fully data-driven forecast models (aka digital twins) for multi-scale nonlinear dynamical systems such as Earth’s climate and turbulence. Recent studies have shown the promise of deep neural networks (DNNs) for this purpose, producing models (learned from data) with short-term forecast skills comparable to those of high-fidelity numerical solvers. However, these models are all long-time unstable, failing to reproduce the statistics of the system. The roots of these instabilities are not understood. Here, using observational weather data and simulation data of turbulent flows, we show that the root of the instability is “spectral bias”: the well-known inability of DNNs in learning small scales. We demonstrate how the DNNs’ fundamental inability to represent small scales instigates instability, which then propagates to the large scales. We further show that training a DNN using a Fourier spectral loss function that promotes the learning of small scales combined with a time-integration scheme that dampens error propagation leads to a long-time stable DNN for both testcases. The framework developed here is readily applicable to stabilizing digital twins of any other multi-scale chaotic or turbulent system. Such long-time stable digital twins have wide applications in science and engineering.   </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75500' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk887' style='display:block'><a href='javascript:toggle_star(887)' class='star'><span class='star887'>&star;</span></a> <b>2:10 PM&ndash;2:25 PM (E107)</b> Omri Azencot, Multifactor Sequential Disentanglement via Spectrally-Structured Koopman Autoencoders <span id='bitlink-816'><small><a href='javascript:show_bit(816)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-816' style='display:none'><small><a href='javascript:hide_bit(816)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Multifactor Sequential Disentanglement via Spectrally-Structured Koopman Autoencoders</b><br />Omri Azencot<br />Wednesday, March 1 2:10 PM&ndash;2:25 PM<br />This is the 2nd talk in <a href='session-194.html'>Deep Learning for Sequence Modeling - Part II of II</a> (1:50 PM&ndash;3:30 PM)<br />E107<br /><br /><small>Disentangling complex data to its latent factors of variation is a fundamental task in representation learning. Existing work on sequential disentanglement mostly provides two factor representations, i.e., it separates the data to time-varying and time-invariant factors. In contrast, we consider multifactor disentanglement in which multiple (more than two) semantic disentangled components are generated. Key to our approach is a strong inductive bias where we assume that the underlying dynamics can be represented linearly in the latent space. Under this assumption, it becomes natural to exploit the recently introduced Koopman autoencoder models. However, disentangled representations are not guaranteed in Koopman approaches, and thus we propose a novel spectral loss term which leads to structured Koopman matrices and disentanglement. Overall, we propose a simple and easy to code new deep model that is fully unsupervised and it supports multifactor disentanglement. We showcase new disentangling abilities such as swapping of individual static factors between characters, and an incremental swap of disentangled factors from the source to the target. Moreover, we evaluate our method extensively on two factor standard benchmark tasks where we significantly improve over competing unsupervised approaches, and we perform competitively in comparison to weakly- and self-supervised state-of-the-art approaches.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75500' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
