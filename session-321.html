<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Geometry and Shape Analysis for Neuroscience: Geometric Structures in Neuroscience - Part I of II</h2><div class='index-talk' id='talk1457' style='display:block'><a href='javascript:toggle_star(1457)' class='star'><span class='star1457'>&star;</span></a> <b>9:45 AM&ndash;10:00 AM (D408)</b> Sophia Sanborn, From Neural Manifolds to Neural Lie Groups: Group Learning for Neuroscience <span id='bitlink-1341'><small><a href='javascript:show_bit(1341)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1341' style='display:none'><small><a href='javascript:hide_bit(1341)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>From Neural Manifolds to Neural Lie Groups: Group Learning for Neuroscience</b><br />Sophia Sanborn<br />Monday, February 27 9:45 AM&ndash;10:00 AM<br />This is the 1st talk in <a href='session-321.html'>Geometry and Shape Analysis for Neuroscience: Geometric Structures in Neuroscience - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D408<br /><br /><small>An emerging set of findings in sensory and motor neuroscience is beginning to illuminate a new perspective on neural coding. Across sensory and motor regions of the brain, neural circuits are found to mirror the group transformation structure of the systems they represent—either in their synaptic structure, or in the implicit manifold generated by their activity. This phenomenon can be observed in the circuit of neurons representing head direction in the fly (S1), in the activities of grid cells for spatial navigation (T2), and even in the physical structure of the ear’s semi-circular canals for the vestibular sense (SO(3)). These findings suggest a general computational strategy that is employed throughout the brain to preserve the group structure of data throughout stages of information processing. However, to discover phenomena beyond these relatively simple and well-understood circuits, we will need methods capable of discovering unknown group structure in data—an approach I call group learning. In this talk, I present Bispectral Neural Networks for group learning and demonstrate the potential of this approach an algebraic alternative to topological data analysis or manifold learning in the neuroscience toolkit.     
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75724' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1459' style='display:block'><a href='javascript:toggle_star(1459)' class='star'><span class='star1459'>&star;</span></a> <b>10:05 AM&ndash;10:20 AM (D408)</b> Benjamin Dunn, Topological Structures in Neural Activity <span id='bitlink-1342'><small><a href='javascript:show_bit(1342)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1342' style='display:none'><small><a href='javascript:hide_bit(1342)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Topological Structures in Neural Activity</b><br />Benjamin Dunn<br />Monday, February 27 10:05 AM&ndash;10:20 AM<br />This is the 2nd talk in <a href='session-321.html'>Geometry and Shape Analysis for Neuroscience: Geometric Structures in Neuroscience - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D408<br /><br /><small>Using the principle of Dowker duality, we show that the topological features of the space of correlations are equivalent to those of the neural state space when the neural population consists of a single ensemble of neurons with convex receptive fields. We then consider the implications of these conditions both for topological approaches to neural data as well as in a latent variable model (a modified VAE with spline-based, parametric receptive fields and ensemble selection).    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75724' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1460' style='display:block'><a href='javascript:toggle_star(1460)' class='star'><span class='star1460'>&star;</span></a> <b>10:25 AM&ndash;10:40 AM (D408)</b> Erik Bekkers, Neural Ideograms: Unsupervised Learning of Geometric Symbols via Kendall Shape Space VAEs <span id='bitlink-1343'><small><a href='javascript:show_bit(1343)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1343' style='display:none'><small><a href='javascript:hide_bit(1343)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Neural Ideograms: Unsupervised Learning of Geometric Symbols via Kendall Shape Space VAEs</b><br />Erik Bekkers<br />Monday, February 27 10:25 AM&ndash;10:40 AM<br />This is the 3rd talk in <a href='session-321.html'>Geometry and Shape Analysis for Neuroscience: Geometric Structures in Neuroscience - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D408<br /><br /><small>A picture speaks a thousand words, which is poetically beautiful, but highly problematic from a practical point of view. That is why in our daily lives we work with symbols, ideograms in general, to convey information as efficiently as possible. In this work, we show that the idea of compressing information into geometric symbols is not only computationally feasible with artificial neural networks, but also that it allows for learning effective and interpretable neural representations. Kendall shape VAEs can learn shapes that are translation, rotation, and scale invariant in an unsupervised manner. We qualitatively inspect the learned symbols and observe that they consistently represent classes of images that are equivalent by group actions and semantic meaning. In a quantitative comparison to other types of VAEs, we show that Kendall shape VAEs are more efficient in learning compressed representations and that the learned representations convey more information for downstream tasks such as classification.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75724' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1462' style='display:block'><a href='javascript:toggle_star(1462)' class='star'><span class='star1462'>&star;</span></a> <b>10:45 AM&ndash;11:00 AM (D408)</b> Yann Thanwerdas, Geometries of Covariance and Correlation Matrices <span id='bitlink-1344'><small><a href='javascript:show_bit(1344)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1344' style='display:none'><small><a href='javascript:hide_bit(1344)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Geometries of Covariance and Correlation Matrices</b><br />Yann Thanwerdas<br />Monday, February 27 10:45 AM&ndash;11:00 AM<br />This is the 4th talk in <a href='session-321.html'>Geometry and Shape Analysis for Neuroscience: Geometric Structures in Neuroscience - Part I of II</a> (9:45 AM&ndash;11:25 AM)<br />D408<br /><br /><small>Many data from neuroscience can me modeled by covariance matrices (EEG, MEG, MRI). This talk tackles two important challenges: decoupling the correlation part from the scaling part in the covariance matrix, and computing with singular covariance matrices. To do so, we present new geometries of the set of full-rank correlation matrices and we present the Bures-Wasserstein geometry of symmetric positive semi-definite matrices.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75724' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
