<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Progresses on Batched Linear Solver Algorithms - Part I of II</h2><div class='index-talk' id='talk1260' style='display:block'><a href='javascript:toggle_star(1260)' class='star'><span class='star1260'>&star;</span></a> <b>2:35 PM&ndash;2:50 PM (G107)</b> Siva Rajamanickam, Batched Linear Solvers in Kokkos Kernels <span id='bitlink-1166'><small><a href='javascript:show_bit(1166)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1166' style='display:none'><small><a href='javascript:hide_bit(1166)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Batched Linear Solvers in Kokkos Kernels</b><br />Siva Rajamanickam<br />Thursday, March 2 2:35 PM&ndash;2:50 PM<br />This is the 1st talk in <a href='session-277.html'>Progresses on Batched Linear Solver Algorithms - Part I of II</a> (2:35 PM&ndash;4:15 PM)<br />G107<br /><br /><small>Applications such as multilevel finite element methods, modeling collision in plasma and reentry simulations are reformulating algorithms to match the hierarchical parallel manycore CPUs/GPUs. This reformulation results in a need for linear solvers at different levels of hierarchical parallel algorithms. Several of these linear systems are small or medium size, independent, linear systems that can be solved at the same time. Depending on the problems, some of these linear systems are sparse and others dense. The community has been focused on solving dense linear systems as a batch, or solving multiple small dense linear systems. However, the work has focused primarily on one level of hierarchical parallelism - using an entire accelerator to solve the linear systems. We have been focused on solving batched dense/sparse linear systems, both at the device level and at the team level so applications can use these options effectively. We will present recent results on CPUs/GPUs using our hierarchical parallel batched linear solver implementations.    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75625' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1261' style='display:block'><a href='javascript:toggle_star(1261)' class='star'><span class='star1261'>&star;</span></a> <b>2:55 PM&ndash;3:10 PM (G107)</b> Piotr Luszczek, Batched Sparse Linear Algebra Interfaces, Solvers, Libraries, and Preconditioners <span id='bitlink-1167'><small><a href='javascript:show_bit(1167)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1167' style='display:none'><small><a href='javascript:hide_bit(1167)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Batched Sparse Linear Algebra Interfaces, Solvers, Libraries, and Preconditioners</b><br />Piotr Luszczek<br />Thursday, March 2 2:55 PM&ndash;3:10 PM<br />This is the 2nd talk in <a href='session-277.html'>Progresses on Batched Linear Solver Algorithms - Part I of II</a> (2:35 PM&ndash;4:15 PM)<br />G107<br /><br /><small>Batched sparse linear algebra solvers form the new frontier for  algorithmic development and performance engineering. Many applications  require simultaneous solutions of small linear systems of equations that  are structurally sparse. To move towards high hardware utilization, it  is important to provide these applications with appropriate interfaces  to efficient batched sparse solvers running on modern hardware  accelerators. We present interface designs in use by HPC software  libraries supporting batched sparse linear algebra and the development  of sparse batched kernel codes for solvers and preconditioners. We also  address the potential interoperability opportunities to keep the  software portable between the major hardware accelerators. The presented  interface specifications includes batched band, sparse iterative, and  sparse direct solvers.    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75625' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1262' style='display:block'><a href='javascript:toggle_star(1262)' class='star'><span class='star1262'>&star;</span></a> <b>3:15 PM&ndash;3:30 PM (G107)</b> Pratik Nayak, Batched Solvers and Preconditioners in Ginkgo. <span id='bitlink-1168'><small><a href='javascript:show_bit(1168)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1168' style='display:none'><small><a href='javascript:hide_bit(1168)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Batched Solvers and Preconditioners in Ginkgo.</b><br />Pratik Nayak<br />Thursday, March 2 3:15 PM&ndash;3:30 PM<br />This is the 3rd talk in <a href='session-277.html'>Progresses on Batched Linear Solver Algorithms - Part I of II</a> (2:35 PM&ndash;4:15 PM)<br />G107<br /><br /><small>In this talk, we will elaborate on the batched functionality in Ginkgo, a high performance numerical linear algebra library. Many applications which perform grid-wise computations, such as combustion and fusion plasma simulations require solution of independent linear systems at each grid point. This level of embarrassing parallelism is very well suited to the GPU. Traditionally, for these small problems, batched direct methods have been the method of choice. In this talk we will showcase the advantages of using an iterative solver in a batched fashion, and elaborate on the implementation and the challenges. We will show that we can achieve significant speedups compared to the dense and direct methods within our applications. Finally, we will show some techniques for enhancing and accelerating these batched iterative solvers with batched preconditioners and present the results for general matrices as well as matrices from specific applications.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75625' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1263' style='display:block'><a href='javascript:toggle_star(1263)' class='star'><span class='star1263'>&star;</span></a> <b>3:35 PM&ndash;3:50 PM (G107)</b> Sherry Li, Design and Evaluation of Batched Sparse Direct Solver for Multi-GPUs <span id='bitlink-1169'><small><a href='javascript:show_bit(1169)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1169' style='display:none'><small><a href='javascript:hide_bit(1169)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Design and Evaluation of Batched Sparse Direct Solver for Multi-GPUs</b><br />Sherry Li<br />Thursday, March 2 3:35 PM&ndash;3:50 PM<br />This is the 4th talk in <a href='session-277.html'>Progresses on Batched Linear Solver Algorithms - Part I of II</a> (2:35 PM&ndash;4:15 PM)<br />G107<br /><br /><small>We will present our recent work on a batched sparse  direct solver on multi-GPU machines. Our initial implementation assumes  that all the matrices in a batch have the same size and sparsity pattern.  This allows us to perform sparsity-preserving ordering and symbolic factorization only once.  For the numerical phases we designed two parallel schemes:  (1) coarse-grained approach by which multiple MPI ranks are assigned one linear system     and multiple linear systems are mapped to one GPU;  (2) fine-grained approach by which one MPI rank is assigned multiple linear systems,     and the batched operations occur in the internal sparse LU and sparse triangular solve.  We will show how each approach can be used in different usage scenarios,  and the possibility of hybridizing the two approaches.    
    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75625' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk1264' style='display:block'><a href='javascript:toggle_star(1264)' class='star'><span class='star1264'>&star;</span></a> <b>3:55 PM&ndash;4:10 PM (G107)</b> Yoav Moran, Multiplying 2x2 Sub-Blocks Using 4 Multiplications <span id='bitlink-1170'><small><a href='javascript:show_bit(1170)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-1170' style='display:none'><small><a href='javascript:hide_bit(1170)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Multiplying 2x2 Sub-Blocks Using 4 Multiplications</b><br />Yoav Moran<br />Thursday, March 2 3:55 PM&ndash;4:10 PM<br />This is the 5th talk in <a href='session-277.html'>Progresses on Batched Linear Solver Algorithms - Part I of II</a> (2:35 PM&ndash;4:15 PM)<br />G107<br /><br /><small>Fast recursive matrix multiplication algorithms switch to the cubic time classical algorithm on small sub-blocks as the classical algorithm requires fewer operations on small blocks. We obtain a new algorithm that can outperform the classical one, even on small blocks, by trading multiplications with additions. To this end, we introduce commutative algorithms that generalize Winogradâ€™s folding technique and combine it with fast matrix multiplication algorithms and the fast base change technique. Thus, when a single scalar multiplication requires $\rho$ times more clock cycles than an addition our technique reduces the computation cost of multiplying the small sub-blocks by a factor of $\frac{\rho+3}{2\left(\rho+1\right)}$ compared to using the classical algorithm. Our technique also reduces the energy cost of the algorithm, as the $\rho$ values for energy costs are typically even larger than the values for arithmetic costs.    
Specifically, we obtain an algorithm for multiplying 2x2 blocks using only four multiplications. This algorithm seemingly contradicts the lower bound of Winograd (1971) on multiplying 2x2 matrices. However, we obtain this algorithm by bypassing the implicit assumptions of the lower bound. We provide a new lower bound matching our algorithm for 2x2 block multiplication, thus showing our technique is optimal.    
Joint work with Oded Schwartz    </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75625' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
