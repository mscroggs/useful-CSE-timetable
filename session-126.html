<html>
<head>
<link rel="apple-touch-icon" sizes="57x57" href="apple-icon-57x57.png">
<link rel="apple-touch-icon" sizes="60x60" href="apple-icon-60x60.png">
<link rel="apple-touch-icon" sizes="72x72" href="apple-icon-72x72.png">
<link rel="apple-touch-icon" sizes="76x76" href="apple-icon-76x76.png">
<link rel="apple-touch-icon" sizes="114x114" href="apple-icon-114x114.png">
<link rel="apple-touch-icon" sizes="120x120" href="apple-icon-120x120.png">
<link rel="apple-touch-icon" sizes="144x144" href="apple-icon-144x144.png">
<link rel="apple-touch-icon" sizes="152x152" href="apple-icon-152x152.png">
<link rel="apple-touch-icon" sizes="180x180" href="apple-icon-180x180.png">
<link rel="icon" type="image/png" sizes="192x192"  href="android-icon-192x192.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="96x96" href="favicon-96x96.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="manifest.json">
<meta name="msapplication-TileColor" content="#ffffff">
<meta name="msapplication-TileImage" content="ms-icon-144x144.png">
<meta name="theme-color" content="#ffffff">

<title>Useful CSE timetable</title>
<style type='text/css'>
a.star {text-decoration:none;font-size:150%}
.header-links a {display:inline-block;padding:10px}
.header-links a,.header-links a:link,.header-links a:visited,.header-links a:active {color:blue;text-decoration:none}
.header-links a:hover {color:blue;text-decoration:underline}
</style>
<script type='text/javascript' src='fave.js?v=2023-02-27-02'></script>

</head>
<body>
<div class='header-links'>
<a href='index.html'>Personal timetable</a>
<a href='speakers.html'>List of speakers</a>
<a href='titles.html'>List of talk titles</a>
<a href='sync.html'>Copy favourites to another device</a>
</div>
<div class='header-links'>
<a href='https://meetings.siam.org/program.cfm?CONFCODE=cse23' target='new'><small>Official conference programme</small></a>
<a href='https://raw.githubusercontent.com/mscroggs/useful-CSE-timetable/json/talks.json' target='new'><small>Talks in JSON format</small></a>
<a href='https://github.com/mscroggs/useful-CSE-timetable/' target='new'><small>GitHub</small></a>
</div>
<h1>SIAM CSE 2023</h1>


<h2>Model-Based Optimal Experimental Design - Part I of II</h2><div class='index-talk' id='talk571' style='display:block'><a href='javascript:toggle_star(571)' class='star'><span class='star571'>&star;</span></a> <b>4:00 PM&ndash;4:15 PM (G108)</b> Matthias Chung, The Art to Repeatedly Project Your Problems &#8212; The Variable Projected Augmented Lagrangian Method <span id='bitlink-530'><small><a href='javascript:show_bit(530)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-530' style='display:none'><small><a href='javascript:hide_bit(530)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>The Art to Repeatedly Project Your Problems &#8212; The Variable Projected Augmented Lagrangian Method</b><br />Matthias Chung<br />Wednesday, March 1 4:00 PM&ndash;4:15 PM<br />This is the 1st talk in <a href='session-126.html'>Model-Based Optimal Experimental Design - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />G108<br /><br /><small>Inference by means of mathematical modeling from a collection of observations remains a crucial tool for scientific discovery and is ubiquitous in application areas such as signal compression, imaging restoration, and supervised machine learning. With ever-increasing model complexities and growing data size, new specially designed methods are urgently needed to recover meaningful quantities of interest. We consider the broad spectrum of linear inverse problems where the aim is to reconstruct quantities with a sparse representation on some vector space; often solved using the (generalized) least absolute shrinkage and selection operator (lasso).  The associated optimization problems have received significant attention, in particular in the early 2000s, because of their connection to compressed sensing and the reconstruction of solutions with favorable sparsity properties using augmented Lagrangians, alternating directions and splitting methods. We provide a new perspective on the underlying l1 regularized inverse problem by exploring the generalized lasso problem through variable projection methods. We arrive at our proposed variable projected augmented Lagrangian (vpal) method. We provide numerical examples demonstrating the computational efficiency for various imaging problems.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75413' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk572' style='display:block'><a href='javascript:toggle_star(572)' class='star'><span class='star572'>&star;</span></a> <b>4:20 PM&ndash;4:35 PM (G108)</b> Tapio Helin, Approximative Bayesian Optimal Experimental Design in Inverse Problems <span id='bitlink-531'><small><a href='javascript:show_bit(531)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-531' style='display:none'><small><a href='javascript:hide_bit(531)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Approximative Bayesian Optimal Experimental Design in Inverse Problems</b><br />Tapio Helin<br />Wednesday, March 1 4:20 PM&ndash;4:35 PM<br />This is the 2nd talk in <a href='session-126.html'>Model-Based Optimal Experimental Design - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />G108<br /><br /><small>We study the stability properties of the expected utility function in Bayesian optimal experimental design. We provide a framework for this problem in the case of expected information gain criterion in an infinite-dimensional setting, where we obtain the convergence of the expected utility with respect to perturbations. To make the problem more concrete we demonstrate that non-linear Bayesian inverse problems with Gaussian likelihood satisfy necessary assumptions in our theory. Some numerical simulations with different examples are explored.  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75413' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk573' style='display:block'><a href='javascript:toggle_star(573)' class='star'><span class='star573'>&star;</span></a> <b>4:40 PM&ndash;4:55 PM (G108)</b> Ruben Villarreal, Deep Reinforcement Learning for Experimental Design of Material Model Calibration <span id='bitlink-532'><small><a href='javascript:show_bit(532)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-532' style='display:none'><small><a href='javascript:hide_bit(532)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Deep Reinforcement Learning for Experimental Design of Material Model Calibration</b><br />Ruben Villarreal<br />Wednesday, March 1 4:40 PM&ndash;4:55 PM<br />This is the 3rd talk in <a href='session-126.html'>Model-Based Optimal Experimental Design - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />G108<br /><br /><small>This work presents a deep reinforcement learning (RL) algorithm for design of experiments that uses the Kullback-Leibler (KL) divergence, measured by a Kalman filter (KF), to maximize information gain. We apply the RL algorithm to optimally calibrate continuum plastic flow material models that capture the strain history dependence of materials exhibiting elastoplastic response. Although a subject matter expert can select experiments based on prior knowledge, as the complexity (anisotropy) of the model grows, so does the parameter space, and therefore it becomes increasingly difficult for a human expert to design a minimal set of experiments that simultaneously satisfy budget constraints and accuracy requirements. We cast the design of experiments as a game of traversing a decision tree to reach an optimally calibrated model at the end of the tree. Each decision represents an experimental control action. The material response is passed to an extended switching Kalman filter, which performs a Bayesian update of the model parameters and relays the information gain reward to the agent.  SNL is managed and operated by NTESS under DOE NNSA contract DE-NA0003525.    
  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75413' target='new'>More information on the conference website</a></small></div></span></div><div class='index-talk' id='talk575' style='display:block'><a href='javascript:toggle_star(575)' class='star'><span class='star575'>&star;</span></a> <b>5:00 PM&ndash;5:15 PM (G108)</b> Michael Ludkovski, Adaptive Batching for Gaussian Process Surrogates <span id='bitlink-533'><small><a href='javascript:show_bit(533)'>&#x25BC; Show talk info &#x25BC;</a></small></span><span id='bit-533' style='display:none'><small><a href='javascript:hide_bit(533)'>&#x25B2; Hide talk info &#x25B2;</a></small><div style='padding:20px'><b>Adaptive Batching for Gaussian Process Surrogates</b><br />Michael Ludkovski<br />Wednesday, March 1 5:00 PM&ndash;5:15 PM<br />This is the 4th talk in <a href='session-126.html'>Model-Based Optimal Experimental Design - Part I of II</a> (4:00 PM&ndash;5:40 PM)<br />G108<br /><br /><small>We develop adaptive replicated designs for Gaussian process surrogates of stochastic experiments. Replication offers efficiency in learning tasks that require large scale (thousands) stochastic simulations by reducing the size of the effective covariance matrix. Adaptive batching is a natural extension of sequential design heuristics with the benefit of replication growing as response features are learned, inputs concentrate, and the metamodeling overhead rises. Our algorithms propose novel acquisition functions that simultaneously determine the sequential design inputs $x^n$ and respective number of replicates $r^n$. We consider both schemes that gradually ratchet up the replication quantities $r^n$ as the number of inputs $n$ grows, and those that allocate additional replicates to existing inputs $x^k, k \le n$. Illustrations with synthetic experiments and an application in discrete-action stochastic control (learning noisily observed level sets that determine optimal actions) show that adaptive batching brings significant computational speed-ups with minimal loss of modeling fidelity.    
	  </small><br /><br /><small><a href='https://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=75413' target='new'>More information on the conference website</a></small></div></span></div>

</body>
</html>
